{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MET CS 777 - Big Data Analytics, prof. Dimitar Trajanov\n",
    "# PySpark RDD Basics\n",
    "This notebook is a simple introduction to the Spark RDD API.  It uses learning by example to demonstrate how the RDD API works and how to use it. For a comprehensive guide to the RDD API, please see the [Spark RDD API Documentation](https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#rdd-apis).\n",
    "\n",
    "*) Visual diagrams depicting the Spark API Created by Jeff Thomspon,Â https://github.com/jkthompson/pyspark-pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Before restarting the kernel (Notebook), stop the spark context\n",
    "# sc.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelizing and Collecting the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parallelize(c, numSlices=None)**\n",
    "\n",
    "The `parallelize()` function is used to create an RDD from a collection or an iterable object by distributing the data across multiple partitions in a parallel manner.\n",
    "\n",
    "**Parameters:**\n",
    "- `c`: The collection or iterable object to be parallelized into an RDD.\n",
    "- `numSlices` (optional): The number of partitions to split the data into. By default, it is set to `None`, and the number of partitions is determined automatically based on the cluster configuration.\n",
    "\n",
    "**Returns:**\n",
    "An RDD representing the distributed data.\n",
    "\n",
    "**Note:**\n",
    "- The `parallelize()` function is a method available in PySpark's `SparkContext` class.\n",
    "- The data in the collection or iterable object is partitioned and distributed across multiple partitions, allowing for parallel computation on a cluster.\n",
    "- The resulting RDD is immutable and can be operated upon using various transformation and action operations available in PySpark.\n",
    "- It is generally recommended to have a sufficient number of partitions to fully utilize the available cluster resources and enable parallel processing.\n",
    "- The `parallelize()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The `parallelize()` function is commonly used when working with small to medium-sized datasets that can fit into memory on a single machine.\n",
    "- The `parallelize()` function provides a convenient way to create an RDD from in-memory data, but for larger datasets, it is often more efficient to read the data from external storage systems using input operations like `textFile()` or `csvFile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([1,2,3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collect\">\n",
    "<img align=left src=\"images/pyspark-page22.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect()**\n",
    "\n",
    "The `collect()` method returns a list that contains all the elements in the Resilient Distributed Dataset (RDD). This operation transfers the data from the Spark Java Core to the Python environment. However, it's important to note that `collect()` can be an expensive operation.\n",
    "\n",
    "**Note**: It is recommended to use this method only when the resulting array is expected to be small, as it loads all the data into the driver's memory. Handling large datasets with `collect()` may lead to memory constraints and performance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "y = rdd.collect()\n",
    "print(rdd)  # distributed object\n",
    "print(y)  # not distributed, local data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
    "<img align=left src=\"images/pyspark-page39.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**take(num)**\n",
    "\n",
    "The `take()` method retrieves the first `num` elements from an RDD. It works by initially scanning one partition and using the results from that partition to estimate the number of additional partitions needed to satisfy the limit.\n",
    "\n",
    "**Parameters:**\n",
    "- `num`: The number of elements to retrieve from the RDD.\n",
    "\n",
    "**Returns:**\n",
    "A list containing the first `num` elements of the RDD.\n",
    "\n",
    "**Note:**\n",
    "- It is important to note that the order in which elements are retrieved is not guaranteed unless the RDD has been sorted in a specific order.\n",
    "- If `num` is larger than the total number of elements in the RDD, it will return all the elements available in the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 1, 2, 3]\n",
      "[1, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# take\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.take(num = 3)\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeOrdered\">\n",
    "<img align=left src=\"images/pyspark-page38.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**takeOrdered(num, key=None)**\n",
    "\n",
    "The `takeOrdered()` method retrieves the N elements from an RDD, ordered in ascending order or as specified by an optional key function.\n",
    "\n",
    "**Parameters:**\n",
    "- `num`: The number of elements to retrieve from the RDD.\n",
    "- `key` (optional): A function that specifies the sorting criteria. If provided, the elements will be sorted based on this key.\n",
    "\n",
    "**Returns:**\n",
    "A list containing the N elements from the RDD, ordered in ascending order or as specified by the key function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 1, 2, 3]\n",
      "[1, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# takeOrdered\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.takeOrdered(num = 3)\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.first\">\n",
    "<img align=left src=\"images/pyspark-page40.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first()**\n",
    "\n",
    "The `first()` method returns the first element in the Resilient Distributed Dataset (RDD).\n",
    "\n",
    "**Returns:**\n",
    "The first element in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `first()` operation retrieves the first element from the RDD. The order in which elements are returned is not guaranteed unless the RDD has been sorted in a specific order.\n",
    "- If the RDD is empty, calling `first()` will result in an error. It is recommended to handle such scenarios by checking the RDD's size or using alternative methods like `take(1)` to retrieve the first element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 1, 2, 3]\n",
      "The first element is 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "RDD is empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m x \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([])\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# this will return an error\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx=\u001b[39m\u001b[38;5;124m\"\u001b[39m,x\u001b[38;5;241m.\u001b[39mcollect())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(y)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\rdd.py:1591\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   1590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1591\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD is empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: RDD is empty"
     ]
    }
   ],
   "source": [
    "# first\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.first()\n",
    "print(\"x=\",x.collect())\n",
    "print('The first element is',y)\n",
    "\n",
    "x = sc.parallelize([])\n",
    "# this will return an error\n",
    "y = x.first()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.top\">\n",
    "<img align=left src=\"images/pyspark-page37.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**top(num, key=None)**\n",
    "\n",
    "The `top()` method retrieves the top N elements from an RDD. It returns a list sorted in descending order by default.\n",
    "\n",
    "**Parameters:**\n",
    "- `num`: The number of elements to retrieve from the RDD.\n",
    "- `key` (optional): A function that specifies the sorting criteria. If provided, the elements will be sorted based on this key.\n",
    "\n",
    "**Returns:**\n",
    "A list containing the top N elements from the RDD, sorted in descending order.\n",
    "\n",
    "**Note:**\n",
    "- If `num` is larger than the total number of elements in the RDD, it will return all the elements in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 1, 2, 4]\n",
      "Biggest elements [4, 3, 2]\n",
      "Smallest elements [1, 1, 2]\n",
      "All elements [4, 3, 2, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# top\n",
    "x = sc.parallelize([1,3,1,2,4])\n",
    "y = x.top(num = 3)\n",
    "print(\"x=\",x.collect())\n",
    "print(\"Biggest elements\",y)\n",
    "\n",
    "# top with key function that will return the top 3 smallest elements\n",
    "y = x.top(num = 3, key = lambda x: -x)\n",
    "print(\"Smallest elements\",y)\n",
    "\n",
    "# If `num` is larger than the total number of elements in the RDD, it will return all the elements in descending order.\n",
    "y = x.top(num = 10)\n",
    "print(\"All elements\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collectAsMap\">\n",
    "<img align=left src=\"images/pyspark-page41.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collectAsMap()**\n",
    "\n",
    "The `collectAsMap()` method returns the key-value pairs in the Resilient Distributed Dataset (RDD) as a dictionary to the master node.\n",
    "\n",
    "**Returns:**\n",
    "A dictionary containing the key-value pairs from the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `collectAsMap()` operation transfers the data from the distributed RDD to the master node and represents it as a dictionary.\n",
    "- Each key-value pair in the RDD is mapped to a corresponding entry in the dictionary, with the keys being unique.\n",
    "- It is important to ensure that the resulting dictionary can fit into the memory of the master node, as all the data is loaded into memory.\n",
    "- If there are duplicate keys in the RDD, the final dictionary will contain the value corresponding to the last occurrence of each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 3), ('A', 1), ('B', 2), ('D', 4), ('E', 5)]\n",
      "The dictionary of {key:vlaue} pairs: {'C': 3, 'A': 1, 'B': 2, 'D': 4, 'E': 5}\n",
      "x= [('C', 3), ('A', 1), ('B', 2), ('A', 4), ('B', 5)]\n",
      "Result if there are duplicates: {'C': 3, 'A': 4, 'B': 5}\n"
     ]
    }
   ],
   "source": [
    "# collectAsMap\n",
    "# collectAsMap\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2), ('D', 4), ('E', 5)])\n",
    "y = x.collectAsMap()\n",
    "print(\"x=\", x.collect())\n",
    "print(\"The dictionary of {key:vlaue} pairs:\",y)\n",
    "\n",
    "# If there are duplicate keys, the value of the last key will be retained.\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2), ('A', 4), ('B', 5)])\n",
    "y = x.collectAsMap()\n",
    "print(\"x=\",x.collect())\n",
    "print(\"Result if there are duplicates:\",y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.map\">\n",
    "<img align=left src=\"images/pyspark-page3.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**map(f, preservesPartitioning=False)**\n",
    "\n",
    "The `map()` method returns a new distributed dataset formed by passing each element of the source RDD through a function `f`.\n",
    "\n",
    "**Parameters:**\n",
    "- `f`: The function to apply to each element of the RDD.\n",
    "- `preservesPartitioning` (optional): A boolean flag indicating whether the new RDD should preserve the original partitioning. The default value is `False`.\n",
    "\n",
    "**Returns:**\n",
    "A new distributed dataset (RDD) where each element is the result of applying the function `f` to the corresponding element of the source RDD.\n",
    "\n",
    "**Note:**\n",
    "- The function `f` can be any Python function, lambda function, or a callable object that accepts an element of the RDD as input and produces a transformed output.\n",
    "- The `map()` operation is a transformation operation in PySpark, meaning it is **lazily evaluated**. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- By default, the new RDD does not preserve the original partitioning. If `preservesPartitioning` is set to `True`, the resulting RDD will have the same partitioning as the source RDD, assuming the transformation does not change the keys of the elements.\n",
    "- The `map()` operation is commonly used for element-wise transformations, such as applying mathematical operations, data cleaning, feature extraction, or any other custom logic on each element of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['b', 'a', 'c']\n",
      "[('b', 1), ('a', 1), ('c', 1)]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "y = x.map(lambda x: (x, 1))\n",
    "print(\"x=\",x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "[(1, 1), (2, 4), (3, 9)]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object\n",
    "y = x.map(lambda x: (x,x**2))\n",
    "print(\"x=\",x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "# map\n",
    "x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object\n",
    "y = x.map(lambda x: x**2)\n",
    "print(\"x=\",x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMap\">\n",
    "<img align=left src=\"images/pyspark-page4.svg\" width=360 height=203 /></a>\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**flatMap(f, preservesPartitioning=False)**\n",
    "\n",
    "The `flatMap()` method returns a new RDD by first applying a function `f` to all elements of the source RDD, and then flattening the resulting sequences or collections.\n",
    "\n",
    "**Parameters:**\n",
    "- `f`: The function to apply to each element of the RDD.\n",
    "- `preservesPartitioning` (optional): A boolean flag indicating whether the new RDD should preserve the original partitioning. The default value is `False`.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD resulting from applying the function `f` to each element of the source RDD and flattening the results.\n",
    "\n",
    "**Note:**\n",
    "- The function `f` can be any Python function, lambda function, or a callable object that accepts an element of the RDD as input and returns an iterable (e.g., a list, tuple, set) of elements.\n",
    "- The `flatMap()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD may have more or fewer elements than the source RDD, depending on the transformation function `f`.\n",
    "- By default, the new RDD does not preserve the original partitioning. If `preservesPartitioning` is set to `True`, the resulting RDD will have the same partitioning as the source RDD, assuming the transformation does not change the keys of the elements.\n",
    "- The `flatMap()` operation is commonly used when each input element of the RDD is mapped to multiple output elements, such as when exploding nested structures, tokenizing text, or performing any operation that expands or flattens the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
      "[(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n"
     ]
    }
   ],
   "source": [
    "# Map\n",
    "x = sc.parallelize([(1,2,3),(2,3,4),(3,4,5)])\n",
    "y = x.map(lambda x: x)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
      "[1, 2, 3, 2, 3, 4, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([(1,2,3),(2,3,4),(3,4,5)])\n",
    "y = x.flatMap(lambda x: x)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "[1, 100, 1, 2, 200, 4, 3, 300, 9]\n"
     ]
    }
   ],
   "source": [
    "# flatMap\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "[(1, 100, 1), (2, 200, 4), (3, 300, 9)]\n"
     ]
    }
   ],
   "source": [
    "# Map\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.map(lambda x: (x, 100*x, x**2))\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [2, 3, 4]\n",
      "[1, 1, 2, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "x = sc.parallelize([2, 3, 4])\n",
    "y = x.flatMap(lambda x: range(1, x))\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apache Spark is a unified analytics engine for large-scale data processing.', 'It provides high-level APIs in Java, Scala, Python and R', 'It also supports a rich set of higher-level tools including Spark SQL', 'MLlib for machine learning', 'GraphX for graph processing', 'Structured Streaming for incremental computation and stream processing']\n",
      "['Apache', 'Spark', 'is', 'a', 'unified', 'analytics', 'engine', 'for', 'large-scale', 'data', 'processing.', 'It', 'provides', 'high-level', 'APIs', 'in', 'Java,', 'Scala,', 'Python', 'and', 'R', 'It', 'also', 'supports', 'a', 'rich', 'set', 'of', 'higher-level', 'tools', 'including', 'Spark', 'SQL', 'MLlib', 'for', 'machine', 'learning', 'GraphX', 'for', 'graph', 'processing', 'Structured', 'Streaming', 'for', 'incremental', 'computation', 'and', 'stream', 'processing']\n"
     ]
    }
   ],
   "source": [
    "# Split sentence into words\n",
    "lines = sc.parallelize([\n",
    "    \"Apache Spark is a unified analytics engine for large-scale data processing.\",\n",
    "    \"It provides high-level APIs in Java, Scala, Python and R\",\n",
    "    \"It also supports a rich set of higher-level tools including Spark SQL\",\n",
    "    \"MLlib for machine learning\",\n",
    "    \"GraphX for graph processing\",\n",
    "    \"Structured Streaming for incremental computation and stream processing\"\n",
    " ])\n",
    "words = lines.flatMap(lambda x: x.split(' '))\n",
    "print(lines.collect())\n",
    "print(words.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapValues\">\n",
    "<img align=left src=\"images/pyspark-page56.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mapValues(f)**\n",
    "\n",
    "The `mapValues()` method applies a map function `f` to each value in a key-value pair RDD, while retaining the original keys and the partitioning of the RDD.\n",
    "\n",
    "**Parameters:**\n",
    "- `f`: The function to apply to each value in the key-value pairs.\n",
    "\n",
    "**Returns:**\n",
    "A new key-value pair RDD with the same keys as the original RDD, where each value has been transformed by the function `f`.\n",
    "\n",
    "**Note:**\n",
    "- The `mapValues()` operation only applies the provided function `f` to the values of the key-value pairs, keeping the keys unchanged.\n",
    "- The function `f` can be any Python function, lambda function, or a callable object that accepts a value from the key-value pairs as input and returns a transformed value.\n",
    "- The `mapValues()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have the same partitioning as the original RDD, preserving the partitioning scheme of the key-value pair RDD.\n",
    "- The `mapValues()` operation is useful when you want to apply a transformation to the values of a key-value pair RDD while keeping the keys intact. It is commonly used for value-specific operations, such as mathematical transformations, data cleaning, or feature extraction on the values of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', [1, 4, 9]), ('B', [16, 25])]\n"
     ]
    }
   ],
   "source": [
    "# mapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('A', (1, 2, 3)), ('B', (4, 5))]\n",
      "[('A', [1, 4, 9]), ('B', [16, 25])]\n"
     ]
    }
   ],
   "source": [
    "# mapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.map(lambda x: (x[0], [i**2 for i in x[1]])) # function is applied to entire value\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
    "<img align=left src=\"images/pyspark-page8.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter(func)**\n",
    "\n",
    "The `filter()` method returns a new dataset formed by selecting the elements from the source dataset for which the function `func` returns `True`.\n",
    "\n",
    "**Parameters:**\n",
    "- `func`: The function that determines the filtering condition for each element.\n",
    "\n",
    "**Returns:**\n",
    "A new dataset (RDD) that contains the elements from the source dataset for which the function `func` returns `True`.\n",
    "\n",
    "**Note:**\n",
    "- The function `func` can be any Python function, lambda function, or a callable object that accepts an element of the dataset as input and returns a Boolean value indicating whether the element should be included (`True`) or excluded (`False`).\n",
    "- The `filter()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The `filter()` operation is commonly used to perform data filtering or selection based on certain criteria. It allows you to include or exclude specific elements from the dataset based on a custom filtering logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters even odd elements\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
    "<img align=left src=\"images/pyspark-page9.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distinct(numPartitions=None)**\n",
    "\n",
    "The `distinct()` method returns a new RDD that contains only the distinct elements from the source RDD, removing any duplicate elements.\n",
    "\n",
    "**Parameters:**\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD containing only the distinct elements from the source RDD.\n",
    "\n",
    "**Note:**\n",
    "- The order of elements in the resulting RDD may not be preserved.\n",
    "- The `distinct()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The `distinct()` operation is useful for eliminating duplicate elements in a dataset, ensuring that each element appears only once. It is commonly used for data deduplication or to extract unique values from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['A', 'A', 'B']\n",
      "['A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = x.distinct()\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keys\">\n",
    "<img align=left src=\"images/pyspark-page42.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**keys()**\n",
    "\n",
    "The `keys()` method returns a new RDD that contains only the keys of each tuple in a key-value pair RDD.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing only the keys of each tuple in the key-value pair RDD.\n",
    "\n",
    "**Note:**\n",
    "- The resulting RDD will have the same number of elements as the original RDD, with each element representing a key from the tuples.\n",
    "- The `keys()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The `keys()` operation is commonly used when you need to perform operations specifically on the keys of a key-value pair RDD, such as filtering or joining based on the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 3), ('A', 1), ('B', 2)]\n",
      "['C', 'A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# keys\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.keys()\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.values\">\n",
    "<img align=left src=\"images/pyspark-page43.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**values()**\n",
    "\n",
    "The `values()` method returns a new RDD that contains only the values of each tuple in a key-value pair RDD.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing only the values of each tuple in the key-value pair RDD.\n",
    "\n",
    "**Note:**\n",
    "- The resulting RDD will have the same number of elements as the original RDD, with each element representing a value from the tuples.\n",
    "- The `values()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The `values()` operation is commonly used when you need to perform operations specifically on the values of a key-value pair RDD, such as aggregations, calculations, or transformations on the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 3), ('A', 1), ('B', 2)]\n",
      "[3, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "# values\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.values()\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitions\n",
    "Partitions in Spark refer to the fundamental units of parallelism in distributed processing. When you work with large datasets in Spark, they are divided into smaller, more manageable chunks called partitions. Each partition contains a subset of the data and can be processed independently on different executor nodes in a cluster.\n",
    "\n",
    "Here are some key points about partitions in Spark:\n",
    "\n",
    "- **Parallel Processing**: Spark performs computations in parallel by dividing the data into partitions. Each partition is processed independently by a task running on a separate executor, enabling parallelism and distributed computing.\n",
    "- **Data Distribution**: Partitions help distribute the data across the nodes in a cluster. By dividing the data into smaller partitions, Spark can achieve load balancing and utilize the available resources efficiently.\n",
    "- **Partitioning Schemes**: Spark provides various partitioning schemes, such as hash partitioning and range partitioning, to determine how the data is divided among partitions. The choice of partitioning scheme can impact data distribution and performance.\n",
    "- **Transformation and Actions**: Transformations in Spark, such as `map()` or `filter()`, are applied on a per-partition basis. Actions like `reduce()` or `collect()` operate on the data across all partitions, leveraging parallelism.\n",
    "- **Control and Optimization**: Partitions provide fine-grained control over data processing. Developers can control the number of partitions, repartition data, or perform custom partitioning to optimize performance and resource usage.\n",
    "- **Data Locality**: Spark tries to achieve data locality, where partitions are processed on the same nodes where the data resides or is cached. This reduces data transfer across the network and improves performance.\n",
    "- **Shuffling**: Certain operations, like `groupBy()` or `join()`, may require data to be shuffled across partitions. Shuffling involves redistributing data based on specific keys or criteria, which can incur additional overhead.\n",
    "- **Partition Size**: The optimal partition size depends on factors like the available resources, data characteristics, and the specific workload. Choosing an appropriate partition size helps balance data distribution, minimize data skew, and avoid memory or performance issues.\n",
    "\n",
    "Understanding and managing partitions in Spark is crucial for efficient data processing and performance optimization. By appropriately configuring and utilizing partitions, you can leverage the distributed processing capabilities of Spark to handle large-scale datasets effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.getNumPartitions\">\n",
    "<img align=left src=\"images/pyspark-page7.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**getNumPartitions()**\n",
    "\n",
    "The `getNumPartitions()` method returns the number of partitions in an RDD.\n",
    "\n",
    "**Returns:**\n",
    "The number of partitions in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- Partitions in an RDD represent the division of data into smaller, manageable chunks for distributed processing.\n",
    "- The number of partitions affects parallelism and the degree of concurrency during RDD processing.\n",
    "- The actual number of partitions in an RDD depends on factors such as the input data, data sources, transformations applied, and the cluster configuration.\n",
    "- The `getNumPartitions()` operation is a metadata operation in Spark and does not trigger any computation.\n",
    "- By knowing the number of partitions in an RDD, you can optimize transformations, resource allocation, and workload management accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4], [5, 6, 7, 8], [9, 10], [11, 12, 13, 14]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# getNumPartitions\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14], 5)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.repartition\">\n",
    "<img align=left src=\"images/pyspark-page63.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**repartition(numPartitions)**\n",
    "\n",
    "The `repartition()` method returns a new RDD that has exactly `numPartitions` partitions. This operation can increase or decrease the level of parallelism in the RDD by redistributing the data using a shuffle.\n",
    "\n",
    "**Parameters:**\n",
    "- `numPartitions`: The desired number of partitions for the resulting RDD.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD with exactly `numPartitions` partitions.\n",
    "\n",
    "**Note:**\n",
    "- The `repartition()` operation reshuffles the data in the RDD to create a new RDD with the specified number of partitions.\n",
    "- If the number of partitions is increased, a shuffle is performed to redistribute the data across the new partitions. This can be an expensive operation.\n",
    "- If the number of partitions is decreased, it is recommended to use the `coalesce()` operation instead of `repartition()`. `coalesce()` can avoid a full shuffle by merging partitions without redistributing the data randomly.\n",
    "- The resulting RDD may have a different distribution of data across partitions compared to the original RDD.\n",
    "- The `repartition()` operation is a transformation operation in PySpark and is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- Use `repartition()` when you explicitly need to change the number of partitions in an RDD, such as to increase parallelism or balance data distribution. If you want to decrease the number of partitions without performing a full shuffle, consider using `coalesce()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4, 5]]\n",
      "[[], [1, 2], [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "# repartition\n",
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "y = x.repartition(numPartitions=3)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"images/pyspark-page64.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**coalesce(numPartitions, shuffle=False)**\n",
    "\n",
    "The `coalesce()` method returns a new RDD that is reduced into `numPartitions` partitions.\n",
    "\n",
    "**Parameters:**\n",
    "- `numPartitions`: The number of partitions to reduce the RDD into.\n",
    "- `shuffle` (optional): A boolean flag indicating whether to shuffle the data during the coalesce operation. The default value is `False`.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD that has been reduced into `numPartitions` partitions.\n",
    "\n",
    "**Note:**\n",
    "- The `coalesce()` operation reduces the number of partitions in an RDD to the specified `numPartitions`.\n",
    "- If `shuffle` is set to `False`, the coalesce operation tries to minimize data movement and avoids a full shuffle. It merges partitions into larger ones by moving data across the partitions, if necessary.\n",
    "- If `shuffle` is set to `True`, the coalesce operation performs a full shuffle, redistributing the data across partitions randomly. This can be more expensive in terms of performance and resource usage.\n",
    "- The resulting RDD may have fewer partitions than the original RDD, but it does not guarantee a balanced distribution of data across the partitions.\n",
    "- The `coalesce()` operation is a transformation operation in PySpark and is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The `coalesce()` operation can be useful for reducing the number of partitions to optimize resource usage, improve data locality, or prepare the data for subsequent operations that require a specific partitioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12, 13, 14]]\n",
      "[[1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11, 12, 13, 14]]\n"
     ]
    }
   ],
   "source": [
    "# coalesce\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8,9,10,11,12,13,14], 4)\n",
    "y = x.coalesce(numPartitions=2)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The difference between repartition and coalesce**\n",
    "The main difference between `repartition()` and `coalesce()` in PySpark is how they handle the number of partitions and data shuffling:\n",
    "\n",
    "1. **Number of Partitions:**\n",
    "   - `repartition(numPartitions)` explicitly sets the exact number of partitions for the resulting RDD. It can increase or decrease the number of partitions by performing a full shuffle of the data.\n",
    "   - `coalesce(numPartitions)` can only decrease the number of partitions to a smaller value. It tries to minimize data movement by merging partitions without a full shuffle.\n",
    "\n",
    "2. **Data Shuffling:**\n",
    "   - `repartition(numPartitions)` always performs a shuffle operation, redistributing the data across the new partitions. It is an expensive operation as it involves data movement and network communication.\n",
    "   - `coalesce(numPartitions)` avoids shuffling if `shuffle=False` (default). It merges partitions by moving data within the existing partitions, which can be more efficient than a full shuffle. However, if `shuffle=True` is explicitly set, it performs a shuffle.\n",
    "\n",
    "Considerations:\n",
    "- If you need to increase or explicitly set the number of partitions or perform a random redistribution of data, use `repartition()`.\n",
    "- If you want to decrease the number of partitions without shuffling or when the desired number of partitions is smaller than the current number, use `coalesce()` to minimize data movement.\n",
    "- `coalesce()` is more efficient than `repartition()` when reducing the number of partitions, as it avoids the costly shuffle operation. However, it may result in an uneven data distribution across partitions.\n",
    "- If you are uncertain about whether to use `repartition()` or `coalesce()`, consider factors such as the desired level of parallelism, data skew, available resources, and the cost of shuffling.\n",
    "- Both operations are transformation operations and are lazily evaluated, meaning they won't be executed until an action is triggered on the resulting RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.glom\">\n",
    "<img align=left src=\"images/pyspark-page16.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**glom()**\n",
    "\n",
    "The `glom()` method returns a new RDD created by coalescing all elements within each partition into a list.\n",
    "\n",
    "**Returns:**\n",
    "An RDD where each partition is represented as a single list containing all the elements of that partition.\n",
    "\n",
    "**Note:**\n",
    "- The `glom()` operation is a transformation operation in PySpark that restructures the RDD by combining all elements within each partition into a single list.\n",
    "- Each partition in the resulting RDD is represented as a list containing all the elements from that partition.\n",
    "- The order of elements within each list is the same as the original order of elements within the partition.\n",
    "- The `glom()` operation is useful when you need to process the entire partition as a whole, rather than individual elements. It can be beneficial for certain types of computations or operations that require aggregating or analyzing data within each partition.\n",
    "- The `glom()` operation is lazily evaluated and will not be executed until an action is triggered on the resulting RDD.\n",
    "- It is important to consider the size of partitions and memory constraints when using `glom()`, as coalescing all elements into a single list within each partition can increase memory requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['C', 'B', 'A']\n",
      "[['C'], ['B', 'A']]\n"
     ]
    }
   ],
   "source": [
    "# glom\n",
    "x = sc.parallelize(['C','B','A'], 2)\n",
    "y = x.glom()\n",
    "print(\"x=\",x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeSample\">\n",
    "<img align=left src=\"images/pyspark-page11.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**takeSample(withReplacement, num, seed=None)**\n",
    "\n",
    "The `takeSample()` method returns a fixed-size sampled subset of the RDD. This operation requires the `numpy` library.\n",
    "\n",
    "**Parameters:**\n",
    "- `withReplacement`: A boolean flag indicating whether sampling should be done with replacement (`True`) or without replacement (`False`).\n",
    "- `num`: The number of elements to sample from the RDD.\n",
    "- `seed` (optional): The seed value for the random number generator used for sampling. Providing a seed ensures reproducibility of the sampled subset.\n",
    "\n",
    "**Returns:**\n",
    "A list containing a fixed-size sampled subset of the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `takeSample()` operation randomly selects a fixed-size subset of elements from the RDD.\n",
    "- If `withReplacement` is set to `True`, the same element can be sampled multiple times, allowing duplicates in the resulting subset. If `False`, each element can be selected at most once, ensuring distinct elements in the subset.\n",
    "- The `num` parameter specifies the size of the sampled subset.\n",
    "- The `seed` parameter is used to initialize the random number generator for reproducibility. If not provided, the sampling will be different each time the operation is executed.\n",
    "- The `takeSample()` operation requires the `numpy` library to be available.\n",
    "- The resulting subset may contain fewer elements than the specified `num` if the RDD has fewer elements available for sampling.\n",
    "- The `takeSample()` operation is an action in PySpark, and it triggers the execution of the sampling and returns the sampled subset as a list.\n",
    "- The `takeSample()` operation is commonly used when you need a random subset of elements from an RDD for analysis, testing, or sampling purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [1, 2, 4]\n",
      "sample:1 y = [1, 2, 4]\n",
      "sample:2 y = [4, 1, 2]\n",
      "sample:3 y = [5, 6, 2]\n",
      "sample:4 y = [5, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# takeSample\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set oprations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.union\">\n",
    "<img align=left src=\"images/pyspark-page12.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**union(other)**\n",
    "\n",
    "The `union()` method returns a new RDD that represents the union of the elements in the source RDD and another RDD.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The RDD to be combined with the source RDD.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD that contains all the elements from the source RDD and the `other` RDD.\n",
    "\n",
    "**Note:**\n",
    "- The resulting RDD contains all the elements from both the source RDD and the `other` RDD, without eliminating duplicates. If an element appears in both RDDs, it will be included twice in the resulting RDD.\n",
    "- The source RDD and the `other` RDD must have the same element type.\n",
    "- The `union()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have a number of partitions based on the partitioning scheme of the source RDD and the `other` RDD.\n",
    "- The `union()` operation is useful for combining the data from two RDDs into a single RDD, enabling you to perform operations or analysis on the combined dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['A', 'A', 'B']\n",
      "y= ['D', 'C', 'A']\n",
      "['A', 'A', 'B', 'D', 'C', 'A']\n"
     ]
    }
   ],
   "source": [
    "# union\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['D','C','A'])\n",
    "z = x.union(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(\"y=\",y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.intersection\">\n",
    "<img align=left src=\"images/pyspark-page13.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intersection(other)**\n",
    "\n",
    "The `intersection()` method returns a new RDD that represents the intersection of the elements in the source RDD and another RDD. The resulting RDD contains only the distinct elements that are common to both RDDs.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The RDD to find the intersection with.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD that contains the distinct elements common to both the source RDD and the `other` RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `intersection()` operation finds the common elements between the source RDD and the `other` RDD, removing any duplicates in the process.\n",
    "- The resulting RDD contains only the distinct elements that are present in both RDDs. If an element appears multiple times in either RDD, it will appear only once in the resulting RDD.\n",
    "- The `intersection()` operation performs a shuffle internally to identify the common elements across partitions of the RDDs. This shuffle can incur additional overhead.\n",
    "- The source RDD and the `other` RDD must have the same element type.\n",
    "- The `intersection()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have a number of partitions based on the partitioning scheme of the source RDD and the `other` RDD.\n",
    "- The `intersection()` operation is useful for finding common elements between two RDDs, such as identifying shared data points, performing set operations, or filtering datasets based on common attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['A', 'A', 'B']\n",
      "['A', 'C', 'D']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "# intersection\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['A','C','D'])\n",
    "z = x.intersection(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"images/pyspark-page23.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduce(f)**\n",
    "\n",
    "The `reduce()` method reduces the elements of the RDD using the specified **commutative and associative binary operator** `f`. It performs the reduction operation locally within each partition of the RDD.\n",
    "\n",
    "**Parameters:**\n",
    "- `f`: The commutative and associative binary operator function to be applied to the elements.\n",
    "\n",
    "**Returns:**\n",
    "The result of reducing the elements of the RDD using the binary operator `f`.\n",
    "\n",
    "**Note:**\n",
    "- The `reduce()` operation applies the binary operator `f` to the elements of the RDD in a cumulative manner, combining them to produce a single result.\n",
    "- The binary operator `f` must be commutative and associative, meaning the order of applying the operator does not affect the result, and the grouping of elements does not impact the final output.\n",
    "- The reduction is performed independently within each partition of the RDD, resulting in partial results for each partition.\n",
    "- The partial results from each partition are then combined together using the same binary operator `f` to produce the final result.\n",
    "- The `reduce()` operation is useful for aggregating the elements of an RDD into a single value, such as calculating sums, maximum or minimum values, or any other operation that can be expressed as a commutative and associative binary operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.reduce(lambda x, y: x + y)  # computes a cumulative sum\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.fold\">\n",
    "<img align=left src=\"images/pyspark-page24.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fold(zeroValue, op)**\n",
    "\n",
    "The `fold()` method aggregates the elements of each partition in an RDD and then combines the results of all partitions using a given associative function and a neutral \"zero value\". The `op` function is used to perform the aggregation and must adhere to certain requirements.\n",
    "\n",
    "**Parameters:**\n",
    "- `zeroValue`: The initial or neutral value for the aggregation operation.\n",
    "- `op`: The associative function used for aggregating the elements.\n",
    "\n",
    "**Returns:**\n",
    "The result of aggregating the elements of the RDD using the `op` function and the `zeroValue`.\n",
    "\n",
    "**Note:**\n",
    "- The `fold()` operation applies the `op` function to each partition of the RDD to aggregate the elements within that partition.\n",
    "- The `op` function must be associative, meaning the order of applying the function does not affect the final result.\n",
    "- The `zeroValue` serves as an initial value for the aggregation operation and is used as a neutral element that does not change the result when combined with any other element using the `op` function.\n",
    "- The `op(t1,t2)` function is allowed to modify the `t1` parameter and return it as the result value, avoiding object allocation. However, it should not modify the `t2` parameter.\n",
    "- The aggregation is performed independently within each partition, resulting in partial results for each partition.\n",
    "- The partial results from each partition are then combined together using the `op` function to produce the final result.\n",
    "- The `fold()` operation is useful for aggregating data in RDDs by applying a user-defined associative function. It allows for custom aggregation logic while providing the flexibility to optimize performance by avoiding object allocation and minimizing data shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "x = sc.parallelize([1,2,3])\n",
    "neutral_zero_value = 0  # 0 for sum, 1 for multiplication\n",
    "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated + obj) # computes cumulative sum\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregate\">\n",
    "<img align=left src=\"images/pyspark-page25.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aggregate(zeroValue, seqOp, combOp)**\n",
    "\n",
    "The `aggregate()` method aggregates the elements of each partition in an RDD and then combines the results of all partitions using given combine functions and a neutral \"zero value\". This operation allows for different result types for the sequential and combined operations.\n",
    "\n",
    "**Parameters:**\n",
    "- `zeroValue`: The initial or neutral value for the aggregation operation.\n",
    "- `seqOp`: The sequential operation function used for aggregating the elements within each partition.\n",
    "- `combOp`: The combined operation function used for merging the results of different partitions.\n",
    "\n",
    "**Returns:**\n",
    "The result of aggregating the elements of the RDD using the `seqOp` and `combOp` functions and the `zeroValue`.\n",
    "\n",
    "**Note:**\n",
    "- The `aggregate()` operation applies the `seqOp` function to each partition of the RDD to aggregate the elements within that partition. The result type of `seqOp` can be different from the RDD's element type.\n",
    "- The `combOp` function is used to merge the results of different partitions, combining them into a single result.\n",
    "- The `zeroValue` serves as an initial value for the aggregation operation and is used as a neutral element that does not change the result when combined with any other element using the `seqOp` and `combOp` functions.\n",
    "- Both the `seqOp` and `combOp` functions are allowed to modify the `t1` parameter and return it as the result value to avoid object allocation. However, they should not modify the `t2` parameter.\n",
    "- The aggregation is performed independently within each partition, resulting in partial results for each partition.\n",
    "- The partial results from each partition are then combined together using the `combOp` function to produce the final result.\n",
    "- The `aggregate()` operation is useful for performing custom aggregation operations on RDDs, allowing for flexibility in the result types and providing efficient ways to combine the results of different partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [2, 3, 4]\n",
      "(9, 24)\n"
     ]
    }
   ],
   "source": [
    "# aggregate\n",
    "x = sc.parallelize([2,3,4])\n",
    "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
    "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
    "combOp = (lambda aggregated1, aggregated2: (aggregated1[0] + aggregated2[0], aggregated1[1] * aggregated2[1]))\n",
    "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
    "<img align=left src=\"images/pyspark-page44.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reduceByKey(func, numPartitions=None)**\n",
    "\n",
    "The `reduceByKey()` method merges the values for each key in an RDD using an associative reduce function.\n",
    "\n",
    "**Parameters:**\n",
    "- `func`: The associative reduce function to merge values for each key.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD with the values for each key merged using the reduce function.\n",
    "\n",
    "**Note:**\n",
    "- The reduce function `func` must be associative, meaning that the order of applying the function does not affect the result.\n",
    "- The `reduceByKey()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `reduceByKey()` operation is commonly used for aggregation tasks, such as summing values for each key, finding maximum or minimum values, or any other reduction operation that combines values based on the key.\n",
    "- It is important to choose an appropriate reduce function that can handle the merging of values for each key efficiently and correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 3), ('A', 12)]\n"
     ]
    }
   ],
   "source": [
    "# reduceByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foldByKey\">\n",
    "<img align=left src=\"images/pyspark-page53.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**foldByKey(zeroValue, func, numPartitions=None)**\n",
    "\n",
    "The `foldByKey()` method merges the values for each key in an RDD using an associative function `func` and a neutral `zeroValue`. The `zeroValue` can be added to the result an arbitrary number of times and should not affect the final outcome.\n",
    "\n",
    "**Parameters:**\n",
    "- `zeroValue`: The neutral value that can be added to the result an arbitrary number of times.\n",
    "- `func`: The associative function used to merge values for each key.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD with the values for each key merged using the fold function.\n",
    "\n",
    "**Note:**\n",
    "- The `func` function must be associative, meaning that the order of applying the function and adding the `zeroValue` does not affect the result.\n",
    "- The `zeroValue` serves as a neutral element that does not change the result when combined with any other element using the `func` function.\n",
    "- The `foldByKey()` operation is a transformation operation in PySpark, meaning it is lazily evaluated.\n",
    "- The resulting RDD will have the keys from the original RDD and the merged values based on the fold function and the `zeroValue`.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `foldByKey()` operation is commonly used for tasks where values for each key need to be aggregated or combined, such as calculating sums, products, or any other operation that can be expressed as an associative function with a neutral element.\n",
    "- It is important to choose an appropriate `func` function and `zeroValue` that can handle the merging of values for each key correctly and ensure the neutral element does not affect the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', 2), ('A', 60)]\n"
     ]
    }
   ],
   "source": [
    "# foldByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = 1 # one is 'zero value' for multiplication\n",
    "y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregateByKey\">\n",
    "<img align=left src=\"images/pyspark-page52.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**aggregateByKey(zeroValue, seqFunc, combFunc, numPartitions=None)**\n",
    "\n",
    "The `aggregateByKey()` method aggregates the values of each key in an RDD using given combine functions and a neutral \"zero value\". This function allows for a different result type `U` than the type of the values in the RDD `V`. It requires two operations: one for merging a `V` into a `U` within a partition and another for merging two `U` values between partitions. These operations can modify and return their first argument to avoid memory allocation.\n",
    "\n",
    "**Parameters:**\n",
    "- `zeroValue`: The neutral value or zero value for the aggregation operation.\n",
    "- `seqFunc`: The function used to merge a value `V` into an intermediate result `U` within each partition.\n",
    "- `combFunc`: The function used to merge two intermediate results `U` between partitions.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD with the values for each key aggregated using the provided combine functions.\n",
    "\n",
    "**Note:**\n",
    "- The `zeroValue` serves as the initial or neutral element for the aggregation operation and is used when merging values within a partition or between partitions.\n",
    "- The `seqFunc` function is used to merge a value `V` into an intermediate result `U` within each partition. It modifies and returns its first argument to avoid memory allocation.\n",
    "- The `combFunc` function is used to merge two intermediate results `U` between partitions. It also modifies and returns its first argument to avoid memory allocation.\n",
    "- Both `seqFunc` and `combFunc` must be associative, meaning that the order of applying the functions does not affect the result.\n",
    "- The `aggregateByKey()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have the keys from the original RDD and the aggregated values based on the provided combine functions.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `aggregateByKey()` operation is commonly used for tasks where values for each key need to be aggregated or combined using custom combine functions, such as calculating sums, averages, or any other operation that can be expressed as associative functions with a neutral element.\n",
    "- It is important to choose appropriate `seqFunc` and `combFunc` functions and a suitable `zeroValue` to handle the merging of values for each key correctly and efficiently, modifying and returning the first argument to avoid unnecessary object creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "[('B', [(1, 1), (2, 4)]), ('A', [(3, 9), (4, 16), (5, 25)])]\n"
     ]
    }
   ],
   "source": [
    "# aggregateByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = [] # empty list is 'zero value' for append operation\n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
    "y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupByKey\">\n",
    "<img align=left src=\"images/pyspark-page54.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupByKey()**\n",
    "\n",
    "The `groupByKey()` method groups the values of each key in an RDD, returning a new RDD where each unique key is associated with a sequence of its corresponding values.\n",
    "\n",
    "**Returns:**\n",
    "A new RDD where each unique key is associated with an iterable sequence of its corresponding values.\n",
    "\n",
    "**Note:**\n",
    "- The resulting RDD is a key-value pair RDD, where the keys are the unique keys from the original RDD, and the values are sequences (iterables) containing all the corresponding values for each key.\n",
    "- The `groupByKey()` operation is a transformation operation in PySpark, meaning it is lazily evaluated.\n",
    "- It is important to note that the `groupByKey()` operation can lead to data skew, especially if there are keys with a large number of associated values. In such cases, it may be more efficient to use other operations like `reduceByKey()` or `aggregateByKey()` to perform aggregations on the values.\n",
    "- The `groupByKey()` operation is useful when you need to gather all the values for each unique key, such as when you want to perform further computations or analysis on a per-key basis. However, be cautious when working with large datasets and keys with high cardinality, as it can impact performance and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('B', 5), ('B', 4), ('A', 3), ('A', 2), ('A', 1)]\n",
      "[('B', [5, 4]), ('A', [3, 2, 1])]\n"
     ]
    }
   ],
   "source": [
    "# groupByKey\n",
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(\"x=\",x.collect())\n",
    "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByKey\">\n",
    "<img align=left src=\"images/pyspark-page46.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**countByKey()**\n",
    "\n",
    "The `countByKey()` method counts the number of elements for each key in an RDD and returns the result as a dictionary to the driver program.\n",
    "\n",
    "**Returns:**\n",
    "A dictionary where each unique key is mapped to the count of elements associated with it.\n",
    "\n",
    "**Note:**\n",
    "- The `countByKey()` operation triggers the execution of the RDD and collects the counts on the driver program. Therefore, it is important to consider the memory limitations of the driver program when using `countByKey()` on large datasets.\n",
    "- It is important to note that `countByKey()` returns the count for each unique key as a dictionary, which means the results are collected and stored in memory on the driver program. If the number of unique keys or the size of the resulting dictionary is large, it can impact memory usage on the driver program.\n",
    "- The `countByKey()` operation is useful when you need to determine the count of elements associated with each key in an RDD, such as in frequency analysis, data profiling, or for generating summary statistics based on the key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('B', 1), ('B', 2), ('A', 3), ('A', 4), ('A', 5)]\n",
      "defaultdict(<class 'int'>, {'B': 2, 'A': 3})\n"
     ]
    }
   ],
   "source": [
    "# countByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.countByKey()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByValue\">\n",
    "<img align=left src=\"images/pyspark-page36.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**countByValue()**\n",
    "\n",
    "The `countByValue()` method counts the occurrences of each unique value in an RDD and returns the result as a dictionary where the keys are the unique values and the values are the counts.\n",
    "\n",
    "**Returns:**\n",
    "A dictionary containing the count of each unique value in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `countByValue()` operation is an action in PySpark, meaning it triggers the execution of the RDD and collects the counts.\n",
    "- The `countByValue()` operation can be useful for analyzing the distribution or frequency of values in an RDD, such as when working with categorical or discrete data.\n",
    "- It is important to note that the `countByValue()` operation collects the counts to the driver program, so the resulting dictionary should fit into memory. For RDDs with a large number of unique values, consider using other methods like `reduceByKey()` or `aggregateByKey()` to perform distributed counting and aggregation.\n",
    "- The `countByValue()` operation does not guarantee a specific order of the values in the resulting dictionary.\n",
    "- The keys in the resulting dictionary correspond to the unique values in the RDD, and the values represent the count of each unique value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 1, 2, 3]\n",
      "defaultdict(<class 'int'>, {1: 2, 3: 2, 2: 1})\n"
     ]
    }
   ],
   "source": [
    "# countByValue\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.countByValue()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.count\">\n",
    "<img align=left src=\"images/pyspark-page29.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count()**\n",
    "\n",
    "The `count()` method returns the number of elements in an RDD.\n",
    "\n",
    "**Returns:**\n",
    "The number of elements in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The operation is performed on the RDD and returns the count as an integer value.\n",
    "- The `count()` operation is an action in PySpark, meaning it triggers the execution of the RDD and collects the count on the driver program.\n",
    "- If the RDD is empty, the `count()` operation will return 0.\n",
    "- The `count()` operation can be useful for tasks such as calculating the size of an RDD.\n",
    "- Keep in mind that invoking `count()` on a very large RDD can be time-consuming and resource-intensive. In such cases, consider using approximate methods like `countApprox()` or sampling techniques to estimate the count without processing the entire RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.count()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.max\">\n",
    "<img align=left src=\"images/pyspark-page26.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**max(key=None)**\n",
    "\n",
    "The `max()` method is used to find the maximum item in an RDD based on the elements' natural order or a custom key function.\n",
    "\n",
    "**Parameters:**\n",
    "- `key` (optional): A function used to generate a key for comparing the elements. By default, the elements' natural order is used.\n",
    "\n",
    "**Returns:**\n",
    "The maximum item in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- If the RDD contains elements with a natural order (e.g., numeric or string values), the maximum item is determined based on that order.\n",
    "- If a `key` function is provided, it is applied to each element to generate a key for comparison.\n",
    "- The `max()` operation is an action in PySpark, meaning it triggers the execution of the RDD to find the maximum item.\n",
    "- If the RDD is empty, the `max()` operation will throw an exception. Ensure that the RDD has at least one element before using `max()`.\n",
    "- If multiple elements have the maximum value, `max()` will return one of them, but the specific element chosen may not be deterministic.\n",
    "- The `max()` operation can be useful for finding the maximum value in an RDD or determining the maximum element based on a specific attribute or key.\n",
    "- If you want to find the maximum item based on a custom key function, provide the `key` parameter to transform the elements before comparison. The `key` function should return the attribute or value to be used for comparison.\n",
    "- Keep in mind that the `max()` operation requires comparing all elements in the RDD and can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2, 11]\n",
      "11\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# max\n",
    "x = sc.parallelize([1,3,2,11])\n",
    "y = x.max()\n",
    "z = x.max(key=str)\n",
    "print(\"x=\",x.collect())\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.min\">\n",
    "<img align=left src=\"images/pyspark-page27.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**min(key=None)**\n",
    "\n",
    "The `min()` method is used to find the minimum item in an RDD based on the elements' natural order or a custom key function.\n",
    "\n",
    "**Parameters:**\n",
    "- `key` (optional): A function used to generate a key for comparing the elements. By default, the elements' natural order is used.\n",
    "\n",
    "**Returns:**\n",
    "The minimum item in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- If the RDD contains elements with a natural order (e.g., numeric or string values), the minimum item is determined based on that order.\n",
    "- If a `key` function is provided, it is applied to each element to generate a key for comparison.\n",
    "- The `min()` operation is an action in PySpark, meaning it triggers the execution of the RDD to find the minimum item.\n",
    "- If the RDD is empty, the `min()` operation will throw an exception. Ensure that the RDD has at least one element before using `min()`.\n",
    "- If multiple elements have the minimum value, `min()` will return one of them, but the specific element chosen may not be deterministic.\n",
    "- The `min()` operation can be useful for finding the minimum value in an RDD or determining the minimum element based on a specific attribute or key.\n",
    "- If you want to find the minimum item based on a custom key function, provide the `key` parameter to transform the elements before comparison. The `key` function should return the attribute or value to be used for comparison.\n",
    "- Keep in mind that the `min()` operation requires comparing all elements in the RDD and can be computationally expensive for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# min\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.min()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sum\">\n",
    "<img align=left src=\"images/pyspark-page28.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sum()**\n",
    "\n",
    "The `sum()` method is used to add up the elements in an RDD.\n",
    "\n",
    "**Returns:**\n",
    "The sum of the elements in the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The operation is performed on the RDD and returns the sum as a numerical value.\n",
    "- The `sum()` operation is an action in PySpark, meaning it triggers the execution of the RDD and aggregates the elements to calculate the sum.\n",
    "- If the RDD is empty, the `sum()` operation will return `0`.\n",
    "- The `sum()` operation can be used with RDDs containing numerical values, such as integers or floating-point numbers.\n",
    "- It is important to note that the `sum()` operation requires accessing and aggregating all elements in the RDD, which can be computationally expensive and memory-intensive for large datasets. Ensure that the RDD can fit in memory and consider using approximate methods or distributed computing techniques if the dataset is too large to process entirely on a single machine.\n",
    "- If the RDD contains non-numeric elements or elements that cannot be added together, the `sum()` operation will throw an exception. Make sure that the RDD contains elements that can be summed or use appropriate transformations to filter or convert the elements before applying `sum()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sum()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mean\">\n",
    "<img align=left src=\"images/pyspark-page31.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mean()**\n",
    "\n",
    "The `mean()` method computes the mean (average) of the elements in an RDD.\n",
    "\n",
    "**Returns:**\n",
    "The mean of the RDD's elements as a floating-point value.\n",
    "\n",
    "**Note:**\n",
    "- The operation is performed on the RDD and returns the mean as a floating-point value.\n",
    "- The `mean()` operation is an action in PySpark, meaning it triggers the execution of the RDD and collects the necessary statistics to compute the mean.\n",
    "- If the RDD is empty, the `mean()` operation will return `None`.\n",
    "- The `mean()` operation can be useful for calculating the average of numerical values in an RDD, such as when working with datasets that represent measurements, statistics, or numerical features.\n",
    "- It is important to note that the `mean()` operation requires accessing and aggregating all elements in the RDD, which can be computationally expensive and memory-intensive for large datasets. Ensure that the RDD can fit in memory and consider using sampling techniques or approximate methods for calculating the mean if the dataset is too large to process entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2]\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# mean\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.mean()\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.variance\">\n",
    "<img align=left src=\"images/pyspark-page32.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**variance()**\n",
    "\n",
    "The `variance()` method computes the variance of the elements in an RDD. The variance is a measure of how spread out the values in the RDD are from the mean. It quantifies the average squared difference between each element and the mean of the RDD.\n",
    "\n",
    "**Returns:**\n",
    "The variance of the RDD's elements as a floating-point value.\n",
    "\n",
    "**Note:**\n",
    "- The `variance()` operation is an action in PySpark, meaning it triggers the execution of the RDD and collects the necessary statistics to compute the variance.\n",
    "- If the RDD is empty or contains only one element, the `variance()` operation will return `None` or `0`, respectively, as the variance is undefined in these cases.\n",
    "- The `variance()` operation can be useful for analyzing the distribution and variability of numerical values in an RDD, such as when working with datasets that represent measurements, statistics, or numerical features.\n",
    "- The variance is sensitive to outliers and can be strongly influenced by extreme values in the RDD. Consider preprocessing or filtering the data if outliers or extreme values are present and affecting the variance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2]\n",
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# variance\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.variance()  # divides by N\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.stdev\">\n",
    "<img align=left src=\"images/pyspark-page33.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stdev()**\n",
    "\n",
    "The `stdev()` method computes the standard deviation of the elements in an RDD. The standard deviation is a measure of the spread or dispersion of the values in the RDD. It quantifies the average deviation of each element from the mean of the RDD.\n",
    "\n",
    "**Returns:**\n",
    "The standard deviation of the RDD's elements as a floating-point value.\n",
    "\n",
    "**Note:**\n",
    "- The `stdev()` operation is an action in PySpark, meaning it triggers the execution of the RDD and collects the necessary statistics to compute the standard deviation.\n",
    "- If the RDD is empty or contains only one element, the `stdev()` operation will return `None` or `0`, respectively, as the standard deviation is undefined in these cases.\n",
    "- The `stdev()` operation can be useful for analyzing the dispersion and variability of numerical values in an RDD, such as when working with datasets that represent measurements, statistics, or numerical features.\n",
    "- The standard deviation is influenced by outliers and extreme values in the RDD. Consider preprocessing or filtering the data if outliers or extreme values are present and affecting the standard deviation calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 3, 2]\n",
      "0.816496580927726\n"
     ]
    }
   ],
   "source": [
    "# stdev\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.stdev()  # divides by N\n",
    "print(\"x=\",x.collect())\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join and combine RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.join\">\n",
    "<img align=left src=\"images/pyspark-page47.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**join(other, numPartitions=None)**\n",
    "\n",
    "The `join()` method returns an RDD containing all pairs of elements with matching keys in the source RDD (`self`) and another RDD (`other`). It performs a hash join across the cluster.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The other RDD to join with.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing all pairs of elements with matching keys, represented as tuples `(k, (v1, v2))`, where `(k, v1)` is in the source RDD and `(k, v2)` is in the other RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `join()` operation combines elements from the source RDD and the other RDD based on matching keys.\n",
    "- It performs a hash join, which is a type of join that utilizes hashing techniques to efficiently match elements with the same keys across partitions in a distributed manner.\n",
    "- The resulting RDD contains tuples where the key `k` is the common key and `(v1, v2)` represents the values associated with that key, with `v1` from the source RDD (`self`) and `v2` from the other RDD.\n",
    "- The `join()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `join()` operation can be used to combine datasets based on common keys, such as merging data from different sources, performing relational-style joins, or joining datasets for subsequent analysis or processing.\n",
    "- It is important to consider the data distribution and partitioning scheme of the source and other RDDs to ensure efficient execution of the join operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "y= [('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.join(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(\"y=\",y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.leftOuterJoin\">\n",
    "<img align=left src=\"images/pyspark-page48.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**leftOuterJoin(other, numPartitions=None)**\n",
    "\n",
    "The `leftOuterJoin()` method performs a left outer join between the source RDD (`self`) and another RDD (`other`). It combines the elements based on their keys, including all pairs from the source RDD and matching pairs from the other RDD. For each element `(k, v)` in the source RDD, the resulting RDD will contain pairs `(k, (v, w))` for `w` in the other RDD, or the pair `(k, (v, None))` if no elements in the other RDD have the key `k`.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The RDD to join with the source RDD.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing the joined pairs `(k, (v, w))` for elements `(k, v)` in the source RDD and elements `(k, w)` in the other RDD, or `(k, (v, None))` if no elements in the other RDD have the key `k`.\n",
    "\n",
    "**Note:**\n",
    "- The `leftOuterJoin()` operation is a transformation operation in PySpark, meaning it is lazily evaluated.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `leftOuterJoin()` operation is useful when you want to combine elements from two RDDs based on their keys, while preserving all elements from the source RDD and including `None` for keys that do not exist in the other RDD.\n",
    "- It is important to note that the `leftOuterJoin()` operation performs a join based on keys and does not consider the values of the elements. If you need to perform more complex operations or filtering based on both keys and values, you may need to use other operations like `join()` with appropriate transformations and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('C', (4, None)), ('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6))]\n"
     ]
    }
   ],
   "source": [
    "# leftOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.leftOuterJoin(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.rightOuterJoin\">\n",
    "<img align=left src=\"images/pyspark-page49.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**rightOuterJoin(other, numPartitions=None)**\n",
    "\n",
    "The `rightOuterJoin()` method performs a right outer join between the source RDD (`self`) and another RDD (`other`). It combines the elements based on their keys, including all pairs from the other RDD and matching pairs from the source RDD. For each element `(k, w)` in the other RDD, the resulting RDD will contain pairs `(k, (v, w))` for `v` in the source RDD, or the pair `(k, (None, w))` if no elements in the source RDD have the key `k`.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The RDD to join with the source RDD.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing the joined pairs `(k, (v, w))` for elements `(k, w)` in the other RDD and elements `(k, v)` in the source RDD, or `(k, (None, w))` if no elements in the source RDD have the key `k`.\n",
    "\n",
    "**Note:**\n",
    "- The `rightOuterJoin()` operation is a transformation operation in PySpark, meaning it is lazily evaluated.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `rightOuterJoin()` operation is useful when you want to combine elements from two RDDs based on their keys, while preserving all elements from the other RDD and including `None` for keys that do not exist in the source RDD.\n",
    "- It is important to note that the `rightOuterJoin()` operation performs a join based on keys and does not consider the values of the elements. If you need to perform more complex operations or filtering based on both keys and values, you may need to use other operations like `join()` with appropriate transformations and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "[('A', 8), ('B', 7), ('A', 6), ('D', 5)]\n",
      "[('B', (3, 7)), ('A', (2, 8)), ('A', (2, 6)), ('A', (1, 8)), ('A', (1, 6)), ('D', (None, 5))]\n"
     ]
    }
   ],
   "source": [
    "# rightOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.rightOuterJoin(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cartesian\">\n",
    "<img align=left src=\"images/pyspark-page17.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cartesian(other)**\n",
    "\n",
    "The `cartesian()` method returns an RDD representing the Cartesian product of the elements in the source RDD and another RDD. It generates pairs of all possible combinations where an element `a` is from the source RDD and an element `b` is from the other RDD.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The other RDD to form the Cartesian product with.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing all pairs of elements `(a, b)` where `a` is in the source RDD and `b` is in the other RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `cartesian()` operation generates pairs of all possible combinations between the elements in the source RDD and the elements in the other RDD.\n",
    "- The resulting RDD contains tuples `(a, b)` where `a` is an element from the source RDD and `b` is an element from the other RDD.\n",
    "- The Cartesian product generates every possible combination of elements, so the resulting RDD can be large and memory-intensive, especially if the source and other RDDs have many elements.\n",
    "- The `cartesian()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have a number of partitions determined by the default partitioning scheme or the partitioning scheme of the source RDD, depending on the version of PySpark being used.\n",
    "- The `cartesian()` operation is useful when you need to generate all possible pairs of elements between two RDDs, such as for cross-referencing, combining data from different sources, or performing extensive data exploration.\n",
    "- Care should be taken when using `cartesian()` on large RDDs, as the resulting RDD can be computationally expensive and memory-intensive due to the exponential growth in the number of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['A', 'B']\n",
      "['C', 'D']\n",
      "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian\n",
    "x = sc.parallelize(['A','B'])\n",
    "y = sc.parallelize(['C','D'])\n",
    "z = x.cartesian(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtract\">\n",
    "<img align=left src=\"images/pyspark-page61.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subtract(other, numPartitions=None)**\n",
    "\n",
    "The `subtract()` method returns an RDD containing the values from the source RDD (`self`) that are not present in another RDD (`other`).\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The RDD to subtract from the source RDD.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing the values from the source RDD that are not present in the other RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `subtract()` operation compares the values of the source RDD with the values of the other RDD and returns the values that are present in the source RDD but not in the other RDD.\n",
    "- The comparison is performed based on the equality of the elements in both RDDs.\n",
    "- The `subtract()` operation is a transformation operation in PySpark, meaning it is lazily evaluated.\n",
    "- If `numPartitions` is specified, it determines the number of partitions for the resulting RDD. Otherwise, the default partitioning scheme will be used.\n",
    "- The `subtract()` operation can be useful for filtering out specific values or removing duplicates between two RDDs.\n",
    "- The performance of the `subtract()` operation depends on the data distribution and partitioning scheme of the RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('C', 4), ('B', 3), ('A', 2), ('A', 1)]\n",
      "x= [('C', 8), ('A', 2), ('D', 2)]\n",
      "[('C', 4), ('B', 3), ('A', 1)]\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('C',8),('A',2),('D',2)])\n",
    "z = x.subtract(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(\"x=\",y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zip\">\n",
    "<img align=left src=\"images/pyspark-page65.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**zip(other)**\n",
    "\n",
    "The `zip()` method zips together two RDDs, returning an RDD of key-value pairs where the first element of each RDD is paired with the corresponding element from the other RDD. This operation assumes that both RDDs have the same number of partitions and the same number of elements in each partition.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The other RDD to zip with.\n",
    "\n",
    "**Returns:**\n",
    "An RDD of key-value pairs where the elements from each RDD are paired together.\n",
    "\n",
    "**Note:**\n",
    "- Both RDDs should have the same number of partitions and the same number of elements in each partition. The elements should be ordered such that the first element in each RDD corresponds to the second element in each RDD, and so on.\n",
    "- The resulting RDD contains key-value pairs, where the key is an element from the first RDD and the value is the corresponding element from the other RDD.\n",
    "- The `zip()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have the same number of partitions as the input RDDs.\n",
    "- The `zip()` operation is useful when you need to combine the elements of two RDDs that are related in some way, such as when you want to pair data from two different sources based on a common key or perform parallel processing on related datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= ['B', 'A', 'A']\n",
      "[66, 65, 65]\n",
      "[('B', 66), ('A', 65), ('A', 65)]\n"
     ]
    }
   ],
   "source": [
    "# zip\n",
    "x = sc.parallelize(['B','A','A'])\n",
    "y = x.map(lambda x: ord(x))  # zip expects x and y to have same #partitions and #elements/partition\n",
    "z = x.zip(y)\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithIndex\">\n",
    "<img align=left src=\"images/pyspark-page66.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**zipWithIndex()**\n",
    "\n",
    "The `zipWithIndex()` method zips an RDD with its element indices, creating a new RDD where each element is paired with its corresponding index. The ordering of the elements is based on the partition index and the ordering within each partition.\n",
    "\n",
    "**Returns:**\n",
    "An RDD where each element is paired with its index.\n",
    "\n",
    "**Note:**\n",
    "- The `zipWithIndex()` operation pairs each element in the RDD with its corresponding index.\n",
    "- The resulting RDD contains tuples `(element, index)` where the element is an element from the original RDD, and the index represents the position of the element in the RDD.\n",
    "- The ordering of the elements is determined first by the partition index and then by the ordering of items within each partition. Elements within the same partition will have contiguous indices, and the indices will be assigned in increasing order across partitions.\n",
    "- The `zipWithIndex()` operation is a transformation operation in PySpark, meaning it is lazily evaluated. It will not be executed until an action is triggered on the resulting RDD.\n",
    "- The resulting RDD will have the same number of partitions as the original RDD.\n",
    "- The `zipWithIndex()` operation is useful when you need to associate an index with each element in an RDD. It can be helpful for tasks such as ranking elements, creating unique identifiers, or tracking the order of elements in the RDD.\n",
    "- Be aware that using `zipWithIndex()` on a large RDD can introduce a performance overhead, especially if the RDD has a skewed distribution or a large number of elements, as it requires assigning an index to each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B'], ['A', 'A']]\n",
      "[('B', 0), ('A', 1), ('A', 2)]\n"
     ]
    }
   ],
   "source": [
    "# zipWithIndex\n",
    "x = sc.parallelize(['B','A','A'],2)\n",
    "y = x.zipWithIndex()\n",
    "print(x.glom().collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortByKey\">\n",
    "<img align=left src=\"images/pyspark-page14.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sortByKey(ascending=True, numPartitions=None, keyfunc=lambda)**\n",
    "\n",
    "The `sortByKey()` method sorts an RDD that consists of `(key, value)` pairs based on the keys. The sort order can be specified as ascending or descending. \n",
    "\n",
    "**Parameters:**\n",
    "- `ascending`: A Boolean value indicating whether the sorting should be in ascending order (`True`) or descending order (`False`). Default is `True`.\n",
    "- `numPartitions` (optional): The number of partitions to use for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "- `keyfunc` (optional): A function to extract a comparison key from each element in the RDD. This function is applied to the keys of the `(key, value)` pairs.\n",
    "\n",
    "**Returns:**\n",
    "An RDD containing the `(key, value)` pairs sorted by the keys.\n",
    "\n",
    "**Note:**\n",
    "- By default, the sorting is done in ascending order, but you can specify `ascending=False` to sort in descending order.\n",
    "- The `numPartitions` parameter determines the number of partitions for the resulting RDD. If not specified, the default partitioning scheme will be used.\n",
    "- The `keyfunc` parameter allows you to provide a custom function to extract a comparison key from each element in the RDD. This function is applied to the keys of the `(key, value)` pairs. The default behavior uses the keys as is.\n",
    "- The `sortByKey()` operation is a transformation operation in PySpark, meaning it is lazily evaluated.\n",
    "- Sorting is performed based on the keys, while preserving the association with the corresponding values.\n",
    "- If two keys are equal, the order of the corresponding values is preserved during sorting.\n",
    "- The `sortByKey()` operation can be useful for tasks where you need to sort an RDD of key-value pairs, such as finding the top values per key, performing range queries, or preparing data for further analysis or processing based on key order.\n",
    "- Depending on the data distribution and the size of the RDD, the `sortByKey()` operation can be computationally expensive and may require a significant amount of memory, especially if the RDD has a large number of keys or if the keys have high cardinality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [('B', 1), ('A', 2), ('C', 3)]\n",
      "[('A', 2), ('B', 1), ('C', 3)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey\n",
    "x = sc.parallelize([('B',1),('A',2),('C',3)])\n",
    "y = x.sortByKey()\n",
    "print(\"x=\",x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreach\">\n",
    "<img align=left src=\"images/pyspark-page20.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**foreach(f)**\n",
    "\n",
    "The `foreach()` method applies a function `f` to each element of the RDD. It executes the provided function on each element of the RDD in a distributed manner.\n",
    "\n",
    "**Parameters:**\n",
    "- `f`: The function to be applied to each element of the RDD.\n",
    "\n",
    "**Note:**\n",
    "- The `foreach()` operation applies the function `f` to each element of the RDD, allowing for custom processing or side effects on the elements.\n",
    "- The provided function `f` should be a void function or a function that does not return any value.\n",
    "- The `foreach()` operation is an action in PySpark, and it triggers the execution of the provided function on each element of the RDD.\n",
    "- The execution of `foreach()` is distributed across the worker nodes in the cluster, applying the function in parallel to each element.\n",
    "- The order of execution of the provided function on elements is not guaranteed, as it depends on the distributed processing and the available resources.\n",
    "- The `foreach()` operation does not return any result or new RDD. It is primarily used for performing operations or side effects on each element of the RDD, such as writing to an external system, updating shared variables, or performing other custom actions.\n",
    "- The provided function `f` should be carefully designed to ensure it is idempotent and does not have any dependencies on the order or specific execution of elements.\n",
    "- It is important to consider the potential side effects and the function's execution time when using `foreach()`, as it directly operates on the RDD elements and can impact the performance and behavior of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x= [1, 2, 3]\n",
      "None\n",
      "1\n",
      "2\n",
      "3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreach\n",
    "from __future__ import print_function\n",
    "x = sc.parallelize([1,2,3])\n",
    "def f(el):\n",
    "    '''side effect: append the current RDD elements to a file'''\n",
    "    f1=open(\"./foreachExample.txt\", 'a+') \n",
    "    print(el,file=f1)\n",
    "\n",
    "open('./foreachExample.txt', 'w').close()  # first clear the file contents\n",
    "\n",
    "y = x.foreach(f) # writes into foreachExample.txt\n",
    "\n",
    "print(\"x=\",x.collect())\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of foreachExample.txt\n",
    "with open(\"./foreachExample.txt\", \"r\") as foreachExample:\n",
    "    print (foreachExample.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db6cbf0fd79f8e79653fe7b0c50b956ca6e525ee712295da3c66f75e4fe96ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
