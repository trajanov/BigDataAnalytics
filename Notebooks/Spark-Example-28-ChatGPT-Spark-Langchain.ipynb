{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Agents in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('Spark-Example-30-ChatGPT') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a OpenAI API key on the following page: https://platform.openai.com/account/api-keys\n",
    "Open AI models: https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in deps\n",
    "import os \n",
    "from langchain.llms import OpenAI\n",
    "from pyspark.sql import SparkSession\n",
    "from langchain_experimental.agents import create_spark_dataframe_agent\n",
    "from working.apikey import apikey\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# create a file named apikey.py and add the following line: apikey = 'sk-xxxxxxxxxxxxxxxxxxxxxxxx' (where xxxx is your api key)\n",
    "os.environ['OPENAI_API_KEY'] = apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+---------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|    pickup_datetime|   dropoff_datetime|trip_time|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+-------------------+-------------------+---------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|07290D3599E7A0D62...|E7750A37CAB07D0DF...|2013-01-01 00:00:00|2013-01-01 00:02:00|      120|         0.44|      -73.956528|      40.716976|        -73.96244|       40.715008|         CSH|        3.5|      0.5|    0.5|       0.0|         0.0|         4.5|\n",
      "|22D70BF00EEB0ADC8...|3FF2709163DE7036F...|2013-01-01 00:02:00|2013-01-01 00:02:00|        0|          0.0|             0.0|            0.0|              0.0|             0.0|         CSH|       27.0|      0.0|    0.5|       0.0|         0.0|        27.5|\n",
      "|0EC22AAF491A8BD91...|778C92B26AE78A9EB...|2013-01-01 00:01:00|2013-01-01 00:03:00|      120|         0.71|      -73.973145|      40.752827|       -73.965897|       40.760445|         CSH|        4.0|      0.5|    0.5|       0.0|         0.0|         5.0|\n",
      "|1390FB380189DF6BB...|BE317B986700F63C4...|2013-01-01 00:01:00|2013-01-01 00:03:00|      120|         0.48|      -74.004173|      40.720947|       -74.003838|       40.726189|         CSH|        4.0|      0.5|    0.5|       0.0|         0.0|         5.0|\n",
      "|3B4129883A1D05BE8...|7077F9FD5AD649AEA...|2013-01-01 00:01:00|2013-01-01 00:03:00|      120|         0.61|      -73.987373|      40.724861|       -73.983772|       40.730995|         CRD|        4.0|      0.5|    0.5|       0.0|         0.0|         5.0|\n",
      "|5FAA7F69213D26A42...|00B7691D86D96AEBD...|2013-01-01 00:02:00|2013-01-01 00:03:00|       60|          0.0|             0.0|            0.0|              0.0|             0.0|         CRD|        2.5|      0.5|    0.5|      0.25|         0.0|        3.75|\n",
      "|DFBFA82ECA8F7059B...|CF8604E72D83840FB...|2013-01-01 00:02:00|2013-01-01 00:03:00|       60|         0.39|      -73.981544|      40.781475|       -73.979439|       40.784386|         CRD|        3.0|      0.5|    0.5|       0.7|         0.0|         4.7|\n",
      "|1E5F4C1CAE7AB3D06...|E0B2F618053518F24...|2013-01-01 00:03:00|2013-01-01 00:04:00|       60|          0.0|      -73.993973|      40.751266|              0.0|             0.0|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|         3.5|\n",
      "|468244D1361B8A3EB...|BB899DFEA9CC964B5...|2013-01-01 00:00:00|2013-01-01 00:04:00|      240|         1.71|      -73.955383|      40.779728|       -73.967758|       40.760326|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|5F78CC6D4ECD0541B...|B7567F5BFD558C665...|2013-01-01 00:00:00|2013-01-01 00:04:00|      240|         1.21|         -73.973|       40.79314|       -73.981453|       40.778465|         CRD|        6.0|      0.5|    0.5|       1.3|         0.0|         8.3|\n",
      "|6BA29E9A69B10F218...|ED368552102F12EA2...|2013-01-01 00:01:00|2013-01-01 00:04:00|      180|         0.74|      -73.971138|       40.75898|       -73.972206|       40.752502|         CRD|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "|75C90377AB710B047...|00B7691D86D96AEBD...|2013-01-01 00:03:00|2013-01-01 00:04:00|       60|          0.0|             0.0|            0.0|              0.0|             0.0|         CRD|        3.0|      0.5|    0.5|      0.07|         0.0|        4.07|\n",
      "|C306CAC565429C128...|E255D5DFB9A967B9D...|2013-01-01 00:01:00|2013-01-01 00:04:00|      180|         0.84|      -73.942841|      40.797031|        -73.93454|       40.797314|         CSH|        4.5|      0.5|    0.5|       0.0|         0.0|         5.5|\n",
      "|C4D6E189EF44EB83E...|95B5B3CDA9EDC88AA...|2013-01-01 00:03:00|2013-01-01 00:04:00|       60|          0.0|      -73.989189|      40.721924|              0.0|             0.0|         CSH|        2.5|      0.5|    0.5|       0.0|         0.0|         3.5|\n",
      "|DD467ED2E7DDB5C8B...|1D7876BD692018A5B...|2013-01-01 00:01:00|2013-01-01 00:04:00|      180|         0.95|      -73.976753|      40.750706|       -73.990089|       40.750729|         CSH|        5.0|      0.5|    0.5|       0.0|         0.0|         6.0|\n",
      "|F8A0B52B22BB58B3C...|00B7691D86D96AEBD...|2013-01-01 00:03:00|2013-01-01 00:04:00|       60|          0.0|      -73.937637|      40.758369|       -73.937607|        40.75835|         CRD|        2.5|      0.5|    0.5|      0.25|         0.0|        3.75|\n",
      "|120E700FE35B2DDBE...|3EDDD1433E2276DF9...|2013-01-01 00:03:00|2013-01-01 00:05:00|      120|         0.52|      -73.981972|      40.752525|       -73.985313|       40.747738|         CSH|        4.0|      0.5|    0.5|       0.0|         0.0|         5.0|\n",
      "|256C6A3CA47F81497...|8FA961982C3FEF438...|2013-01-01 00:00:00|2013-01-01 00:05:00|      300|         0.13|      -73.969841|      40.797359|       -73.961899|       40.812466|         CSH|        6.5|      0.5|    0.5|       0.0|         0.0|         7.5|\n",
      "|655E773C92FA44635...|B6EAE07E2AD023B38...|2013-01-01 00:02:00|2013-01-01 00:05:00|      180|         1.55|      -74.003197|      40.733032|       -74.012985|       40.717377|         CRD|        6.0|      0.5|    0.5|      1.62|         0.0|        8.62|\n",
      "|76942C3205E17D7E7...|25BA06A87905667AA...|2013-01-01 00:00:00|2013-01-01 00:05:00|      300|         0.61|      -73.955925|      40.781887|       -73.963181|       40.777832|         CSH|        5.0|      0.5|    0.5|       0.0|         0.0|         6.0|\n",
      "+--------------------+--------------------+-------------------+-------------------+---------+-------------+----------------+---------------+-----------------+----------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_file_path = \"data\\\\taxi-data-sorted-verysmall-header.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Spark DataFrame LangChain agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "# set llm model of openai to chatgpt 4.0\n",
    "llm = ChatOpenAI(model_name = \"gpt-4.1-2025-04-14\", temperature=0)\n",
    "#llm = ChatOpenAI(model_name = \"gpt-4o-2024-11-20\", temperature=0)\n",
    "#llm = OpenAI(temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4.1-2025-04-14'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_spark_dataframe_agent(llm=llm, df=df, verbose=True, \n",
    "                                     handle_parsing_errors=True,\n",
    "                                      allow_dangerous_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the number of rows in a Spark DataFrame, I should use the `count()` method.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.count()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m10000\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: There are 10,000 rows in the dataframe.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There are 10,000 rows in the dataframe.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"how many rows are there?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the top ten taxis (medallions) with the largest number of drivers (hack_license), I need to group the dataframe by 'medallion', count the distinct 'hack_license' for each, order by this count descending, and select the top 10.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupBy(\"medallion\").agg({\"hack_license\": \"countDistinct\"}).withColumnRenamed(\"count(DISTINCT hack_license)\", \"num_drivers\").orderBy(\"num_drivers\", ascending=False).limit(10).show()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAnalysisException: Undefined function: 'countdistinct'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The error is because the aggregation function should be 'countDistinct' (with correct case) from pyspark.sql.functions, not as a string in agg. I should import countDistinct and use it properly in the aggregation.\n",
      "Action: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.sql.functions import countDistinct\n",
      "df.groupBy(\"medallion\").agg(countDistinct(\"hack_license\").alias(\"num_drivers\")).orderBy(\"num_drivers\", ascending=False).limit(10).show()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+--------------------+-----------+\n",
      "|           medallion|num_drivers|\n",
      "+--------------------+-----------+\n",
      "|DE85355130669A290...|          2|\n",
      "|582CAF46446FA0332...|          2|\n",
      "|F899990AB4A6582D6...|          2|\n",
      "|753BC0484097BB236...|          1|\n",
      "|80F732B990A7E3763...|          1|\n",
      "|846DFE2D59F6E76EC...|          1|\n",
      "|BEA5A07E7B365D7F6...|          1|\n",
      "|EF882BDAF03D41517...|          1|\n",
      "|E1BD31C1BF8DDCFCB...|          1|\n",
      "|A02946A94C960AF04...|          1|\n",
      "+--------------------+-----------+\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The top ten taxis (medallions) that have had the largest number of drivers (distinct hack_license values) are:\n",
      "\n",
      "1. DE85355130669A290... (2 drivers)\n",
      "2. 582CAF46446FA0332... (2 drivers)\n",
      "3. F899990AB4A6582D6... (2 drivers)\n",
      "4. 753BC0484097BB236... (1 driver)\n",
      "5. 80F732B990A7E3763... (1 driver)\n",
      "6. 846DFE2D59F6E76EC... (1 driver)\n",
      "7. BEA5A07E7B365D7F6... (1 driver)\n",
      "8. EF882BDAF03D41517... (1 driver)\n",
      "9. E1BD31C1BF8DDCFCB... (1 driver)\n",
      "10. A02946A94C960AF04... (1 driver)\n",
      "\n",
      "The top three medallions each had 2 drivers, while the rest had 1 driver each.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The top ten taxis (medallions) that have had the largest number of drivers (distinct hack_license values) are:\\n\\n1. DE85355130669A290... (2 drivers)\\n2. 582CAF46446FA0332... (2 drivers)\\n3. F899990AB4A6582D6... (2 drivers)\\n4. 753BC0484097BB236... (1 driver)\\n5. 80F732B990A7E3763... (1 driver)\\n6. 846DFE2D59F6E76EC... (1 driver)\\n7. BEA5A07E7B365D7F6... (1 driver)\\n8. EF882BDAF03D41517... (1 driver)\\n9. E1BD31C1BF8DDCFCB... (1 driver)\\n10. A02946A94C960AF04... (1 driver)\\n\\nThe top three medallions each had 2 drivers, while the rest had 1 driver each.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"The top ten taxis that have had the largest number of drivers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the top 10 best drivers by average earned money per minute, I need to group by the driver identifier (likely \"hack_license\"), calculate the average of (total_amount / trip_time) for each driver, and then order by this value in descending order, taking the top 10.\n",
      "Action: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.sql import functions as F\n",
      "\n",
      "df.withColumn(\n",
      "    \"money_per_minute\", F.col(\"total_amount\") / (F.col(\"trip_time\") / 60)\n",
      ").groupBy(\n",
      "    \"hack_license\"\n",
      ").agg(\n",
      "    F.avg(\"money_per_minute\").alias(\"avg_money_per_minute\")\n",
      ").orderBy(\n",
      "    F.desc(\"avg_money_per_minute\")\n",
      ").limit(10).show()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+--------------------+--------------------+\n",
      "|        hack_license|avg_money_per_minute|\n",
      "+--------------------+--------------------+\n",
      "|7890F735BB4C3BB07...|               73.75|\n",
      "|1FE259D800533F3B8...|               36.75|\n",
      "|AC8F60CA2C993BE4A...|          30.9453125|\n",
      "|3AAB0BF924B7A2F16...|   18.23777777777778|\n",
      "|11DDF1FD5B2C815FA...|   9.357142857142858|\n",
      "|371910291B554B982...|               7.025|\n",
      "|2C7D2EFF7686CFED6...|   6.769360269360269|\n",
      "|BABDC5E896D679E9F...|   6.013888888888889|\n",
      "|01D771BD290991D50...|   5.768710317460317|\n",
      "|911299BE5F3FF1DFD...|   5.571428571428571|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The top 10 best drivers in terms of their average earned money per minute spent carrying a customer (by hack_license) are:\n",
      "\n",
      "1. 7890F735BB4C3BB07... — 73.75\n",
      "2. 1FE259D800533F3B8... — 36.75\n",
      "3. AC8F60CA2C993BE4A... — 30.95\n",
      "4. 3AAB0BF924B7A2F16... — 18.24\n",
      "5. 11DDF1FD5B2C815FA... — 9.36\n",
      "6. 371910291B554B982... — 7.03\n",
      "7. 2C7D2EFF7686CFED6... — 6.77\n",
      "8. BABDC5E896D679E9F... — 6.01\n",
      "9. 01D771BD290991D50... — 5.77\n",
      "10. 911299BE5F3FF1DFD... — 5.57\n",
      "\n",
      "(Values are average dollars earned per minute, rounded to two decimal places.)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The top 10 best drivers in terms of their average earned money per minute spent carrying a customer (by hack_license) are:\\n\\n1. 7890F735BB4C3BB07... — 73.75\\n2. 1FE259D800533F3B8... — 36.75\\n3. AC8F60CA2C993BE4A... — 30.95\\n4. 3AAB0BF924B7A2F16... — 18.24\\n5. 11DDF1FD5B2C815FA... — 9.36\\n6. 371910291B554B982... — 7.03\\n7. 2C7D2EFF7686CFED6... — 6.77\\n8. BABDC5E896D679E9F... — 6.01\\n9. 01D771BD290991D50... — 5.77\\n10. 911299BE5F3FF1DFD... — 5.57\\n\\n(Values are average dollars earned per minute, rounded to two decimal places.)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who are the top 10 best drivers are in terms of their average earned money per minute spent carrying a customer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this, I need to count the number of rows where payment_type is 'CSH' (cash) and where it is an electronic card (likely 'CRD'), then calculate the percentage of each over the total number of rows.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.groupBy(\"payment_type\").count().show()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+------------+-----+\n",
      "|payment_type|count|\n",
      "+------------+-----+\n",
      "|         CSH| 5998|\n",
      "|         CRD| 4000|\n",
      "|         UNK|    2|\n",
      "+------------+-----+\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Now I need to calculate the percentage of 'CSH' and 'CRD' payments out of the total number of rows (5998 + 4000 + 2 = 10000). Then, I will compute the percentages for cash and electronic card payments.\n",
      "Action: python_repl_ast\n",
      "Action Input: (5998/10000)*100, (4000/10000)*100\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(59.98, 40.0)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: 59.98% of taxi customers pay with cash, and 40.0% use electronic cards.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'59.98% of taxi customers pay with cash, and 40.0% use electronic cards.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"How many percent of taxi customers pay with cash, and how many percent use electronic cards?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To find the top-10 most efficient taxi drivers based on earned money per mile, I need to:\n",
      "1. Group the dataframe by driver (hack_license).\n",
      "2. For each driver, sum the total earned money (total_amount) and total miles (trip_distance).\n",
      "3. Calculate the efficiency as total_amount / total_trip_distance for each driver.\n",
      "4. Order by efficiency in descending order and select the top 10.\n",
      "\n",
      "Action: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.sql import functions as F\n",
      "\n",
      "df_efficiency = (\n",
      "    df.groupBy(\"hack_license\")\n",
      "      .agg(\n",
      "          F.sum(\"total_amount\").alias(\"total_earned\"),\n",
      "          F.sum(\"trip_distance\").alias(\"total_miles\")\n",
      "      )\n",
      "      .withColumn(\"efficiency\", F.col(\"total_earned\") / F.col(\"total_miles\"))\n",
      "      .orderBy(F.desc(\"efficiency\"))\n",
      "      .limit(10)\n",
      ")\n",
      "\n",
      "df_efficiency.show()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+--------------------+-----------------+-------------------+------------------+\n",
      "|        hack_license|     total_earned|        total_miles|        efficiency|\n",
      "+--------------------+-----------------+-------------------+------------------+\n",
      "|ED506323B59B1F557...|             22.5|               0.02|            1125.0|\n",
      "|EA14140EF36CBCFF0...|             63.0|               0.06|            1050.0|\n",
      "|7F364376C78806B0E...|             10.5|               0.02|             525.0|\n",
      "|4E9DA457D6EC727AC...|             11.5|               0.03|383.33333333333337|\n",
      "|2EE83D77768FEE22C...|              6.5|               0.02|             325.0|\n",
      "|C2E67E72B7D6DFA4D...|              6.0|               0.04|             150.0|\n",
      "|E0C54FD4238BC93F3...|70.25999999999999|0.47000000000000003|149.48936170212764|\n",
      "|8994FB75C5F11A994...|              7.0|               0.05|             140.0|\n",
      "|0760304D0CAEC6949...|             46.0|               0.33|139.39393939393938|\n",
      "|47C715F18BE3EA6A9...|             20.5|               0.19|107.89473684210526|\n",
      "+--------------------+-----------------+-------------------+------------------+\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The top-10 most efficient taxi drivers based on earned money per mile are:\n",
      "\n",
      "1. ED506323B59B1F557... with efficiency 1125.0\n",
      "2. EA14140EF36CBCFF0... with efficiency 1050.0\n",
      "3. 7F364376C78806B0E... with efficiency 525.0\n",
      "4. 4E9DA457D6EC727AC... with efficiency 383.33\n",
      "5. 2EE83D77768FEE22C... with efficiency 325.0\n",
      "6. C2E67E72B7D6DFA4D... with efficiency 150.0\n",
      "7. E0C54FD4238BC93F3... with efficiency 149.49\n",
      "8. 8994FB75C5F11A994... with efficiency 140.0\n",
      "9. 0760304D0CAEC6949... with efficiency 139.39\n",
      "10. 47C715F18BE3EA6A9... with efficiency 107.89\n",
      "\n",
      "These drivers earned the most money per mile driven.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The top-10 most efficient taxi drivers based on earned money per mile are:\\n\\n1. ED506323B59B1F557... with efficiency 1125.0\\n2. EA14140EF36CBCFF0... with efficiency 1050.0\\n3. 7F364376C78806B0E... with efficiency 525.0\\n4. 4E9DA457D6EC727AC... with efficiency 383.33\\n5. 2EE83D77768FEE22C... with efficiency 325.0\\n6. C2E67E72B7D6DFA4D... with efficiency 150.0\\n7. E0C54FD4238BC93F3... with efficiency 149.49\\n8. 8994FB75C5F11A994... with efficiency 140.0\\n9. 0760304D0CAEC6949... with efficiency 139.39\\n10. 47C715F18BE3EA6A9... with efficiency 107.89\\n\\nThese drivers earned the most money per mile driven.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "agent.run(\"Find the top-10 most efficient taxi divers based on earned money per mile. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To answer this, I need to calculate the mean and median of the \"tip_amount\" column in the Spark dataframe `df`. For the mean, I can use the `mean` function from `pyspark.sql.functions`. For the median, Spark does not have a direct median function, but I can use the `approxQuantile` method to get the median (50th percentile).\n",
      "Action: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.sql import functions as F\n",
      "\n",
      "mean_tip = df.select(F.mean(\"tip_amount\")).first()[0]\n",
      "median_tip = df.approxQuantile(\"tip_amount\", [0.5], 0.01)[0]\n",
      "(mean_tip, median_tip)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(0.9702309999999977, 0.0)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: The mean of tip amount is approximately 0.97, and the median of tip amount is 0.0.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The mean of tip amount is approximately 0.97, and the median of tip amount is 0.0.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What are the mean, median of tip amount?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To create a linear model to predict total paid amount (`total_amount`) based on duration, I need to use Spark MLlib. The duration is given in seconds as `trip_time`, so 10 minutes = 600 seconds. The steps are: select relevant columns, assemble features, fit a linear regression model, and use it to predict for 600 seconds.\n",
      "Action: import necessary libraries and select the relevant columns\n",
      "Action Input: \n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "\n",
      "df_model = df.select(\"trip_time\", \"total_amount\")\n",
      "df_model.show(5)\u001b[0m\n",
      "Observation: import necessary libraries and select the relevant columns is not a valid tool, try one of [python_repl_ast].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: select the relevant columns to inspect the data\n",
      "Action Input: df.select(\"trip_time\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: select the relevant columns to inspect the data is not a valid tool, try one of [python_repl_ast].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl_ast\n",
      "Action Input: df.select(\"trip_time\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+---------+------------+\n",
      "|trip_time|total_amount|\n",
      "+---------+------------+\n",
      "|      120|         4.5|\n",
      "|        0|        27.5|\n",
      "|      120|         5.0|\n",
      "|      120|         5.0|\n",
      "|      120|         5.0|\n",
      "+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: Assemble the features into a vector column for MLlib\n",
      "Action Input: \n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "assembler = VectorAssembler(inputCols=[\"trip_time\"], outputCol=\"features\")\n",
      "df_features = assembler.transform(df.select(\"trip_time\", \"total_amount\"))\n",
      "df_features.select(\"features\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: Assemble the features into a vector column for MLlib is not a valid tool, try one of [python_repl_ast].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "assembler = VectorAssembler(inputCols=[\"trip_time\"], outputCol=\"features\")\n",
      "df_features = assembler.transform(df.select(\"trip_time\", \"total_amount\"))\n",
      "df_features.select(\"features\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+--------+------------+\n",
      "|features|total_amount|\n",
      "+--------+------------+\n",
      "| [120.0]|         4.5|\n",
      "|   [0.0]|        27.5|\n",
      "| [120.0]|         5.0|\n",
      "| [120.0]|         5.0|\n",
      "| [120.0]|         5.0|\n",
      "+--------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: Fit a linear regression model using the features and total_amount columns\n",
      "Action Input: \n",
      "from pyspark.ml.regression import LinearRegression\n",
      "\n",
      "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
      "lr_model = lr.fit(df_features)\n",
      "lr_model.coefficients, lr_model.intercept\u001b[0m\n",
      "Observation: Fit a linear regression model using the features and total_amount columns is not a valid tool, try one of [python_repl_ast].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.ml.regression import LinearRegression\n",
      "\n",
      "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
      "lr_model = lr.fit(df_features)\n",
      "lr_model.coefficients, lr_model.intercept\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(DenseVector([0.0153]), 3.2899506825250358)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: The linear model to predict total paid amount is: total_amount = 0.0153 * trip_time + 3.29. For a duration of 10 minutes (600 seconds), the predicted total paid amount is:\n",
      "\n",
      "total_amount = 0.0153 * 600 + 3.29 ≈ 9.18 + 3.29 = 12.47\n",
      "\n",
      "So, the predicted total paid amount for a 10-minute ride is approximately $12.47.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The linear model to predict total paid amount is: total_amount = 0.0153 * trip_time + 3.29. For a duration of 10 minutes (600 seconds), the predicted total paid amount is:\\n\\ntotal_amount = 0.0153 * 600 + 3.29 ≈ 9.18 + 3.29 = 12.47\\n\\nSo, the predicted total paid amount for a 10-minute ride is approximately $12.47.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Create a linear model to predict the total paid amounts of Taxi rides based on duration. What will be the predicted total paid amount if the duration is 10 minutes?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: To create a linear model in PySpark, I need to:\n",
      "1. Select the relevant columns: trip_time, trip_distance, fare_amount, tolls_amount, and total_amount.\n",
      "2. Assemble the features into a single vector column using VectorAssembler.\n",
      "3. Split the data into training and test sets.\n",
      "4. Fit a LinearRegression model using the features to predict total_amount.\n",
      "5. Show the model summary.\n",
      "\n",
      "Action: I will start by selecting the relevant columns.\n",
      "Action Input: df.select(\"trip_time\", \"trip_distance\", \"fare_amount\", \"tolls_amount\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: I will start by selecting the relevant columns. is not a valid tool, try one of [python_repl_ast].\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl_ast\n",
      "Action Input: df.select(\"trip_time\", \"trip_distance\", \"fare_amount\", \"tolls_amount\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+---------+-------------+-----------+------------+------------+\n",
      "|trip_time|trip_distance|fare_amount|tolls_amount|total_amount|\n",
      "+---------+-------------+-----------+------------+------------+\n",
      "|      120|         0.44|        3.5|         0.0|         4.5|\n",
      "|        0|          0.0|       27.0|         0.0|        27.5|\n",
      "|      120|         0.71|        4.0|         0.0|         5.0|\n",
      "|      120|         0.48|        4.0|         0.0|         5.0|\n",
      "|      120|         0.61|        4.0|         0.0|         5.0|\n",
      "+---------+-------------+-----------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl_ast\n",
      "Action Input: \n",
      "from pyspark.ml.feature import VectorAssembler\n",
      "\n",
      "assembler = VectorAssembler(\n",
      "    inputCols=[\"trip_time\", \"trip_distance\", \"fare_amount\", \"tolls_amount\"],\n",
      "    outputCol=\"features\"\n",
      ")\n",
      "df_features = assembler.transform(df.select(\"trip_time\", \"trip_distance\", \"fare_amount\", \"tolls_amount\", \"total_amount\"))\n",
      "df_features.select(\"features\", \"total_amount\").show(5)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m+--------------------+------------+\n",
      "|            features|total_amount|\n",
      "+--------------------+------------+\n",
      "|[120.0,0.44,3.5,0.0]|         4.5|\n",
      "|      (4,[2],[27.0])|        27.5|\n",
      "|[120.0,0.71,4.0,0.0]|         5.0|\n",
      "|[120.0,0.48,4.0,0.0]|         5.0|\n",
      "|[120.0,0.61,4.0,0.0]|         5.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mAction: python_repl_ast\n",
      "Action Input: \n",
      "# Split the data into training and test sets\n",
      "train_data, test_data = df_features.randomSplit([0.8, 0.2], seed=42)\n",
      "from pyspark.ml.regression import LinearRegression\n",
      "\n",
      "# Create and fit the linear regression model\n",
      "lr = LinearRegression(featuresCol=\"features\", labelCol=\"total_amount\")\n",
      "lr_model = lr.fit(train_data)\n",
      "\n",
      "# Show the model summary\n",
      "training_summary = lr_model.summary\n",
      "training_summary.r2, training_summary.coefficients, training_summary.intercept\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mAttributeError: 'LinearRegressionTrainingSummary' object has no attribute 'coefficients'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: The error indicates that the coefficients and intercept are not accessed from the summary object, but directly from the model object. I should access lr_model.coefficients and lr_model.intercept instead.\n",
      "Action: python_repl_ast\n",
      "Action Input: lr_model.coefficients, lr_model.intercept\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m(DenseVector([-0.0003, 0.0099, 1.1024, 0.9944]), 0.9829663836225442)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mFinal Answer: \n",
      "A linear regression model was created to predict total_amount using trip_time, trip_distance, fare_amount, and tolls_amount. The learned coefficients are:\n",
      "- trip_time: -0.0003\n",
      "- trip_distance: 0.0099\n",
      "- fare_amount: 1.1024\n",
      "- tolls_amount: 0.9944\n",
      "\n",
      "The model intercept is 0.983. This means the model predicts total_amount as:\n",
      "total_amount ≈ 0.983 - 0.0003 × trip_time + 0.0099 × trip_distance + 1.1024 × fare_amount + 0.9944 × tolls_amount\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A linear regression model was created to predict total_amount using trip_time, trip_distance, fare_amount, and tolls_amount. The learned coefficients are:\\n- trip_time: -0.0003\\n- trip_distance: 0.0099\\n- fare_amount: 1.1024\\n- tolls_amount: 0.9944\\n\\nThe model intercept is 0.983. This means the model predicts total_amount as:\\ntotal_amount ≈ 0.983 - 0.0003 × trip_time + 0.0099 × trip_distance + 1.1024 × fare_amount + 0.9944 × tolls_amount'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Create a linear model to predict the total paid amounts of Taxi rides based on duration, trip_distance, fare_amount, and tolls_amount.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
