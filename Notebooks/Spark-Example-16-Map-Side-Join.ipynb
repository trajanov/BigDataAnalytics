{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Operation in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f']\n",
      "['AA', 'BB', 'DD']\n",
      "[1, 3, 2, 2, 3, 1]\n",
      "[1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Let us create some RDDs\n",
    "valueRDDA = sc.parallelize(['a', 'b', 'c', 'd', 'e', 'f'])\n",
    "valueRDDB = sc.parallelize(['AA', 'BB', 'DD'])\n",
    "\n",
    "rddB = sc.parallelize([1, 3, 2, 2, 3, 1])\n",
    "rddC = sc.parallelize([1, 2, 4])\n",
    "\n",
    "print(valueRDDA.collect())\n",
    "print(valueRDDB.collect())\n",
    "print(rddB.collect())\n",
    "print(rddC.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD 1 :  [(1, 'a'), (3, 'b'), (2, 'c'), (2, 'd'), (3, 'e'), (1, 'f')]\n",
      "RDD 2 :  [(1, 'AA'), (2, 'BB'), (4, 'DD')]\n"
     ]
    }
   ],
   "source": [
    "# Now, we can zip these RDDs to create new RDDs with different keys\n",
    "\n",
    "rdd1 = rddB.zip(valueRDDA)\n",
    "rdd2 = rddC.zip(valueRDDB)\n",
    "\n",
    "print(\"RDD 1 : \", rdd1.collect())\n",
    "print(\"RDD 2 : \", rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ('c', 'BB')), (2, ('d', 'BB')), (1, ('a', 'AA')), (1, ('f', 'AA'))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd1.join(rdd2)\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map-Side Join Operation\n",
    "\n",
    "If we are joining a small RDD with a large RDD and the small RDD is so small that it can fit into the main memory of a single executor, then we can convert the Join operation into a map operation. This is then called Map-Side Join operation. \n",
    "\n",
    "1. Collect the small RDD as Map (a dict in python)\n",
    "2. Broadcast the small dictionary so that a copy of it is avaiable on each worker. \n",
    "3. Do the map to run the join operation instead of the actual join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'AA', 2: 'BB', 4: 'DD'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, ('a', 'AA')), (2, ('c', 'BB')), (2, ('d', 'BB')), (1, ('f', 'AA'))]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the small RDD as Map (a dict in python)\n",
    "my_small_Data = rdd2.collectAsMap()\n",
    "\n",
    "print(my_small_Data)\n",
    "\n",
    "# Now we broad cast this to all worker nodes. \n",
    "sc.broadcast(my_small_Data)\n",
    "\n",
    "# We can do a simple map on it. \n",
    "rdd3=rdd1.map(lambda x: \n",
    "        (x[0], (x[1], my_small_Data.get(x[0]))) if x[0] in  my_small_Data.keys() else None )\\\n",
    "        .filter(lambda x: x!=None)\n",
    "\n",
    "rdd3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining a large and a medium size RDD\n",
    "\n",
    "In case that the RDD is large so that it can not fit into the memory, then maybe the keys only can fit into the memory. This would allow us to keep the keys of the medium size RDD in memory and use it to reduce the size of the large RDD and then run the join operation on it. \n",
    "\n",
    "1. Collect the Keys of the Medium size RDD into a set of keys \n",
    "2. Use the keys to filter the large RDD and reduce the size of it\n",
    "3. Then run the join on the smaller RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 4}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2, ('c', 'BB')), (2, ('d', 'BB')), (1, ('a', 'AA')), (1, ('f', 'AA'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Collect the Keys of the Medium size RDD into a set of keys\n",
    "keys = set(rdd2.map(lambda x: x[0]).collect())\n",
    "\n",
    "print(keys)\n",
    "# Now we broadcast keys to all worker nodes. \n",
    "sc.broadcast(keys)\n",
    "\n",
    "# We can do a simple map on it. \n",
    "rdd3 = rdd1.map(lambda x: (x[0], x[1] if x[0] in  keys else None ))\\\n",
    "           .filter(lambda x: x[1]!=None)\n",
    "\n",
    "# Now we can run the join operation on a smaller RDD\n",
    "rdd4=rdd3.join(rdd2)\n",
    "\n",
    "rdd4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db6cbf0fd79f8e79653fe7b0c50b956ca6e525ee712295da3c66f75e4fe96ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
