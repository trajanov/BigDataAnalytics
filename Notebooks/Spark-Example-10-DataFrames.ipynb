{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark DataFrames\n",
    "This notebook contains examples of PySpark DataFrame operations. PySpark is a powerful tool for big data processing and analysis, and DataFrames are a key component of PySpark.\n",
    "\n",
    "In this notebook, we will explore various DataFrame operations such as creating DataFrames, reading from external sources, filtering, aggregating, and more.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data Frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**createDataFrame(data, schema=None, samplingRatio=None, verifySchema=True)**\n",
    "\n",
    "The `createDataFrame()` function is a fundamental method to create a DataFrame in PySpark and allows for seamless integration with various data sources and data structures.\n",
    "\n",
    "**Parameters:**\n",
    "- `data`: An RDD, a list, or a pandas DataFrame containing the data to create the DataFrame.\n",
    "- `schema`: Optional. The schema of the DataFrame. It can be a `pyspark.sql.types.DataType`, a datatype string, or a list of column names. If `schema` is None, the schema will be inferred from the data. The schema determines the column names and types of the DataFrame.\n",
    "- `samplingRatio`: Optional. The sample ratio of rows used for inferring the schema. It is only used when the schema needs to be inferred.\n",
    "- `verifySchema`: Optional. Specifies whether to verify the data types of every row against the schema. If set to `True` (default), the data types will be verified.\n",
    "\n",
    "**Returns:**\n",
    "A DataFrame created from the provided data and schema.\n",
    "\n",
    "**Note:**\n",
    "- The `createDataFrame()` function is a versatile method to create a DataFrame in PySpark from different data sources.\n",
    "- If the schema is provided, it must match the actual data. If the schema is not a `pyspark.sql.types.StructType`, it will be wrapped into a `pyspark.sql.types.StructType` with a single field named \"value\". Each record will also be wrapped into a tuple that can be converted to a row later.\n",
    "- If schema inference is needed, the `samplingRatio` parameter determines the ratio of rows used for inferring the schema. By default, the first row will be used.\n",
    "- The `verifySchema` parameter specifies whether to verify the data types of every row against the schema. It is enabled by default.\n",
    "- This function provides flexibility in creating a DataFrame with specified or inferred schemas, enabling seamless integration with different data representations and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+\n",
      "|drinker|        beer|score|\n",
      "+-------+------------+-----+\n",
      "|  Chris|    Berliner|    5|\n",
      "|  Peter|   Bud Light|    9|\n",
      "|   John|Corona Extra|    6|\n",
      "+-------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From list\n",
    "a = [('Chris', 'Berliner', 5), ('Peter', 'Bud Light', 9), ('John', 'Corona Extra', 6)]\n",
    "df = spark.createDataFrame(a, ['drinker', 'beer', 'score'])  \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[drinker: string, beer: string, score: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+---+\n",
      "|   _1|          _2| _3|\n",
      "+-----+------------+---+\n",
      "|Chris|    Berliner|  5|\n",
      "|Peter|   Bud Light|  9|\n",
      "| John|Corona Extra|  6|\n",
      "+-----+------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame from RDD\n",
    "rdd = sc.parallelize(a)\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+\n",
      "|drinker|        beer|score|\n",
      "+-------+------------+-----+\n",
      "|  Chris|    Berliner|    5|\n",
      "|  Peter|   Bud Light|    9|\n",
      "|   John|Corona Extra|    6|\n",
      "+-------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From RDD and column names from list\n",
    "df = spark.createDataFrame(rdd, ['drinker', 'beer', 'score'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----+\n",
      "|drinker|        beer|score|\n",
      "+-------+------------+-----+\n",
      "|  Chris|    Berliner|    5|\n",
      "|  Peter|   Bud Light|    9|\n",
      "|   John|Corona Extra|    6|\n",
      "+-------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From RDD and add schema\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "    StructField(\"drinker\", StringType(), True),\n",
    "    StructField(\"beer\", StringType(), True),\n",
    "    StructField(\"score\", ByteType(), True)])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is type mismatch in the data, it will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chris', 'Berliner', 5), ('Peter', 'Bud Light', 9), ('John', 'Corona Extra', 't')]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o393.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 20) (DT-Inspiron.lan executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\", line 683, in prepare\n    return obj\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1391, in verify_struct\n    for v, (_, verifier) in zip(obj, verifiers):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1314, in verify_byte\n    if obj < -128 or obj > 127:\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1292, in verify_acceptable_types\n    if type(obj) not in _acceptable_types[_type]:\nTypeError: field score: ByteType can not accept object 't' in type <class 'str'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:492)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\", line 683, in prepare\n    return obj\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1391, in verify_struct\n    for v, (_, verifier) in zip(obj, verifiers):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1314, in verify_byte\n    if obj < -128 or obj > 127:\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1292, in verify_acceptable_types\n    if type(obj) not in _acceptable_types[_type]:\nTypeError: field score: ByteType can not accept object 't' in type <class 'str'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m df3 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(rdd, schema)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# df3 = spark.createDataFrame(rdd) This will not throw an error\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[43mdf3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:494\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o393.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 20.0 failed 1 times, most recent failure: Lost task 0.0 in stage 20.0 (TID 20) (DT-Inspiron.lan executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\", line 683, in prepare\n    return obj\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1391, in verify_struct\n    for v, (_, verifier) in zip(obj, verifiers):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1314, in verify_byte\n    if obj < -128 or obj > 127:\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1292, in verify_acceptable_types\n    if type(obj) not in _acceptable_types[_type]:\nTypeError: field score: ByteType can not accept object 't' in type <class 'str'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2454)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2403)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2402)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2402)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1160)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1160)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2584)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2573)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2214)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2235)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2254)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:492)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:445)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3715)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2728)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2935)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:287)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:326)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 619, in main\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 611, in process\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 74, in wrapper\n    return f(*args, **kwargs)\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\", line 683, in prepare\n    return obj\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1391, in verify_struct\n    for v, (_, verifier) in zip(obj, verifiers):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1410, in verify\n    if not verify_nullability(obj):\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1314, in verify_byte\n    if obj < -128 or obj > 127:\n  File \"C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\types.py\", line 1292, in verify_acceptable_types\n    if type(obj) not in _acceptable_types[_type]:\nTypeError: field score: ByteType can not accept object 't' in type <class 'str'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:350)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "a = [('Chris', 'Berliner', 5), ('Peter', 'Bud Light', 9), ('John', 'Corona Extra', \"t\")]\n",
    "rdd = sc.parallelize(a)\n",
    "print(rdd.collect())   \n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"drinker\", StringType(), True),\n",
    "    StructField(\"beer\", StringType(), True),\n",
    "    StructField(\"score\", ByteType(), True)])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "# df3 = spark.createDataFrame(rdd) This will not throw an error\n",
    "df3.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load a DataFrame from external storage\n",
    "The `DataFrameReader` class is an interface used to load a DataFrame from external storage systems such as file systems, key-value stores, and more. It is accessed through the `read` attribute of the `SparkSession` object.\n",
    "\n",
    "Here are some of the methods available in the `DataFrameReader` class:\n",
    "\n",
    "- `csv(path[, schema, sep, encoding, quote, ...])`: Loads a CSV file and returns the result as a DataFrame.\n",
    "- `format(source)`: Specifies the input data source format.\n",
    "- `jdbc(url, table[, column, lowerBound, ...])`: Constructs a DataFrame representing a database table accessible via a JDBC URL and connection properties.\n",
    "- `json(path[, schema, primitivesAsString, ...])`: Loads JSON files and returns the results as a DataFrame.\n",
    "- `load([path, format, schema])`: Loads data from a data source and returns it as a DataFrame.\n",
    "- `option(key, value)`: Adds an input option for the underlying data source.\n",
    "- `options(**options)`: Adds input options for the underlying data source.\n",
    "- `orc(path[, mergeSchema, pathGlobFilter, ...])`: Loads ORC files and returns the result as a DataFrame.\n",
    "- `parquet(*paths, **options)`: Loads Parquet files and returns the result as a DataFrame.\n",
    "- `schema(schema)`: Specifies the input schema.\n",
    "- `table(tableName)`: Returns the specified table as a DataFrame.\n",
    "- `text(paths[, wholetext, lineSep, ...])`: Loads text files and returns a DataFrame.\n",
    "\n",
    "These methods provide various ways to load data from different sources and formats, allowing you to create DataFrames for further processing and analysis.\n",
    "\n",
    "The `DataFrameReader` class was introduced in version 1.4.0 of PySpark and has undergone changes in version 3.4.0 to support Spark Connect. It provides a flexible and convenient interface to interact with external data sources and integrate them with the PySpark ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+\n",
      "|   TV|Radio|Newspaper|Sales|\n",
      "+-----+-----+---------+-----+\n",
      "|230.1| 37.8|     69.2| 22.1|\n",
      "| 44.5| 39.3|     45.1| 10.4|\n",
      "| 17.2| 45.9|     69.3| 12.0|\n",
      "|151.5| 41.3|     58.5| 16.5|\n",
      "|180.8| 10.8|     58.4| 17.9|\n",
      "|  8.7| 48.9|     75.0|  7.2|\n",
      "| 57.5| 32.8|     23.5| 11.8|\n",
      "|120.2| 19.6|     11.6| 13.2|\n",
      "|  8.6|  2.1|      1.0|  4.8|\n",
      "|199.8|  2.6|     21.2| 15.6|\n",
      "| 66.1|  5.8|     24.2| 12.6|\n",
      "|214.7| 24.0|      4.0| 17.4|\n",
      "| 23.8| 35.1|     65.9|  9.2|\n",
      "| 97.5|  7.6|      7.2| 13.7|\n",
      "|204.1| 32.9|     46.0| 19.0|\n",
      "|195.4| 47.7|     52.9| 22.4|\n",
      "| 67.8| 36.6|    114.0| 12.5|\n",
      "|281.4| 39.6|     55.8| 24.4|\n",
      "| 69.2| 20.5|     18.3| 11.3|\n",
      "|147.3| 23.9|     19.1| 14.6|\n",
      "+-----+-----+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read from CSV file\n",
    "dfa = spark.read.format('csv')\\\n",
    "    .options(header='true', inferSchema='true',\tsep=\",\")\\\n",
    "    .load(\"data/advertising.csv\")\n",
    "dfa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toDF\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page58.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**toDF(\\*cols)**\n",
    "\n",
    "The `toDF()` returns a new DataFrame with new column names\n",
    "\n",
    "**Parameters:**\n",
    "- `cols`: a tuple of string with new column name. The length of the list needs to be the same as the number of columns in the initial DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame with the specified column names.\n",
    "\n",
    "**Note:**\n",
    "- The order of the column names in the `cols` determines the order of the columns in the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+------+-----+---+\n",
      "|seller|buyer|amt|\n",
      "+------+-----+---+\n",
      "| Alice|  Bob|0.1|\n",
      "|   Bob|Carol|0.2|\n",
      "| Carol| Dave|0.3|\n",
      "+------+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# toDF\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toDF(\"seller\",\"buyer\",\"amt\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toPandas\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page60.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**toPandas()**\n",
    "\n",
    "The `toPandas()` method is used to retrieve the contents of a DataFrame as a Pandas DataFrame. This method is available only if the Pandas library is installed and accessible.\n",
    "\n",
    "**Returns:**\n",
    "A Pandas DataFrame containing the contents of the Spark DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- It is important to use the `toPandas()` method with caution, as it brings all the data from the distributed Spark DataFrame into the driver's memory. This can lead to memory-related issues if the resulting Pandas DataFrame is expected to be large.\n",
    "- The `toPandas()` method is particularly useful when you want to leverage the rich visualization and analysis capabilities provided by the Pandas library, especially for cases where columns have very long content that may not be easily displayed in a tabular format in Spark.\n",
    "- If the resulting Pandas DataFrame is expected to be large, it is recommended to explore alternative methods for processing and analyzing the data directly within the Spark ecosystem, such as using Spark SQL or the DataFrame API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Bob</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Carol</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carol</td>\n",
       "      <td>Dave</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    from     to  amt\n",
       "0  Alice    Bob  0.1\n",
       "1    Bob  Carol  0.2\n",
       "2  Carol   Dave  0.3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toPandas\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.toPandas()\n",
    "x.show()\n",
    "print(type(y))\n",
    "y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame to RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.rdd\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page42.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**property rdd**\n",
    "\n",
    "The `rdd` property returns the content of a DataFrame as an RDD of `Row` objects. Each `Row` represents a row of data in the DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "An RDD of `Row` objects representing the content of the DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- The `rdd` property allows you to access the underlying RDD representation of the DataFrame. This can be useful when you want to apply RDD-specific transformations and actions that are not available directly on DataFrames.\n",
    "- If you want to convert the RDD of `Row` objects to an RDD of tuples, you can use the `map()` transformation along with the `tuple()` function. For example, you can use `rdd.map(tuple)` to convert the RDD of `Row` objects to an RDD of tuples. This can be helpful when you need to work with a tuple-based representation of the data in subsequent RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[Row(from='Alice', to='Bob', amt=0.1), Row(from='Bob', to='Carol', amt=0.2), Row(from='Carol', to='Dave', amt=0.3)]\n"
     ]
    }
   ],
   "source": [
    "# rdd\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rdd\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "[('Alice', 'Bob', 0.1), ('Bob', 'Carol', 0.2), ('Carol', 'Dave', 0.3)]\n"
     ]
    }
   ],
   "source": [
    "# rdd\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.rdd.map(tuple)\n",
    "x.show()\n",
    "print(y.collect())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showing dataframe and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.show\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page52.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show(n=20, truncate=True, vertical=False)**\n",
    "\n",
    "The `show()` method is used to print the first `n` rows of a DataFrame to the console in a tabular format.\n",
    "\n",
    "**Parameters:**\n",
    "- `n`: The number of rows to display. By default, it is set to 20.\n",
    "- `truncate`: If set to `True`, strings longer than 20 characters will be truncated by default. If set to a number greater than 1, strings will be truncated to the specified length and aligned to the right.\n",
    "- `vertical`: If set to `True`, the output rows will be printed vertically, with each column value on a separate line.\n",
    "\n",
    "**Note:**\n",
    "- The `show()` method is mainly used for quick data inspection and debugging purposes, especially when dealing with smaller datasets. It is not intended for displaying the entire content of large DataFrames.\n",
    "- By default, the `truncate` parameter is set to `True`, which means that long strings will be truncated to 20 characters. This helps to keep the output concise. You can adjust the truncation length or disable truncation by setting `truncate` to a different value.\n",
    "- The `vertical` parameter is useful when you want to view the values in each column separately, which can be helpful when dealing with wide tables or when inspecting specific columns more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+---+\n",
      "|                from|   to|amt|\n",
      "+--------------------+-----+---+\n",
      "|               Alice|  Bob|0.1|\n",
      "|                 Bob|Carol|0.2|\n",
      "|               Carol| Dave|0.3|\n",
      "|Big name big name...|  Big|0.2|\n",
      "+--------------------+-----+---+\n",
      "\n",
      "+-----------------------------------+-----+---+\n",
      "|from                               |to   |amt|\n",
      "+-----------------------------------+-----+---+\n",
      "|Alice                              |Bob  |0.1|\n",
      "|Bob                                |Carol|0.2|\n",
      "|Carol                              |Dave |0.3|\n",
      "|Big name big name big name big name|Big  |0.2|\n",
      "+-----------------------------------+-----+---+\n",
      "\n",
      "-RECORD 0--------------------\n",
      " from | Alice                \n",
      " to   | Bob                  \n",
      " amt  | 0.1                  \n",
      "-RECORD 1--------------------\n",
      " from | Bob                  \n",
      " to   | Carol                \n",
      " amt  | 0.2                  \n",
      "-RECORD 2--------------------\n",
      " from | Carol                \n",
      " to   | Dave                 \n",
      " amt  | 0.3                  \n",
      "-RECORD 3--------------------\n",
      " from | Big name big name... \n",
      " to   | Big                  \n",
      " amt  | 0.2                  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from</th>\n",
       "      <th>to</th>\n",
       "      <th>amt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Bob</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Carol</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carol</td>\n",
       "      <td>Dave</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Big name big name big name big name</td>\n",
       "      <td>Big</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  from     to  amt\n",
       "0                                Alice    Bob  0.1\n",
       "1                                  Bob  Carol  0.2\n",
       "2                                Carol   Dave  0.3\n",
       "3  Big name big name big name big name    Big  0.2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3), (\"Big name big name big name big name\", \"Big\", 0.2)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.show(truncate=False)\n",
    "x.show(vertical=True)\n",
    "x.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.printSchema\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page40.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**printSchema()**\n",
    "\n",
    "The `printSchema()` method is used to print the schema of a DataFrame in a tree format.\n",
    "\n",
    "**Note:**\n",
    "- The `printSchema()` method provides a concise way to view the structure and data types of columns in a DataFrame.\n",
    "- The output is displayed in a tree format, where the root represents the DataFrame, and each subsequent line represents a column name along with its corresponding data type.\n",
    "- This method is particularly useful when working with complex schemas or when you need to quickly understand the structure of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "root\n",
      " |-- from: string (nullable = true)\n",
      " |-- to: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.schema\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page49.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**property schema**\n",
    "\n",
    "The `schema` property is used to retrieve the schema of a DataFrame as a `pyspark.sql.types.StructType` object.\n",
    "\n",
    "**Returns:**\n",
    "A `pyspark.sql.types.StructType` object representing the schema of the DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- The schema of a DataFrame defines the structure and data types of its columns.\n",
    "- The `schema` property provides access to the schema information, allowing you to inspect and work with the column names and data types programmatically.\n",
    "- The `pyspark.sql.types.StructType` object returned by the `schema` property contains a list of `pyspark.sql.types.StructField` objects, which represent individual columns with their respective name and data type information.\n",
    "- This property is useful when you need to perform advanced operations or transformations based on the DataFrame schema, such as dynamically generating SQL statements or working with nested structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "StructType(List(StructField(from,StringType,true),StructField(to,StringType,true),StructField(amt,DoubleType,true)))\n"
     ]
    }
   ],
   "source": [
    "# schema\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.schema\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.columns\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page8.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**property columns**\n",
    "\n",
    "The `columns` property is used to retrieve all column names of a DataFrame as a list.\n",
    "\n",
    "**Returns:**\n",
    "A list containing the names of all columns in the DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- The `columns` property is a convenient way to access and work with the column names of a DataFrame.\n",
    "- The order of the column names in the list corresponds to the order of the columns in the DataFrame.\n",
    "- This property is often used when you need to reference or manipulate specific columns in a DataFrame, such as selecting columns, renaming columns, or performing aggregations.\n",
    "- You can use the `columns` property along with other DataFrame methods and operations to perform various transformations and computations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "['from', 'to', 'amt']\n"
     ]
    }
   ],
   "source": [
    "# columns\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.columns #creates list of column names on driver\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page14.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**describe(\\*cols)**\n",
    "\n",
    "The `describe()` method computes basic statistics for numeric and string columns in a DataFrame.\n",
    "\n",
    "**Parameters:**\n",
    "- `*cols`: Optional. Columns for which statistics need to be computed. If no columns are provided, statistics will be computed for all numerical or string columns.\n",
    "\n",
    "**Returns:**\n",
    "A DataFrame containing the computed statistics for the specified columns.\n",
    "\n",
    "**Note:**\n",
    "- The `describe()` method provides summary statistics for the specified columns, including count, mean, standard deviation, minimum, and maximum values.\n",
    "- If no columns are provided as arguments, the method will automatically compute statistics for all numerical or string columns in the DataFrame.\n",
    "- The output DataFrame will have the following columns: \"summary\" (which specifies the type of statistic), the specified columns, and their corresponding statistics.\n",
    "- This method is useful for gaining insights into the distribution and summary of data in the specified columns, helping to understand the range, central tendency, and dispersion of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-------+-----+----+-------------------+\n",
      "|summary| from|  to|                amt|\n",
      "+-------+-----+----+-------------------+\n",
      "|  count|    3|   3|                  3|\n",
      "|   mean| null|null|0.20000000000000004|\n",
      "| stddev| null|null|0.09999999999999998|\n",
      "|    min|Alice| Bob|                0.1|\n",
      "|    max|Carol|Dave|                0.3|\n",
      "+-------+-----+----+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "x.describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page50.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**select(\\*cols)**\n",
    "\n",
    "The `select()` method is used to project a set of columns or expressions from a DataFrame and returns a new DataFrame.\n",
    "\n",
    "**Parameters:**\n",
    "- `*cols`: A list of column names (as strings) or expressions (as `Column` objects) to be selected. If one of the column names is `'*'`, it expands to include all columns in the current DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame that includes only the selected columns or expressions.\n",
    "\n",
    "**Note:**\n",
    "- The `select()` method is commonly used to extract specific columns from a DataFrame for further processing or analysis.\n",
    "- You can specify column names as strings in the `cols` list to include those columns in the resulting DataFrame. For example, `select(\"column1\", \"column2\")` selects only the \"column1\" and \"column2\" columns.\n",
    "- Alternatively, you can pass `Column` expressions in the `cols` list to perform transformations or computations on the columns. For example, `select(col(\"column1\") + col(\"column2\")).alias(\"sum\")` computes the sum of \"column1\" and \"column2\" and assigns it an alias \"sum\" in the resulting DataFrame.\n",
    "- By using `'*'` as one of the column names in the `cols` list, you can include all columns from the current DataFrame in the selection.\n",
    "- The order of the columns in the resulting DataFrame will match the order of the columns specified in the `cols` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| from|amt|\n",
      "+-----+---+\n",
      "|Alice|0.1|\n",
      "|  Bob|0.2|\n",
      "|Carol|0.3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - Pandas like select using a list of columns\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.select(['from','amt'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| from|amt|\n",
      "+-----+---+\n",
      "|Alice|0.1|\n",
      "|  Bob|0.2|\n",
      "|Carol|0.3|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - Columns as a parameters\n",
    "x.select(\"from\",\"amt\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                from|amt|\n",
      "+--------------------+---+\n",
      "|               Alice|0.1|\n",
      "|                 Bob|0.2|\n",
      "|               Carol|0.3|\n",
      "|Big name big name...|0.2|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - Using F.col\n",
    "from pyspark.sql import functions as F\n",
    "x.select(F.col(\"from\"),F.col(\"amt\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+\n",
      "|                from|amt|\n",
      "+--------------------+---+\n",
      "|               Alice|0.1|\n",
      "|                 Bob|0.2|\n",
      "|               Carol|0.3|\n",
      "|Big name big name...|0.2|\n",
      "+--------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - Using list of F.col and unpack it\n",
    "x.select(*[F.col(\"from\"),F.col(\"amt\")]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply functions to the columns**\n",
    "PySpark supports a wide range of built-in functions that can be applied to columns in a DataFrame. These functions are available in the `pyspark.sql.functions` module.\n",
    "There are a large number of functions available in PySpark.\n",
    "\n",
    "The full list is available on this link https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         upper(from)|(amt * 2)|\n",
      "+--------------------+---------+\n",
      "|               ALICE|      0.2|\n",
      "|                 BOB|      0.4|\n",
      "|               CAROL|      0.6|\n",
      "|BIG NAME BIG NAME...|      0.4|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply function to the columns\n",
    "x.select(F.upper(F.col(\"from\")),F.col(\"amt\")*2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page51.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**selectExpr(\\*expr)**\n",
    "\n",
    "The `selectExpr()` method is a variant of `select()` that allows you to project a set of SQL expressions and returns a new DataFrame.\n",
    "\n",
    "**Parameters:**\n",
    "- `*expr`: SQL expressions to be selected. Each expression can be a column name, a SQL expression, or an alias expression.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame that includes the selected SQL expressions.\n",
    "\n",
    "**Note:**\n",
    "- The `selectExpr()` method is useful when you want to perform complex transformations or computations on columns using SQL expressions.\n",
    "- You can specify SQL expressions as strings in the `*expr` parameter. For example, `selectExpr(\"column1 + column2\", \"column3 * 2 AS doubled_column\")` performs addition and multiplication operations on columns and assigns an alias to the computed column.\n",
    "- The SQL expressions can include column names, SQL functions, arithmetic operations, or any valid SQL expression that can be evaluated.\n",
    "- By using `*expr` as the parameter, you can pass multiple SQL expressions to be selected.\n",
    "- The resulting DataFrame will include the selected SQL expressions as columns, with the column names derived from the expressions or aliases assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+------------------+----------+\n",
      "|substr(from, 1, 1)|(amt + 10)|\n",
      "+------------------+----------+\n",
      "|                 A|      10.1|\n",
      "|                 B|      10.2|\n",
      "|                 C|      10.3|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selectExpr\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.selectExpr(['substr(from,1,1)','amt+10'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.where\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page63.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**where(condition)** / **filter(condition)**\n",
    "\n",
    "The `where()` and `filter()` methods are used to filter rows in a DataFrame based on the given condition.\n",
    "\n",
    "**Parameters:**\n",
    "- `condition`: A `Column` of type `BooleanType` or a string of SQL expression representing the filter condition.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame that includes only the rows satisfying the given condition.\n",
    "\n",
    "**Note:**\n",
    "- Both `where()` and `filter()` are used interchangeably to filter rows in a DataFrame based on a condition.\n",
    "- The `condition` parameter can be a `Column` object representing a boolean condition, or it can be a string of SQL expression that evaluates to a boolean value.\n",
    "- The resulting DataFrame will contain only the rows that satisfy the provided condition.\n",
    "- Multiple filtering conditions can be combined using logical operators such as `&` (AND) and `|` (OR).\n",
    "- The filter condition can involve comparisons, arithmetic operations, SQL functions, or any valid SQL expression that evaluates to a boolean value.\n",
    "- The `where()` and `filter()` methods are lazy operations, meaning that the filtering is not applied immediately. It is executed when an action is performed on the resulting DataFrame.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# where (filter) - SQL like syntax\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.where(\"amt > 0.1\")\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter: function syntax\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.filter(F.col(\"amt\") > 0.2)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.distinct\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page15.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distinct()**\n",
    "\n",
    "The `distinct()` method is used to return a new DataFrame that contains only the distinct (unique) rows from the original DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame containing the distinct rows.\n",
    "\n",
    "**Note:**\n",
    "- The `distinct()` operation eliminates duplicate rows from the DataFrame, resulting in a DataFrame that contains only unique rows.\n",
    "- The distinctness of rows is determined by comparing all columns in the DataFrame.\n",
    "- The order of rows in the resulting DataFrame may not be the same as the original DataFrame.\n",
    "- The `distinct()` operation is a transformation and is lazily evaluated. The actual computation occurs when an action is performed on the resulting DataFrame.\n",
    "- This method is commonly used to identify unique values or to remove duplicates from a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.distinct()\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dropna\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page18.svg\" width=360 height=203 />\n",
    "<"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dropna(how='any', thresh=None, subset=None)**\n",
    "\n",
    "The `dropna()` method is used to return a new DataFrame omitting rows with null values.\n",
    "\n",
    "**Parameters:**\n",
    "- `how`: Specifies how to drop rows with null values. If set to `'any'`, a row is dropped if it contains any nulls. If set to `'all'`, a row is dropped only if all its values are null. By default, it is set to `'any'`.\n",
    "- `thresh`: If specified, drops rows that have less than `thresh` non-null values. This parameter overrides the `how` parameter.\n",
    "- `subset`: An optional list of column names to consider. If specified, only the specified columns are checked for null values.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame with the rows containing null values dropped.\n",
    "\n",
    "**Note:**\n",
    "- The `dropna()` method is used to remove rows from a DataFrame that have null values, allowing you to clean the data and ensure data quality.\n",
    "- By default, the method drops rows if they have any null values (`how='any'`). If you want to drop rows only if all their values are null, you can set `how='all'`.\n",
    "- The `thresh` parameter allows you to specify a threshold for the number of non-null values a row must have to be retained. Rows with fewer than `thresh` non-null values will be dropped.\n",
    "- The `subset` parameter allows you to specify a subset of columns to consider for null value checking. Only the specified columns will be checked, and rows with null values in other columns will not be dropped.\n",
    "- The `dropna()` operation is a transformation and is lazily evaluated. The actual computation occurs when an action is performed on the resulting DataFrame.\n",
    "- This method is useful for handling missing or null values in a DataFrame and ensuring data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "| null|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| null| 0.3|\n",
      "|  Bob|Carol| 0.2|\n",
      "+-----+-----+----+\n",
      "\n",
      "+----+-----+----+\n",
      "|from|   to| amt|\n",
      "+----+-----+----+\n",
      "| Bob|Carol|null|\n",
      "| Bob|Carol| 0.2|\n",
      "+----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropna\n",
    "x = spark.createDataFrame([(None,\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",None,0.3),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = x.dropna(how='any',subset=['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add columns to Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumn\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page64.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**withColumn(colName, col)**\n",
    "\n",
    "The `withColumn()` method is used to return a new DataFrame by adding a column or replacing an existing column with the same name.\n",
    "\n",
    "**Parameters:**\n",
    "- `colName`: A string representing the name of the new column.\n",
    "- `col`: A `Column` expression for the new column. The expression must be based on the current DataFrame; attempting to add a column from another DataFrame will raise an error.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame with the specified column added or replaced.\n",
    "\n",
    "**Note:**\n",
    "- The `withColumn()` method allows you to add a new column to a DataFrame or replace an existing column with a new one.\n",
    "- The `colName` parameter specifies the name of the new column.\n",
    "- The `col` parameter is a `Column` expression that defines the values of the new column. The expression should be based on the current DataFrame, as attempting to add a column from another DataFrame will result in an error.\n",
    "- If a column with the same name as `colName` already exists in the DataFrame, it will be replaced with the new column.\n",
    "- It's important to note that calling `withColumn()` multiple times, especially within loops, to add multiple columns can result in inefficient plans and potential performance issues. To avoid this, it is recommended to use `select()` with multiple columns at once, rather than calling `withColumn()` multiple times. This helps optimize the execution plan and improves performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "|Alice|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| Dave| 0.3|\n",
      "+-----+-----+----+\n",
      "\n",
      "+-----+-----+----+-----+\n",
      "| from|   to| amt| conf|\n",
      "+-----+-----+----+-----+\n",
      "|Alice|  Bob| 0.1| true|\n",
      "|  Bob|Carol|null|false|\n",
      "|Carol| Dave| 0.3| true|\n",
      "+-----+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# withColumn\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumn('conf',x.amt.isNotNull()) # pandas equivalent df['conf'] = df['amt'].isNotNull()\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "|Alice|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| Dave| 0.3|\n",
      "+-----+-----+----+\n",
      "\n",
      "+-----+-----+----+--------+\n",
      "| from|   to| amt|constant|\n",
      "+-----+-----+----+--------+\n",
      "|Alice|  Bob| 0.1|       1|\n",
      "|  Bob|Carol|null|       1|\n",
      "|Carol| Dave| 0.3|       1|\n",
      "+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lit(val) Creates a Column of a literal value.\n",
    "from pyspark.sql.functions import lit\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",None),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumn('constant',lit(1))\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page65.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**withColumnRenamed(existing, new)**\n",
    "\n",
    "The `withColumnRenamed()` method is used to return a new DataFrame by renaming an existing column. If the schema does not contain the given column name, this operation has no effect.\n",
    "\n",
    "**Parameters:**\n",
    "- `existing`: A string representing the name of the existing column to rename.\n",
    "- `new`: A string representing the new name of the column.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame with the specified column renamed.\n",
    "\n",
    "**Note:**\n",
    "- If the schema of the DataFrame does not contain the given `existing` column name, this operation has no effect. The resulting DataFrame will be the same as the original DataFrame.\n",
    "- Renaming a column does not modify the original DataFrame. Instead, it creates a new DataFrame with the renamed column.\n",
    "- This method is useful when you want to update or clarify the column names in a DataFrame for improved readability or compatibility with downstream operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+------+\n",
      "| from|   to|amount|\n",
      "+-----+-----+------+\n",
      "|Alice|  Bob|   0.1|\n",
      "|  Bob|Carol|   0.2|\n",
      "|Carol| Dave|   0.3|\n",
      "+-----+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# withColumnRenamed\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.withColumnRenamed('amt','amount')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.drop\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page16.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**drop(cols)**\n",
    "\n",
    "The `drop()` method is used to return a new DataFrame that drops the specified column(s). If the schema does not contain the given column name(s), this operation has no effect.\n",
    "\n",
    "**Parameters:**\n",
    "- `cols`: Can be one of the following:\n",
    "  - A string representing the name of the column to drop.\n",
    "  - A `Column` object representing the column to drop.\n",
    "  - A list of string names of the columns to drop.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame with the specified column(s) dropped.\n",
    "\n",
    "**Note:**\n",
    "- If the schema of the DataFrame does not contain any of the given column name(s), this operation has no effect. The resulting DataFrame will be the same as the original DataFrame.\n",
    "- Dropping a column does not modify the original DataFrame. Instead, it creates a new DataFrame without the dropped column(s).\n",
    "- This method is useful when you want to exclude specific columns from a DataFrame for further analysis or processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+\n",
      "| from|   to|\n",
      "+-----+-----+\n",
      "|Alice|  Bob|\n",
      "|  Bob|Carol|\n",
      "|Carol| Dave|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.drop('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page28.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupBy(\\*cols)**\n",
    "\n",
    "The `groupBy()` method is used to group the DataFrame using the specified columns, allowing for subsequent aggregation operations. The `groupBy()` method returns a GroupedData object that provides access to various aggregate functions for performing calculations on the grouped data.\n",
    "\n",
    "**Parameters:**\n",
    "- `cols`: A list of columns to group by. Each element can be a column name (string) or a Column expression.\n",
    "\n",
    "**Returns:**\n",
    "A GroupedData object representing the grouped DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- The resulting GroupedData object provides access to various aggregate functions such as `avg()`, `max()`, `min()`, `sum()`, `count()`, and more. These functions can be used to calculate summary statistics and perform aggregations on the grouped data.\n",
    "- Additionally, user-defined aggregate functions (UDFs) created with `pandas_udf()` can also be used as aggregate functions with `groupBy()`.\n",
    "- The `groupBy()` operation involves a full shuffle of the data, and all the data for each group will be loaded into memory. Therefore, it's important to consider the memory usage and potential out-of-memory risks, especially when dealing with skewed data or groups that are too large to fit in memory.\n",
    "- The `groupBy()` method is commonly used in combination with aggregate functions to perform group-level calculations and summarize data based on specific grouping criteria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|  1|\n",
      "|Alice|Carol|  2|\n",
      "|Carol| Dave|  3|\n",
      "|Carol|  Bob|  4|\n",
      "+-----+-----+---+\n",
      "\n",
      "<pyspark.sql.group.GroupedData object at 0x000001B4D090DF00>\n"
     ]
    }
   ],
   "source": [
    "# groupBy\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",1),(\"Alice\",\"Carol\",2),(\"Carol\",\"Dave\",3),('Carol',\"Bob\",4)], ['from','to','amt'])\n",
    "y = x.groupBy('from')\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page29.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting GroupedData object provides access to various aggregate functions such as `avg()`, `max()`, `min()`, `sum()`, `count()`, and more. These functions can be used to calculate summary statistics and perform aggregations on the grouped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| from|   to| amt|\n",
      "+-----+-----+----+\n",
      "|Alice|  Bob| 0.1|\n",
      "|  Bob|Carol|null|\n",
      "|Carol| Dave| 0.3|\n",
      "+-----+-----+----+\n",
      "\n",
      "+-----+--------+\n",
      "| from|avg(amt)|\n",
      "+-----+--------+\n",
      "|Carol|     0.3|\n",
      "|  Bob|    null|\n",
      "|Alice|     0.1|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupBy(col1).avg(col2)\n",
    "y = x.groupBy('from').avg('amt')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**agg(\\*exprs)**\n",
    "\n",
    "Aggregate on the entire DataFrame without groups (shorthand for df.groupBy().agg()).\n",
    "\n",
    "**Parameters:**\n",
    "- `exprs` Columns or expressions to aggregate DataFrame by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(max(amt)=0.3)]\n",
      "[Row(min(amt)=0.1)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "print(x.agg({\"amt\": \"max\"}).collect())\n",
    "print(x.agg(F.min(x.amt)).collect())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top K queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.orderBy\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page38.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**orderBy(\\*cols, \\*kwargs)**\n",
    "\n",
    "The `orderBy()` method is used to return a new DataFrame that is sorted by the specified column(s).\n",
    "\n",
    "**Parameters:**\n",
    "- `cols`: A list of Column objects or column names to sort the DataFrame by.\n",
    "- `ascending`: Optional. A boolean or a list of boolean values specifying the sort order for each column. If set to `True`, the corresponding column is sorted in ascending order.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame that is sorted based on the specified column(s).\n",
    "\n",
    "**Note:**\n",
    "- The `cols` parameter specifies the columns by which the DataFrame should be sorted. Each element in `cols` can be a Column object or a column name.\n",
    "- By default, the sorting is done in ascending order. You can control the sort order using the `ascending` parameter. If set to `True`, the corresponding column is sorted in ascending order, and if set to `False`, the column is sorted in descending order. If a list of boolean values is provided, it must have the same length as `cols` and specify the sort order for each column individually.\n",
    "- The resulting DataFrame will have the rows sorted based on the specified column(s). If multiple columns are specified, the DataFrame is sorted first by the first column, then by the second column, and so on.\n",
    "- The `orderBy()` operation triggers a global sort, which may involve shuffling the data across the cluster. This can be an expensive operation, especially for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "|Alice|  Bob|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# orderBy\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "y = x.orderBy(['to'],ascending=[False])\n",
    "y.show()\n",
    "# The same functionality \n",
    "y = x.orderBy(F.desc('to'))\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.limit\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page34.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**limit(num)**\n",
    "\n",
    "The `limit()` method is used to limit the number of results in a DataFrame to the specified number.\n",
    "\n",
    "**Parameters:**\n",
    "- `num`: The number of results to limit the DataFrame to.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame containing a limited number of results.\n",
    "\n",
    "**Note:**\n",
    "- The `limit()` method is commonly used to restrict the number of rows in a DataFrame and obtain a smaller subset of the data.\n",
    "- The `num` parameter specifies the maximum number of results to be included in the resulting DataFrame.\n",
    "- The resulting DataFrame will contain at most `num` rows. If the original DataFrame has fewer than `num` rows, all rows from the original DataFrame will be included in the result.\n",
    "- This method is useful when you want to sample a subset of data for exploration or testing purposes or when you want to reduce the size of the DataFrame for improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Carol| Dave|0.3|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.limit(2)\n",
    "z = x.orderBy(['amt'],ascending=False).limit(2)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.join\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page33.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**join(other, on=None, how=None)**\n",
    "\n",
    "The `join()` method is used to join a DataFrame with another DataFrame, using the given join expression.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The right side DataFrame to join with.\n",
    "- `on`: Optional. Specifies the join column name(s) as a string, a list of column names, a join expression (Column), or a list of Columns. If `on` is a string or a list of strings indicating the name of the join column(s), the column(s) must exist on both sides, and this performs an equi-join.\n",
    "- `how`: Optional. Specifies the type of join to perform. It must be one of the following:\n",
    "  - `inner`: Performs an inner join, keeping only the rows that have matching keys in both DataFrames.\n",
    "  - `cross`: Performs a cross join, producing a Cartesian product of the rows in both DataFrames.\n",
    "  - `outer`, `full`, `fullouter`, `full_outer`: Performs a full outer join, keeping all rows from both DataFrames and filling in missing values with null.\n",
    "  - `left`, `leftouter`, `left_outer`: Performs a left outer join, keeping all rows from the left DataFrame and filling in missing values from the right DataFrame with null.\n",
    "  - `right`, `rightouter`, `right_outer`: Performs a right outer join, keeping all rows from the right DataFrame and filling in missing values from the left DataFrame with null.\n",
    "  - `semi`, `leftsemi`, `left_semi`: Performs a semi join, keeping only the rows from the left DataFrame that have matching keys in the right DataFrame.\n",
    "  - `anti`, `leftanti`, `left_anti`: Performs an anti join, keeping only the rows from the left DataFrame that do not have matching keys in the right DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame resulting from the join operation.\n",
    "\n",
    "**Note:**\n",
    "- The resulting DataFrame will contain the combined rows from both DataFrames, based on the join condition and type specified.\n",
    "- Joins can be used to merge data from multiple sources, perform data matching, or combine data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice| 20|\n",
      "|  Bob| 40|\n",
      "| Dave| 80|\n",
      "+-----+---+\n",
      "\n",
      "+-----+----+---+---+\n",
      "| from|  to|amt|age|\n",
      "+-----+----+---+---+\n",
      "|Alice| Bob|0.1| 40|\n",
      "|Carol|Dave|0.3| 80|\n",
      "+-----+----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = spark.createDataFrame([('Alice',20),(\"Bob\",40),(\"Dave\",80)], ['name','age'])\n",
    "z = x.join(y,x.to == y.name,'inner').select('from','to','amt','age')\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.intersect\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page31.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**intersect(other)**\n",
    "\n",
    "The `intersect()` method is used to return a new DataFrame containing rows that exist in both the current DataFrame and another DataFrame. This operation is equivalent to the `INTERSECT` operation in SQL.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The other DataFrame to intersect with.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame containing the intersected rows.\n",
    "\n",
    "**Note:**\n",
    "- The `intersect()` method is used to find the common rows between two DataFrames, creating a new DataFrame that includes only the rows present in both DataFrames.\n",
    "- The resulting DataFrame will contain only the rows that exist in both the current DataFrame and the other DataFrame.\n",
    "- The `intersect()` operation compares the rows based on their values and does not consider the order of the rows.\n",
    "- This method can be useful for finding common records, performing data deduplication, or identifying overlapping data between different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Alice|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+---+\n",
      "| from| to|amt|\n",
      "+-----+---+---+\n",
      "|Alice|Bob|0.1|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# intersect\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Alice\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.intersect(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.subtract\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page56.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**subtract(other)**\n",
    "\n",
    "The `subtract()` method is used to return a new DataFrame containing rows that exist in the current DataFrame but not in another DataFrame. This operation is equivalent to the `EXCEPT DISTINCT` operation in SQL.\n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The other DataFrame to subtract from the current DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame containing the subtracted rows.\n",
    "\n",
    "**Note:**\n",
    "- The resulting DataFrame will contain only the rows that exist in the current DataFrame and do not exist in the other DataFrame.\n",
    "- The `subtract()` operation compares the rows based on their values and does not consider the order of the rows.\n",
    "- This method can be useful for finding unique records, removing duplicate data, or identifying the differences between two datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subtract\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.subtract(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unionAll\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page61.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unionAll(other)**\n",
    "\n",
    "The `unionAll()` method is used to return a new DataFrame containing the union of rows from the current DataFrame and other DataFrame. This operation is equivalent to the `UNION ALL` operation in SQL. \n",
    "\n",
    "**Parameters:**\n",
    "- `other`: The other DataFrame to union with the current DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame containing the union of rows.\n",
    "\n",
    "**Note:**\n",
    "- The resulting DataFrame will contain all the rows from the current DataFrame and all the rows from the other DataFrame, with duplicates included if they exist.\n",
    "- The `unionAll()` operation does not perform deduplication of elements. If you want to remove duplicates and obtain a distinct set of rows, you can follow this function with the `distinct()` function.\n",
    "- The column resolution in `unionAll()` is done by position (not by name), meaning that columns with the same position in both DataFrames will be combined together.\n",
    "- This method is useful for combining multiple DataFrames vertically, appending new rows, or merging datasets with compatible schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.1|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unionAll\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2)], ['from','to','amt'])\n",
    "y = spark.createDataFrame([(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.1)], ['from','to','amt'])\n",
    "z = x.unionAll(y)\n",
    "x.show()\n",
    "y.show()\n",
    "z.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL like Operations on DataFrames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sql(sqlQuery)**\n",
    "\n",
    "The `sql()` method is used to execute a SQL query on a DataFrame and return a new DataFrame representing the result of the query.\n",
    "\n",
    "**Parameters:**\n",
    "- `sqlQuery`: A string containing the SQL query to be executed.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame representing the result of the SQL query.\n",
    "\n",
    "**Note:**\n",
    "- The `sql()` method allows you to perform SQL queries on a DataFrame, leveraging SQL syntax and capabilities for data manipulation and analysis.\n",
    "- The resulting DataFrame represents the result of the SQL query, with the schema and data determined by the query.\n",
    "- The SQL query can include various operations such as selecting columns, filtering rows, aggregating data, joining tables, and more.\n",
    "- This method is particularly useful when you have complex data manipulation or analysis requirements that can be expressed more easily and concisely using SQL syntax.\n",
    "- The `sql()` method internally uses the Spark SQL engine to execute the SQL query on the DataFrame.\n",
    "- It's important to ensure that the DataFrame you are executing the SQL query on has been registered as a temporary table or view using the `createOrReplaceTempView()` method. Otherwise, the SQL query will not be able to reference the DataFrame.\n",
    "- The SQL query can reference column names and perform operations on the columns of the DataFrame. The columns are resolved based on the schema of the DataFrame."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**createOrReplaceTempView(name)**\n",
    "\n",
    "The `createOrReplaceTempView()` method is used to register the DataFrame as a temporary table with the given name. This temporary table can be used to perform SQL queries and operations.\n",
    "\n",
    "**Parameters:**\n",
    "- `name`: A string representing the name to assign to the temporary table.\n",
    "\n",
    "**Note:**\n",
    "- The `createOrReplaceTempView()` method allows you to register the DataFrame as a temporary table, making it available for SQL operations and queries.\n",
    "- The `name` parameter specifies the name to assign to the temporary table.\n",
    "- The temporary table is tied to the lifetime of the SparkSession that was used to create the DataFrame. Once the SparkSession is terminated, the temporary table will no longer be accessible.\n",
    "- Once the DataFrame is registered as a temporary table, you can refer to it by the specified name in SQL queries using the `sql()` method or any other Spark SQL operations.\n",
    "- Registering a DataFrame as a temporary table enables you to leverage SQL capabilities and syntax for data manipulation and analysis, including complex queries, joins, aggregations, and more.\n",
    "- Temporary tables are particularly useful when you need to perform multiple SQL operations on the same DataFrame or when you want to separate the SQL logic from the DataFrame transformations.\n",
    "- Temporary tables can be accessed and queried from different parts of the application or from different Spark jobs as long as they use the same SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# registerTempTable\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.createOrReplaceTempView(name=\"TRANSACTIONS\")\n",
    "y = spark.sql('SELECT * FROM TRANSACTIONS WHERE amt > 0.1')\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page10.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count()**\n",
    "\n",
    "The `count()` method is used to return the number of rows in the DataFrame.\n",
    "\n",
    "**Returns:**\n",
    "The number of rows in the DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- This method is useful for understanding the size of the DataFrame, checking data completeness, or performing basic data profiling tasks.\n",
    "- The `count()` operation is an action, meaning that it triggers the evaluation of the DataFrame and retrieves the actual row count from the underlying data source.\n",
    "- It is important to note that the count operation may require reading and processing the entire DataFrame, which can be time-consuming for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.show()\n",
    "print(x.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.corr\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page9.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corr(col1, col2, method=None)**\n",
    "\n",
    "The `corr()` method calculates the correlation between two columns of a DataFrame as a double value. Currently, it only supports the Pearson Correlation Coefficient.\n",
    "\n",
    "**Parameters:**\n",
    "- `col1`: The name of the first column.\n",
    "- `col2`: The name of the second column.\n",
    "- `method`: Optional. The correlation method to use. Currently, only the \"pearson\" method is supported.\n",
    "\n",
    "**Returns:**\n",
    "The correlation between the two columns as a double value.\n",
    "\n",
    "**Note:**\n",
    "- The correlation measures the strength and direction of the linear relationship between the two columns. The Pearson Correlation Coefficient is a commonly used correlation measure.\n",
    "- The resulting correlation value is returned as a double value, ranging from -1.0 to 1.0. A value of 1.0 indicates a perfect positive correlation, -1.0 indicates a perfect negative correlation, and 0.0 indicates no correlation.\n",
    "- Currently, only the \"pearson\" method is supported for calculating the correlation.\n",
    "- The `corr()` method can be useful for understanding the relationship between different variables in the DataFrame and can be used in exploratory data analysis, feature selection, or statistical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+\n",
      "| from|   to|amt|  fee|\n",
      "+-----+-----+---+-----+\n",
      "|Alice|  Bob|0.1|0.001|\n",
      "|  Bob|Carol|0.2| 0.02|\n",
      "|Carol| Dave|0.3| 0.02|\n",
      "+-----+-----+---+-----+\n",
      "\n",
      "0.8660254037844389\n"
     ]
    }
   ],
   "source": [
    "# corr\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.corr(col1=\"amt\",col2=\"fee\")\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.cov\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page11.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cov(col1, col2)**\n",
    "\n",
    "The `cov()` method calculates the sample covariance between two columns in the DataFrame, specified by their names.\n",
    "\n",
    "**Parameters:**\n",
    "- `col1`: The name of the first column.\n",
    "- `col2`: The name of the second column.\n",
    "\n",
    "**Returns:**\n",
    "The sample covariance between the two columns, represented as a double value.\n",
    "\n",
    "**Note:**\n",
    "- The sample covariance measures the linear relationship between the two columns and provides insight into how they vary together.\n",
    "- The resulting covariance value is returned as a double value.\n",
    "- This method can be useful for understanding the relationship between different variables in the DataFrame and can be used in exploratory data analysis, feature engineering, or statistical modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+-----+\n",
      "| from|   to|amt|  fee|\n",
      "+-----+-----+---+-----+\n",
      "|Alice|  Bob|0.1|0.001|\n",
      "|  Bob|Carol|0.2| 0.02|\n",
      "|Carol| Dave|0.3| 0.02|\n",
      "+-----+-----+---+-----+\n",
      "\n",
      "0.0009500000000000001\n"
     ]
    }
   ],
   "source": [
    "# cov\n",
    "x = spark.createDataFrame([(\"Alice\",\"Bob\",0.1,0.001),(\"Bob\",\"Carol\",0.2,0.02),(\"Carol\",\"Dave\",0.3,0.02)], ['from','to','amt','fee'])\n",
    "y = x.cov(col1=\"amt\",col2=\"fee\")\n",
    "x.show()\n",
    "print(y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.persist\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page39.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**persist(storageLevel=StorageLevel(True, True, False, False, 1))**\n",
    "\n",
    "The `persist()` method is used to set the storage level for persisting the contents of the DataFrame across operations after the first computation. This allows for faster access to the DataFrame in subsequent operations. By default, the storage level is set to `MEMORY_AND_DISK` if no storage level is specified.\n",
    "\n",
    "**Parameters:**\n",
    "- `storageLevel`: Optional. The storage level to assign to the DataFrame. It determines how the DataFrame is stored in memory or on disk. If not specified, the default storage level is `MEMORY_AND_DISK` which provides a good balance between performance and storage usage..\n",
    "\n",
    "**Note:**\n",
    "- The `storageLevel` parameter specifies the storage level to assign to the DataFrame. It determines where and how the DataFrame is stored. The storage level can be customized based on the available memory and disk resources.\n",
    "- The storage level is an instance of the `StorageLevel` class, which represents various options for storing data, such as storing it in memory, on disk, or both.\n",
    "- The storage level set using `persist()` is persistent across operations, meaning that the DataFrame remains stored in memory or on disk until explicitly unpersisted.\n",
    "- The `persist()` operation is lazy and does not trigger immediate storage. The actual storage occurs when an action is performed on the DataFrame.\n",
    "- The `persist()` method is useful when you have intermediate results or frequently accessed DataFrames that you want to cache in memory or on disk for faster access in subsequent operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# persist\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.persist(storageLevel=StorageLevel(True,True,False,True,1)) # StorageLevel(useDisk,useMemory,useOffHeap,deserialized,replication=1)\n",
    "x.show()\n",
    "x.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.unpersist\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page62.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**unpersist(blocking=False)**\n",
    "\n",
    "The `unpersist()` method is used to mark the DataFrame as non-persistent and remove all blocks associated with it from memory and disk. This frees up the resources used by the DataFrame.\n",
    "\n",
    "**Parameters:**\n",
    "- `blocking`: Optional. If set to `True`, the method will block until the DataFrame is unpersisted from all nodes. If set to `False`, the method will return immediately without waiting for the unpersistence to complete. By default, `blocking` is set to `False`.\n",
    "\n",
    "**Note:**\n",
    "- By marking the DataFrame as non-persistent, all the blocks associated with it are removed from memory and disk, allowing the resources to be reclaimed.\n",
    "- When a DataFrame is unpersisted, subsequent access to the DataFrame will trigger recomputation of the DataFrame's contents.\n",
    "- It's important to note that unpersisting a DataFrame does not remove the DataFrame itself from memory. It only removes the persistence status and the associated blocks from memory and disk.\n",
    "- The `unpersist()` method is useful when you want to free up memory and disk resources after you no longer need the persisted DataFrame. This can help optimize resource usage and improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# unpersist\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "x.cache()\n",
    "x.count()\n",
    "x.show()\n",
    "print(x.is_cached)\n",
    "x.unpersist()\n",
    "print(x.is_cached)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.replace\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page45.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**replace(to_replace, value=<no value>, subset=None)**\n",
    "\n",
    "The `replace()` method is used to return a new DataFrame with specified values replaced by other values. This operation can be used to perform value replacement in a DataFrame. \n",
    "\n",
    "**Parameters:**\n",
    "- `to_replace`: The value(s) to be replaced. It can be a boolean, integer, float, string, list, or dictionary. If it is a dictionary, the `value` parameter is ignored, and `to_replace` must be a mapping between a value and its replacement.\n",
    "- `value`: The replacement value(s) to be used. It must be of the same type as `to_replace` and can be a boolean, integer, float, string, list, or None. If `value` is a list, it should have the same length and type as `to_replace`. If `value` is a scalar and `to_replace` is a sequence, the scalar value is used as a replacement for each item in `to_replace`.\n",
    "- `subset`: Optional. A list of column names to consider for replacement. Only the columns specified in `subset` will be processed. Columns with a different data type than the replacement value(s) will be ignored.\n",
    "\n",
    "**Returns:**\n",
    "A new DataFrame with specified values replaced by other values.\n",
    "\n",
    "**Note:**\n",
    "- The `replace()` method allows you to replace specific values in a DataFrame with other values, creating a new DataFrame with the replacements applied.\n",
    "- The `to_replace` parameter specifies the value(s) to be replaced. It can be a boolean, integer, float, string, list, or dictionary. If it is a dictionary, the `value` parameter is ignored, and `to_replace` must be a mapping between a value and its replacement.\n",
    "- The `value` parameter specifies the replacement value(s) to be used. It must be of the same type as `to_replace` and can be a boolean, integer, float, string, list, or None.\n",
    "- The `subset` parameter is optional and allows you to specify a subset of columns to consider for replacement. Only the columns specified in `subset` will be processed. Columns with a different data type than the replacement value(s) will be ignored.\n",
    "- The resulting DataFrame will have the specified values replaced by other values according to the specified replacements.\n",
    "- When performing numeric replacements, it is important to note that all values to be replaced should have unique floating point representations to avoid conflicts. In cases of conflicts, an arbitrary replacement will be used.\n",
    "- The replacement values will be cast to the type of the existing column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol|David|0.3|\n",
      "+-----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replace\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.replace('Dave','David',['from','to'])\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.sample\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page47.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sample(withReplacement=None, fraction=None, seed=None)**\n",
    "\n",
    "The `sample()` method is used to return a sampled subset of the DataFrame.\n",
    "\n",
    "**Parameters:**\n",
    "- `withReplacement`: Optional. Specifies whether to sample with replacement or not. If set to `True`, sampling will be done with replacement (default is `False`).\n",
    "- `fraction`: Optional. Specifies the fraction of rows to include in the sample. It should be a value between 0.0 and 1.0.\n",
    "- `seed`: Optional. Specifies the seed for sampling. If provided, the same seed will generate the same sample in subsequent runs (default is a random seed).\n",
    "\n",
    "**Returns:**\n",
    "A sampled subset of the DataFrame.\n",
    "\n",
    "**Note:**\n",
    "- The `fraction` parameter specifies the fraction of rows to include in the sample. It should be a value between 0.0 and 1.0. For example, a value of 0.5 indicates that the sample should contain 50% of the rows in the DataFrame.\n",
    "- The `seed` parameter allows you to specify a seed for the random number generator used in sampling. Providing a specific seed ensures reproducibility of the sample across multiple runs. If no seed is provided, a random seed will be used.\n",
    "- The resulting DataFrame will contain a random subset of the rows from the original DataFrame, based on the specified sampling parameters.\n",
    "- The `sample()` method is useful when you want to explore or analyze a smaller subset of data, or when you need to perform testing or validation on a fraction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+----+---+\n",
      "| from|  to|amt|\n",
      "+-----+----+---+\n",
      "|Carol|Dave|0.3|\n",
      "+-----+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.sample(False,0.5)\n",
    "x.show()\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/3.0.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit\">\n",
    "<img align=left src=\"images/pyspark-pictures-dataframes-page41.svg\" width=360 height=203 />\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**randomSplit(weights, seed=None)**\n",
    "\n",
    "The `randomSplit()` method is used to randomly split the DataFrame into multiple subsets with the provided weights.\n",
    "\n",
    "**Parameters:**\n",
    "- `weights`: A list of doubles representing the weights with which to split the DataFrame. The weights will be normalized if they don't sum up to 1.0.\n",
    "- `seed`: Optional. The seed for sampling. If provided, the same seed will generate the same split in subsequent runs.\n",
    "\n",
    "**Returns:**\n",
    "A list of DataFrames resulting from the random split.\n",
    "\n",
    "**Note:**\n",
    "- Providing a specific seed ensures reproducibility of the split across multiple runs. If no seed is provided, a random seed will be used.\n",
    "- The resulting list will contain multiple DataFrames, each representing a subset of the original DataFrame. The sizes of the subsets will be proportional to the specified weights.\n",
    "- The subsets are randomly generated, meaning that each row from the original DataFrame has an equal chance of being included in any of the resulting subsets.\n",
    "- The `randomSplit()` method is useful when you need to split the data into training and test sets, or when you want to create multiple subsets for cross-validation or data exploration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|Alice|  Bob|0.4|\n",
      "|Alice|  Bob|0.5|\n",
      "|Alice|  Bob|0.6|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+-----+---+\n",
      "| from|   to|amt|\n",
      "+-----+-----+---+\n",
      "|Alice|  Bob|0.1|\n",
      "|  Bob|Carol|0.2|\n",
      "|Carol| Dave|0.3|\n",
      "+-----+-----+---+\n",
      "\n",
      "+-----+---+---+\n",
      "| from| to|amt|\n",
      "+-----+---+---+\n",
      "|Alice|Bob|0.4|\n",
      "|Alice|Bob|0.5|\n",
      "|Alice|Bob|0.6|\n",
      "+-----+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomSplit\n",
    "x = spark.createDataFrame([('Alice',\"Bob\",0.1),('Alice',\"Bob\",0.4),('Alice',\"Bob\",0.5),('Alice',\"Bob\",0.6),(\"Bob\",\"Carol\",0.2),(\"Carol\",\"Dave\",0.3)], ['from','to','amt'])\n",
    "y = x.randomSplit([0.5,0.5],10)\n",
    "x.show()\n",
    "y[0].show()\n",
    "y[1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
