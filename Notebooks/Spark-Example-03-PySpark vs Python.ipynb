{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Examples in Python vs PySpark\n",
    "The objective of this Notebook is to help students understand the differences between implementing simple algorithms in Python and PySpark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of elements of a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "result = 0\n",
    "for i in range(len(arr)):\n",
    "    result += arr[i]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "result = rdd.reduce(lambda a, b: a + b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 2, 'world': 1}\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "text = \"hello world hello\"\n",
    "word_dict = {}\n",
    "for word in text.split():\n",
    "    word_dict[word] = word_dict.get(word, 0) + 1\n",
    "print(word_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hello': 2, 'world': 1}\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "text = sc.parallelize([\"hello world hello\"])\n",
    "result = (text.flatMap(lambda line: line.split(\" \"))\n",
    "          .map(lambda word: (word, 1))\n",
    "          .reduceByKey(lambda a, b: a + b)\n",
    "          .collect())\n",
    "\n",
    "print(dict(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Maximum Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [2, 4, 1, 8, 5]\n",
    "max_val = arr[0]\n",
    "for i in range(1, len(arr)):\n",
    "    if arr[i] > max_val:\n",
    "        max_val = arr[i]\n",
    "print(max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([2, 4, 1, 8, 5])\n",
    "result = rdd.reduce(lambda a, b: a if a > b else b)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Occurrences of a Specific Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [2, 3, 4, 2, 8, 2]\n",
    "target = 2\n",
    "\n",
    "result = 0\n",
    "for i in range(len(arr)):\n",
    "    if arr[i] == target:\n",
    "        result += 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([2, 3, 4, 2, 8, 2])\n",
    "target = 2\n",
    "result = rdd.filter(lambda x: x == target).count()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Prime Numbers in a Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "\n",
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n ** 0.5)+1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "prime_numbers = []\n",
    "for i in range(2, 30):\n",
    "    if is_prime(i):\n",
    "        prime_numbers.append(i)\n",
    "print(prime_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "\n",
    "rdd = sc.parallelize(range(2, 30))\n",
    "prime_numbers = rdd.filter(is_prime).collect()\n",
    "print(prime_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 3, 2: 2, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "arr = [1, 1, 1, 2, 2, 3]\n",
    "result = {}\n",
    "for i in range(len(arr)):\n",
    "    result[arr[i]] = result.get(arr[i], 0) + 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {1: 3, 2: 2, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 1, 1, 2, 2, 3])\n",
    "result = rdd.countByValue()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sum of Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "arr = [1, 2, 3, 4, 5]\n",
    "sum_of_squares = 0\n",
    "for i in range(len(arr)):\n",
    "    sum_of_squares += arr[i] ** 2\n",
    "print(sum_of_squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "sum_of_squares = rdd.map(lambda x: x ** 2).reduce(lambda a, b: a + b)\n",
    "print(sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF stands for Term Frequency-Inverse Document Frequency. It is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. This method is widely used in information retrieval and text mining.\n",
    "\n",
    "**Term Frequency (TF)**\n",
    "\n",
    "Term Frequency measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (the total number of terms in the document) as a way of normalization:\n",
    "\n",
    "$$\n",
    "TF(t) = \\left(\\frac{\\text{Number of times term } t \\text{ appears in a document}}{\\text{Total number of terms in the document}}\\right)\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (IDF)**\n",
    "\n",
    "Inverse Document Frequency measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms, like \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scaling up the rare ones, by computing the following:\n",
    "\n",
    "$$\n",
    "IDF(t) = \\log\\left(\\frac{1 + \\text{Total number of documents}}{1 + \\text{Number of documents with term } t}\\right) + 1\n",
    "$$\n",
    "\n",
    "This formula ensures that terms with very low frequency do not get a zero IDF by adding 1 in the numerator and denominator. The logarithmic scale is used to ensure that the IDF doesn't grow too quickly with the increase in the number of documents.\n",
    "\n",
    "**TF-IDF Calculation**\n",
    "\n",
    "TF-IDF is simply the product of TF and IDF:\n",
    "\n",
    "$$\n",
    "TFIDF(t, d) = TF(t, d) \\times IDF(t)\n",
    "$$\n",
    "\n",
    "This value is higher when a term is more frequent in a specific document but less frequent across all documents, which implies the term is quite significant in the particular document.\n",
    "\n",
    "**Application in Text Mining**\n",
    "\n",
    "TF-IDF has a variety of applications, mainly in systems involving natural language processing (NLP) and information retrieval such as:\n",
    "\n",
    "- **Search engines**: Ranking documents based on query terms.\n",
    "- **Document clustering**: Grouping similar documents.\n",
    "- **Text summarization**: Extracting key terms that reflect the most relevant information in documents.\n",
    "- **Feature extraction**: Transforming textual data into a format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'apple': 0.8109302162163288, 'orange': 0.4054651081081644}, {'apple': 0.4054651081081644, 'lemon': 0.4054651081081644}, {'orange': 0.4054651081081644, 'lemon': 0.4054651081081644}]\n"
     ]
    }
   ],
   "source": [
    "# Python code\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "documents = [\"apple orange apple\", \"apple lemon\", \"orange lemon\"]\n",
    "N = len(documents)\n",
    "idf_dict = Counter()\n",
    "tf_dict_list = []\n",
    "\n",
    "for doc in documents:\n",
    "    tf_dict = Counter(doc.split())\n",
    "    tf_dict_list.append(tf_dict)\n",
    "    for word in tf_dict.keys():\n",
    "        idf_dict[word] += 1\n",
    "        \n",
    "for word in idf_dict.keys():\n",
    "    idf_dict[word] = math.log(N / idf_dict[word])\n",
    "    \n",
    "tfidf_documents = []\n",
    "\n",
    "for tf_dict in tf_dict_list:\n",
    "    tfidf = {}\n",
    "    for word, count in tf_dict.items():\n",
    "        tfidf[word] = count * idf_dict[word]\n",
    "    tfidf_documents.append(tfidf)\n",
    "\n",
    "print(tfidf_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'apple': 0.8109302162163288, 'orange': 0.4054651081081644}, {'apple': 0.4054651081081644, 'lemon': 0.4054651081081644}, {'orange': 0.4054651081081644, 'lemon': 0.4054651081081644}]\n"
     ]
    }
   ],
   "source": [
    "# PySpark code\n",
    "documents = [\"apple orange apple\", \"apple lemon\", \"orange lemon\"]\n",
    "rdd = sc.parallelize(documents)\n",
    "\n",
    "# TF calculation\n",
    "tf_rdd = rdd.map(lambda doc: Counter(doc.split()))\n",
    "\n",
    "# IDF calculation\n",
    "idf_rdd = tf_rdd.flatMap(lambda tf: tf.keys()).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "N = rdd.count()\n",
    "idf_values = idf_rdd.mapValues(lambda count: math.log(N / count)).collectAsMap()\n",
    "\n",
    "# TF-IDF calculation\n",
    "tfidf_rdd = tf_rdd.map(lambda tf: {word: tf[word] * idf_values[word] for word in tf})\n",
    "\n",
    "tfidf = tfidf_rdd.collect()\n",
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
