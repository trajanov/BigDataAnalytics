{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis model using PySpark\n",
    "The task is to create a sentiment analysis model using PySpark that can classify movie reviews as either positive or negative. The input data consists of a large dataset of movie reviews that have been labeled with a sentiment score (positive or negative). The goal is to build a binary classification model that can accurately predict the sentiment of a new movie review that it has not seen before. \n",
    "\n",
    "The sentiment analysis model will use natural language processing (NLP) techniques to preprocess the text data, including tokenization, stopword removal, count vectorization, and TF-IDF. It will then use different machine learning algorithms, such as logistic regression, SVM, or a multilayer perceptron, to train a binary classifier on the preprocessed text features. The model will be evaluated using metrics precision, recall, F1 score, and confusion matrix. \n",
    "\n",
    "In the end, we show how feature selection techniques such as Chi-Square can be included in the preprocessing pipeline, and we build a logistic regression model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local[*]\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                text|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "|Petter Mattei's L...| positive|\n",
      "|Probably my all-t...| positive|\n",
      "|I sure would like...| positive|\n",
      "|This show was an ...| negative|\n",
      "|Encouraged by the...| negative|\n",
      "|If you like origi...| positive|\n",
      "+--------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tFile=\"data\\IMDB Dataset.csv.bz2\"\n",
    "df0 = spark.read.csv(tFile,header=True)\n",
    "df0.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample the data for faster model training (use the full dataset in reality)\n",
    "df0 = df0.sample(0.25, seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment to numbers positive =1, negative =0\n",
    "df0 = df0.withColumn(\"label\", F.when(F.col(\"sentiment\")==\"positive\",1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
      "|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |sentiment|label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
      "|One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |positive |1    |\n",
      "|This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it's continued its decline further to the complete waste of time it is today.<br /><br />It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I can't believe it's still on the air.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |negative |0    |\n",
      "|So im not a big fan of Boll's work but then again not many are. I enjoyed his movie Postal (maybe im the only one). Boll apparently bought the rights to use Far Cry long ago even before the game itself was even finsished. <br /><br />People who have enjoyed killing mercs and infiltrating secret research labs located on a tropical island should be warned, that this is not Far Cry... This is something Mr Boll have schemed together along with his legion of schmucks.. Feeling loneley on the set Mr Boll invites three of his countrymen to play with. These players go by the names of Til Schweiger, Udo Kier and Ralf Moeller.<br /><br />Three names that actually have made them selfs pretty big in the movie biz. So the tale goes like this, Jack Carver played by Til Schweiger (yes Carver is German all hail the bratwurst eating dudes!!) However I find that Tils acting in this movie is pretty badass.. People have complained about how he's not really staying true to the whole Carver agenda but we only saw carver in a first person perspective so we don't really know what he looked like when he was kicking a**.. <br /><br />However, the storyline in this film is beyond demented. We see the evil mad scientist Dr. Krieger played by Udo Kier, making Genetically-Mutated-soldiers or GMS as they are called. Performing his top-secret research on an island that reminds me of SPOILER Vancouver for some reason. Thats right no palm trees here. Instead we got some nice rich lumberjack-woods. We haven't even gone FAR before I started to CRY (mehehe) I cannot go on any more.. If you wanna stay true to Bolls shenanigans then go and see this movie you will not be disappointed it delivers the true Boll experience, meaning most of it will suck.<br /><br />There are some things worth mentioning that would imply that Boll did a good work on some areas of the film such as some nice boat and fighting scenes. Until the whole cromed/albino GMS squad enters the scene and everything just makes me laugh.. The movie Far Cry reeks of scheisse (that's poop for you simpletons) from a fa,r if you wanna take a wiff go ahead.. BTW Carver gets a very annoying sidekick who makes you wanna shoot him the first three minutes he's on screen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |negative |0    |\n",
      "|The Karen Carpenter Story shows a little more about singer Karen Carpenter's complex life. Though it fails in giving accurate facts, and details.<br /><br />Cynthia Gibb (portrays Karen) was not a fine election. She is a good actress , but plays a very naive and sort of dumb Karen Carpenter. I think that the role needed a stronger character. Someone with a stronger personality.<br /><br />Louise Fletcher role as Agnes Carpenter is terrific, she does a great job as Karen's mother.<br /><br />It has great songs, which could have been included in a soundtrack album. Unfortunately they weren't, though this movie was on the top of the ratings in USA and other several countries                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |positive |1    |\n",
      "|The Cell is an exotic masterpiece, a dizzying trip into not only the vast mind of a serial killer, but also into one of a very talented director. This is conclusive evidence of what can be achieved if human beings unleash their uninhibited imaginations. This is boldness at work, pushing aside thoughts to fall into formulas and cliches and creating something truly magnificent. This is the best movie of the year to date.<br /><br />I've read numerous complaints about this film, anywhere from all style and no substance to poorly cast characters and bad acting. To negatively criticize this film is to miss the point. This movie may be a landmark, a tradition where future movies will hopefully follow. The Cell has just opened the door to another world of imagination. So can we slam the door in its face and tell it and its director Tarsem Singh that we don't want any more? Personally, I would more than welcome another movie by Tarsem, and would love to see someone try to challenge him.<br /><br />We've all heard talk about going inside the mind of a serial killer, and yes, I do agree that the genre is a bit overworked. The 90s were full of movies trying to depict what makes serial killers tick; some of them worked, but most failed. But The Cell does not blaze down the same trail, we are given a new twist, we are physically transported into the mind and presented with nothing less than a fascinating journey of the most mysterious subject matter ever studied.<br /><br />I like how the movie does not bog us down with too much scientific jargon trying to explain how Jennifer Lopez actually gets to enter the brain of another. Instead, she just lies down on a laboratory table and is wrapped with what looks like really long Twizzlers and jaunted into another entity. The Cell wants to let you see what it's all about and not how it's all about, and I guess that's what some people don't like. True, I do like explanations with my movies, but when a movie ventures onto new ground you must let it do what it desires and simply take it in.<br /><br />I noticed how the film was very dark when it showed reality, maybe to contrast the bright visuals when inside the brain of another. Nonetheless, the set design was simply astonishing. I wouldn't be surprised if this film took home a few Oscars in cinematography, best costumes, best director and the like. If it were up to me it'd at least get nominated for best picture.<br /><br />I've noticed that I've kind of been repeating myself. Not because there's nothing else to say, but because I can't stress enough how fantastic I thought The Cell was. If you walk into the movie with a very open mind and to have it taken over with wonders and an eye-popping feast then you are assured a good time. I guess this film was just a little too much for some people, writing it off as weird or crazy. I am very much into psychology and the imagination of the human mind, so it was right down my alley. Leaving the theater, I heard one audience member say Whoever made that movie sure did a lot of good drugs. If so, I want what he was smoking.<br /><br />**** (out of 4)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |positive |1    |\n",
      "|'War movie' is a Hollywood genre that has been done and redone so many times that clichéd dialogue, rehashed plot and over-the-top action sequences seem unavoidable for any conflict dealing with large-scale combat. Once in a while, however, a war movie comes along that goes against the grain and brings a truly original and compelling story to life on the silver screen. The Civil War-era Cold Mountain, starring Jude Law, Nicole Kidman and Renée Zellweger is such a film.<br /><br />Then again, calling Cold Mountain a war movie is not entirely accurate. True enough, the film opens with a (quite literally) quick-and-dirty battle sequence that puts Glory director Edward Zwick shame. However, Cold Mountain is not so much about the Civil War itself as it is about the period and the people of the times. The story centers around disgruntled Confederate soldier Inman, played by Jude Law, who becomes disgusted with the gruesome war and homesick for the beautiful hamlet of Cold Mountain, North Carolina and the equally beautiful southern belle he left behind, Ada Monroe, played by Nicole Kidman. At first glance, this setup appears formulaic as the romantic interest back home gives the audience enough sympathy to root for the reluctant soldier's tribulations on the battlefield. Indeed, the earlier segments of the film are relatively unimpressive and even somewhat contrived.<br /><br />Cold Mountain soon takes a drastic turn, though, as the intrepid hero Inman turns out to be a deserter (incidentally saving the audience from the potentially confusing scenario of wanting to root for the Confederates) and begins a long odyssey homeward. Meanwhile, back at the farm, Ada's cultured ways prove of little use in the fields; soon she is transformed into something of a wilderbeast. Coming to Ada's rescue is the course, tough-as-nails Ruby Thewes, played by Renée Zellweger, who helps Ada put the farm back together and, perhaps more importantly, cope with the loneliness and isolation the war seems to have brought upon Ada.<br /><br />Within these two settings, a vivid, compelling and, at times, very disturbing portrait of the war-torn South unfolds. The characters with whom Inman and Ada interact are surprisingly complex, enhanced by wonderful performances of Brendan Gleeson as Ruby's deadbeat father, Ray Winstone as an unrepentant southern lawman, and Natalie Portman as a deeply troubled and isolated young mother. All have been greatly affected and changed by the war of Northern aggression, mostly for the worse. The dark, pervading anti-war message, accented by an effective, haunting score and chillingly beautiful shots of Virginia and North Carolina, is communicated to the audience not so much by gruesome battle scenes as by the scarred land and traumatized people for which the war was fought. Though the weapons and tactics of war itself have changed much in the past century, it's hellish effect on the land is timelessly relevant.<br /><br />Director Anthony Minghella manages to maintain this gloomy mood for most of the film, but the atmosphere is unfortunately denigrated by a rather tepid climax that does little justice to the wonderfully formed characters. The love story between Inman and Ada is awkwardly tacked onto the beginning and end of the film, though the inherently distant, abstracted and even absurd nature of their relationship in a way fits the dismal nature of the rest of the plot.<br /><br />Make no mistake, Cold Mountain has neither the traits of a feel-good romance nor an inspiring war drama. It is a unique vision of an era that is sure not only to entertain but also to truly absorb the audience into the lives of a people torn apart by a war and entirely desperate to be rid of its terrible repercussions altogether.                                                                                                                                                                                                                                     |positive |1    |\n",
      "|My first exposure to the Templarios & not a good one. I was excited to find this title among the offerings from Anchor Bay Video, which has brought us other cult classics such as Spider Baby. The print quality is excellent, but this alone can't hide the fact that the film is deadly dull. There's a thrilling opening sequence in which the villagers exact a terrible revenge on the Templars (& set the whole thing in motion), but everything else in the movie is slow, ponderous &, ultimately, unfulfilling. Adding insult to injury: the movie was dubbed, not subtitled, as promised on the video jacket.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |negative |0    |\n",
      "|One of the most significant quotes from the entire film is pronounced halfway through by the protagonist, the mafia middle-man Titta Di Girolamo, a physically non-descript, middle-aged man originally from Salerno in Southern Italy. When we're introduced to him at the start of the film, he's been living a non-life in an elegant but sterile hotel in the Italian-speaking Canton of Switzerland for the last ten years, conducting a business we are only gradually introduced to. While this pivotal yet apparently unremarkable scene takes place employees of the the Swiss bank who normally count Di Girolamo's cash tell him that 10,000 dollars are missing from his usual suitcase full of tightly stacked banknotes. At the news, he quietly but icily threatens his coaxing bank manager of wanting to close down his account. Meanwhile he tells us, the spectators, that when you bluff, you have to bluff right through to the end without fear of being caught out or appearing ridiculous. He says: you can't bluff for a while and then halfway through, tell the truth. Having eventually done this - bluffed only halfway through and told the truth, and having accepted the consequences of life and ultimately, love - is exactly the reason behind the beginning of Titta Di Girolamo's troubles. <br /><br />This initially unsympathetic character, a scowling, taciturn, curt man on the verge of 50, a man who won't even reply in kind to chambermaids and waitresses who say hello and goodbye, becomes at one point someone the spectator cares deeply about. At one point in his non-life, Titta decides to feel concern about appearing ridiculous. The first half of the film may be described as slow by some. It does indeed reveal Di Girolamo's days and nights in that hotel at an oddly disjoined, deliberate pace, revealing seemingly mundane and irrelevant details. However, scenes that may have seemed unnecessary reveal just how essential they are as this masterfully constructed and innovative film unfolds before your eyes. The existence of Titta Di Girolamo - the man with no imagination, identity or life, the unsympathetic character you unexpectedly end up loving and feeling for when you least thought you would - is also conveyed with elegantly edited sequences and very interesting use of music (one theme by the Scottish band Boards of Canada especially stood out). <br /><br />Never was the contrast between the way Hollywood and Italy treat mobsters more at odds than since the release of films such as Le Conseguenze dell'Amore or L'Imbalsamatore. Another interesting element was the way in which the film made use of the protagonist's insomnia. Not unlike The Machinist (and in a far more explicit way, the Al Pacino film Insomnia), Le Conseguenze dell'Amore uses this condition to symbolise a deeper emotional malaise that's been rammed so deep into the obscurity of the unconscious, it's almost impossible to pin-point its cause (if indeed there is one). <br /><br />The young and sympathetic hotel waitress Sofia (played by Olivia Magnani, grand-daughter of the legendary Anna) and the memory of Titta's best friend, a man whom he hasn't seen in 20 years, unexpectedly provide a tiny window onto life that Titta eventually (though tentatively at first) accepts to look through again. Though it's never explicitly spelt out, the spectator KNOWS that to a man like Titta, accepting The Consequences of Love will have unimaginable consequences. A film without a single scene of sex or violence, a film that unfolds in its own time and concedes nothing to the spectator's expectations, Le Conseguenze dell'Amore is a fine representative of that small, quiet, discreet Renaissance that has been taking place in Italian cinema since the decline of Cinecittà during the second half of the 70s. The world is waiting for Italy to produce more Il Postino-like fare, more La Vita è Bella-style films... neglecting to explore fine creations like Le Conseguenze dell'Amore, L'Imbalsamatore and others. Your loss, world.|positive |1    |\n",
      "|This movie is based on the book, A Many Splendored Thing by Han Suyin and tackles issues of race relations between Asians and Whites, a topic that comes from Han's personal experiences as an Eurasian growing up in China. That background, and the beautiful Hong Kong settings, gives this love story a unique and rather daring atmosphere for its time.<br /><br />Other than that, the story is a stereotypical romance with a memorable song that is perhaps more remembered than the movie itself. The beautiful Jennifer Jones looks the part and gives a wonderful, Oscar nominated performance as a doctor of mixed breed during the advent of Communism in mainland China. William Holden never looked better playing a romantic lead as a journalist covering war torn regions in the world. The acting is top notch, and the chemistry between the two lovers provides for some genuine moments of silver screen affection sure to melt the hearts of those who are romantically inclined.<br /><br />The cinematography really brings out fifty's Hong Kong, especially the hilltop overlooking the harbor where the two lovers spend their most intimate moments. The ending is a real tear-jerker. Some may consider sentimental romances passé, but, for those who enjoy classic Hollywood love stories, this is a shining example.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |positive |1    |\n",
      "|Of all the films I have seen, this one, The Rage, has got to be one of the worst yet. The direction, LOGIC, continuity, changes in plot-script and dialog made me cry out in pain. How could ANYONE come up with something so crappy? Gary Busey is know for his B movies, but this is a sure W movie. (W=waste).<br /><br />Take for example: about two dozen FBI & local law officers surround a trailer house with a jeep wagoneer. Inside the jeep is MA and is confused as to why all the cops are about. Within seconds a huge gun battle ensues, MA being killed straight off. The cops blast away at the jeep with gary and company blasting away at them. The cops fall like dominoes and the jeep with Gary drives around in circles and are not hit by one single bullet/pellet. MA is killed and gary seems to not to have noticed-damn that guy is tough. Truly a miracle, not since the six-shooter held 300 bullets has there been such a miracle.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |negative |0    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>text_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12643</th>\n",
       "      <td>To be hones, I used to like this show and watc...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>hones  used  like this show and watch  regul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12644</th>\n",
       "      <td>This movie is a disgrace to the Major League F...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>this movie   disgrace  the major league franch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>John Garfield plays a Marine who is blinded by...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>john garfield plays  marine who  blinded   gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>bad plot bad dialogue bad acting idiotic direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12647</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>going  have  disagree with the previous comme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  label   \n",
       "12643  To be hones, I used to like this show and watc...  negative      0  \\\n",
       "12644  This movie is a disgrace to the Major League F...  negative      0   \n",
       "12645  John Garfield plays a Marine who is blinded by...  positive      1   \n",
       "12646  Bad plot, bad dialogue, bad acting, idiotic di...  negative      0   \n",
       "12647  I'm going to have to disagree with the previou...  negative      0   \n",
       "\n",
       "                                                  text_c  \n",
       "12643    hones  used  like this show and watch  regul...  \n",
       "12644  this movie   disgrace  the major league franch...  \n",
       "12645  john garfield plays  marine who  blinded   gre...  \n",
       "12646  bad plot bad dialogue bad acting idiotic direc...  \n",
       "12647   going  have  disagree with the previous comme...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove html tags from text\n",
    "df0 = df0.withColumn(\"text_c\", F.regexp_replace(F.col(\"text\"), r'<[^>]+>', \"\"));\n",
    "# Remove non-letters\n",
    "df0 = df0.withColumn(\"text_c\", F.regexp_replace(\"text_c\", r\"[^a-zA-Z ]\", \"\"))\n",
    "# Remove words 1, 2 char\n",
    "df0 = df0.withColumn(\"text_c\", F.regexp_replace(\"text_c\", r\"\\b\\w{1,2}\\b\", \"\"))\n",
    "# to lower case\n",
    "df0 = df0.withColumn(\"text_c\", F.lower(F.col(\"text_c\")))\n",
    "df0.toPandas().tail(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization (optional)\n",
    "Lemmatization is the process of reducing a word to its base or root form, which is also known as a lemma. The purpose of lemmatization is to simplify text and make it easier to analyze by grouping together different forms of the same word. For example, the words \"running,\" \"ran,\" and \"runs\" can all be reduced to the base form \"run\" through lemmatization. \n",
    "\n",
    "However, lemmatization can be a **time-consuming operation**, especially when dealing with large amounts of text data. This is because the process involves analyzing each word in a text and identifying its base form. It also requires a comprehensive understanding of the grammatical rules of a language to accurately identify the correct lemma for each word.\n",
    "\n",
    "Despite its time-consuming nature, lemmatization can be a powerful tool in natural language processing and text analysis. It can help with tasks such as sentiment analysis, topic modeling, and text classification. When using lemmatization, it's important to use it carefully and correctly to ensure that the text is properly processed and analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>text_c</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12643</th>\n",
       "      <td>To be hones, I used to like this show and watc...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>hone   use   like this show and watch   reg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12644</th>\n",
       "      <td>This movie is a disgrace to the Major League F...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>this movie    disgrace   the major league fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>John Garfield plays a Marine who is blinded by...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>john garfield play   marine who   blind    gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12646</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>bad plot bad dialogue bad act idiotic direct t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12647</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>go   have   disagree with the previous comme...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment  label   \n",
       "12643  To be hones, I used to like this show and watc...  negative      0  \\\n",
       "12644  This movie is a disgrace to the Major League F...  negative      0   \n",
       "12645  John Garfield plays a Marine who is blinded by...  positive      1   \n",
       "12646  Bad plot, bad dialogue, bad acting, idiotic di...  negative      0   \n",
       "12647  I'm going to have to disagree with the previou...  negative      0   \n",
       "\n",
       "                                                  text_c  \n",
       "12643     hone   use   like this show and watch   reg...  \n",
       "12644  this movie    disgrace   the major league fran...  \n",
       "12645  john garfield play   marine who   blind    gre...  \n",
       "12646  bad plot bad dialogue bad act idiotic direct t...  \n",
       "12647    go   have   disagree with the previous comme...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Define a function to apply the lemmatizer to a text\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Define a UDF to apply the lemmatizer to a column\n",
    "lemmatize_udf = udf(lemmatize_text, StringType())\n",
    "\n",
    "# Apply the UDF to a DataFrame column\n",
    "df0 = df0.withColumn(\"text_c\", lemmatize_udf(df0[\"text_c\"]))\n",
    "\n",
    "# Caching must be used !!!!!!\n",
    "df0 = df0.cache()\n",
    "df0.toPandas().tail(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the text to training and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1| 5016|\n",
      "|    0| 5138|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data in train and test (80%-20%)\n",
    "df, test = df0.randomSplit(weights=[0.8,0.2], seed=200)\n",
    "df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign weights to classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5060074847350797 0.4939925152649202\n"
     ]
    }
   ],
   "source": [
    "# Create a weight of each class\n",
    "from pyspark.sql import functions as F\n",
    "size = df.count()\n",
    "number_of_1 = df.filter('label == 1').count()\n",
    "number_of_0 = size - number_of_1\n",
    "p_weight = number_of_1/ size\n",
    "n_weight = number_of_0/ size\n",
    "print(n_weight, p_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "|                text|sentiment|label|              text_c|            weight|\n",
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "| Så som i himmele...| positive|    1|   som   himmelen...|0.5060074847350797|\n",
      "| While sporadical...| negative|    0|  while sporadica...|0.4939925152649202|\n",
      "|'Blue Desert' may...| negative|    0|blue desert may h...|0.4939925152649202|\n",
      "|'Checking Out' is...| positive|    1|check out    extr...|0.5060074847350797|\n",
      "|'Presque Rien' ('...| positive|    1|presque rien come...|0.5060074847350797|\n",
      "+--------------------+---------+-----+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"weight\", F.when(F.col(\"label\")==1,n_weight).otherwise(p_weight))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing pipeline\n",
    "Pipelines in Spark are a powerful tool for data processing and analysis, as they enable the creation of complex data workflows that can be executed efficiently on distributed computing systems. They also simplify the data processing and analysis tasks, as they enable the chaining of multiple stages into a single workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the review text\n",
    "tokenizer = Tokenizer(inputCol=\"text_c\", outputCol=\"words\",)\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "# Create a count vectorizer\n",
    "countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "# Calculate the TF-IDF\n",
    "idf = IDF(inputCol=countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
    "# Crate a preprocessing pipeline with 4 stages\n",
    "pipeline_p = Pipeline(stages=[tokenizer,remover, countVectorizer, idf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn the data preprocessing model\n",
    "data_model = pipeline_p.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>text_c</th>\n",
       "      <th>weight</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>featuresIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Så som i himmelen  .. as above so below.. tha...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>som   himmelen     above   below that very ...</td>\n",
       "      <td>0.506007</td>\n",
       "      <td>[, , , som, , , himmelen, , , , , above, , , b...</td>\n",
       "      <td>[, , , som, , , himmelen, , , , , , , special,...</td>\n",
       "      <td>(77.0, 0.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.015166436921203202, 0.0, 2.144354902634974,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>While sporadically engrossing (including a fe...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>while sporadically engross include   few eff...</td>\n",
       "      <td>0.493993</td>\n",
       "      <td>[, , while, sporadically, engross, include, , ...</td>\n",
       "      <td>[, , sporadically, engross, include, , , effec...</td>\n",
       "      <td>(72.0, 0.0, 3.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.014181603354891307, 0.0, 1.6082661769762305...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Blue Desert' may have had the potential to be...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>blue desert may have have the potential    eve...</td>\n",
       "      <td>0.493993</td>\n",
       "      <td>[blue, desert, may, have, have, the, potential...</td>\n",
       "      <td>[blue, desert, may, potential, , , , even, , ,...</td>\n",
       "      <td>(93.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.01831790433340127, 0.8892534776237404, 0.53...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Checking Out' is an extraordinary film that t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>check out    extraordinary film that tower abo...</td>\n",
       "      <td>0.506007</td>\n",
       "      <td>[check, out, , , , extraordinary, film, that, ...</td>\n",
       "      <td>[check, , , , extraordinary, film, tower, film...</td>\n",
       "      <td>(40.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.00787866853049517, 0.0, 1.6082661769762305,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Presque Rien' ('Come Undone') is an earlier w...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>presque rien come undo    early work   the ino...</td>\n",
       "      <td>0.506007</td>\n",
       "      <td>[presque, rien, come, undo, , , , early, work,...</td>\n",
       "      <td>[presque, rien, come, undo, , , , early, work,...</td>\n",
       "      <td>(232.0, 0.0, 6.0, 3.0, 0.0, 2.0, 0.0, 0.0, 0.0...</td>\n",
       "      <td>(0.04569627747687199, 0.0, 3.216532353952461, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  label   \n",
       "0   Så som i himmelen  .. as above so below.. tha...  positive      1  \\\n",
       "1   While sporadically engrossing (including a fe...  negative      0   \n",
       "2  'Blue Desert' may have had the potential to be...  negative      0   \n",
       "3  'Checking Out' is an extraordinary film that t...  positive      1   \n",
       "4  'Presque Rien' ('Come Undone') is an earlier w...  positive      1   \n",
       "\n",
       "                                              text_c    weight   \n",
       "0     som   himmelen     above   below that very ...  0.506007  \\\n",
       "1    while sporadically engross include   few eff...  0.493993   \n",
       "2  blue desert may have have the potential    eve...  0.493993   \n",
       "3  check out    extraordinary film that tower abo...  0.506007   \n",
       "4  presque rien come undo    early work   the ino...  0.506007   \n",
       "\n",
       "                                               words   \n",
       "0  [, , , som, , , himmelen, , , , , above, , , b...  \\\n",
       "1  [, , while, sporadically, engross, include, , ...   \n",
       "2  [blue, desert, may, have, have, the, potential...   \n",
       "3  [check, out, , , , extraordinary, film, that, ...   \n",
       "4  [presque, rien, come, undo, , , , early, work,...   \n",
       "\n",
       "                                            filtered   \n",
       "0  [, , , som, , , himmelen, , , , , , , special,...  \\\n",
       "1  [, , sporadically, engross, include, , , effec...   \n",
       "2  [blue, desert, may, potential, , , , even, , ,...   \n",
       "3  [check, , , , extraordinary, film, tower, film...   \n",
       "4  [presque, rien, come, undo, , , , early, work,...   \n",
       "\n",
       "                                         rawFeatures   \n",
       "0  (77.0, 0.0, 4.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...  \\\n",
       "1  (72.0, 0.0, 3.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0,...   \n",
       "2  (93.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "3  (40.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "4  (232.0, 0.0, 6.0, 3.0, 0.0, 2.0, 0.0, 0.0, 0.0...   \n",
       "\n",
       "                                         featuresIDF  \n",
       "0  (0.015166436921203202, 0.0, 2.144354902634974,...  \n",
       "1  (0.014181603354891307, 0.0, 1.6082661769762305...  \n",
       "2  (0.01831790433340127, 0.8892534776237404, 0.53...  \n",
       "3  (0.00787866853049517, 0.0, 1.6082661769762305,...  \n",
       "4  (0.04569627747687199, 0.0, 3.216532353952461, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform\n",
    "transformed_data = data_model.transform(df)\n",
    "transformed_data.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "      <th>text_c</th>\n",
       "      <th>words</th>\n",
       "      <th>filtered</th>\n",
       "      <th>rawFeatures</th>\n",
       "      <th>featuresIDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Don't Look In the Basement' is so easy to kno...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>do not look   the basement    easy   knock but...</td>\n",
       "      <td>[do, not, look, , , the, basement, , , , easy,...</td>\n",
       "      <td>[look, , , basement, , , , easy, , , knock, tr...</td>\n",
       "      <td>(63.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.012408902935529893, 0.0, 0.5360887256587435...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>*Flat SPOILERS* &lt;br /&gt;&lt;br /&gt;Five med students,...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>flat spoiler five med student nelson kiefer su...</td>\n",
       "      <td>[flat, spoiler, five, med, student, nelson, ki...</td>\n",
       "      <td>[flat, spoiler, five, med, student, nelson, ki...</td>\n",
       "      <td>(67.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0,...</td>\n",
       "      <td>(0.01319676978857941, 0.8892534776237404, 0.0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.... may seem far fetched.... but there really...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>may seem far fetched but there really be   r...</td>\n",
       "      <td>[, , may, seem, far, fetched, but, there, real...</td>\n",
       "      <td>[, , may, seem, far, fetched, really, , , real...</td>\n",
       "      <td>(69.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.013590703215104168, 0.0, 0.0, 0.0, 0.0, 0.6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...Our the grandpa's hour.&lt;br /&gt;&lt;br /&gt;More tha...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "      <td>our the grandpas hourmore than the gangster it...</td>\n",
       "      <td>[our, the, grandpas, hourmore, than, the, gang...</td>\n",
       "      <td>[grandpas, hourmore, gangster, , , detailed, d...</td>\n",
       "      <td>(36.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,...</td>\n",
       "      <td>(0.007090801677445653, 0.0, 0.5360887256587435...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...but I regret having seen it. Since the rati...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "      <td>but   regret have see   since the rating   imd...</td>\n",
       "      <td>[but, , , regret, have, see, , , since, the, r...</td>\n",
       "      <td>[, , regret, see, , , since, rating, , , imdb,...</td>\n",
       "      <td>(75.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 0.0, 0.0,...</td>\n",
       "      <td>(0.014772503494678443, 0.8892534776237404, 1.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment  label   \n",
       "0  'Don't Look In the Basement' is so easy to kno...  positive      1  \\\n",
       "1  *Flat SPOILERS* <br /><br />Five med students,...  positive      1   \n",
       "2  .... may seem far fetched.... but there really...  negative      0   \n",
       "3  ...Our the grandpa's hour.<br /><br />More tha...  positive      1   \n",
       "4  ...but I regret having seen it. Since the rati...  negative      0   \n",
       "\n",
       "                                              text_c   \n",
       "0  do not look   the basement    easy   knock but...  \\\n",
       "1  flat spoiler five med student nelson kiefer su...   \n",
       "2    may seem far fetched but there really be   r...   \n",
       "3  our the grandpas hourmore than the gangster it...   \n",
       "4  but   regret have see   since the rating   imd...   \n",
       "\n",
       "                                               words   \n",
       "0  [do, not, look, , , the, basement, , , , easy,...  \\\n",
       "1  [flat, spoiler, five, med, student, nelson, ki...   \n",
       "2  [, , may, seem, far, fetched, but, there, real...   \n",
       "3  [our, the, grandpas, hourmore, than, the, gang...   \n",
       "4  [but, , , regret, have, see, , , since, the, r...   \n",
       "\n",
       "                                            filtered   \n",
       "0  [look, , , basement, , , , easy, , , knock, tr...  \\\n",
       "1  [flat, spoiler, five, med, student, nelson, ki...   \n",
       "2  [, , may, seem, far, fetched, really, , , real...   \n",
       "3  [grandpas, hourmore, gangster, , , detailed, d...   \n",
       "4  [, , regret, see, , , since, rating, , , imdb,...   \n",
       "\n",
       "                                         rawFeatures   \n",
       "0  (63.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0,...  \\\n",
       "1  (67.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0,...   \n",
       "2  (69.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0,...   \n",
       "3  (36.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0,...   \n",
       "4  (75.0, 2.0, 2.0, 1.0, 1.0, 0.0, 2.0, 0.0, 0.0,...   \n",
       "\n",
       "                                         featuresIDF  \n",
       "0  (0.012408902935529893, 0.0, 0.5360887256587435...  \n",
       "1  (0.01319676978857941, 0.8892534776237404, 0.0,...  \n",
       "2  (0.013590703215104168, 0.0, 0.0, 0.0, 0.0, 0.6...  \n",
       "3  (0.007090801677445653, 0.0, 0.5360887256587435...  \n",
       "4  (0.014772503494678443, 0.8892534776237404, 1.0...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data\n",
    "transformed_test = data_model.transform(test)\n",
    "transformed_test.toPandas().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tokenizer_a4c2535eb4c8,\n",
       " StopWordsRemover_1e769b2be490,\n",
       " CountVectorizerModel: uid=CountVectorizer_16ff7fcd9e74, vocabularySize=1000,\n",
       " IDFModel: uid=IDF_9aaf4031d182, numDocs=10154, numFeatures=1000]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the sages of the pipeline\n",
    "data_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'movie',\n",
       " 'film',\n",
       " 'one',\n",
       " 'see',\n",
       " 'make',\n",
       " 'like',\n",
       " 'good',\n",
       " 'get',\n",
       " 'well',\n",
       " 'time',\n",
       " 'character',\n",
       " 'watch',\n",
       " 'bad',\n",
       " 'even',\n",
       " 'story',\n",
       " 'really',\n",
       " 'think',\n",
       " 'show',\n",
       " 'scene']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary of the CountVectorizer\n",
    "data_model.stages[2].vocabulary[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.evaluation import MultilabelMetrics\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "def m_metrics_l(ml_model,test_data):\n",
    "    predictions = ml_model.transform(test_data).cache()\n",
    "    predictionAndLabels = predictions.select(\"label\",\"prediction\").rdd.map(lambda x: (float(x[0]), float(x[1]))).cache()\n",
    "    \n",
    "    # Print some predictions vs labels\n",
    "    # print(predictionAndLabels.take(10))\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1Score = metrics.fMeasure(1.0)\n",
    "    print(f\"Precision = {precision:.4f} Recall = {recall:.4f} F1 Score = {f1Score:.4f}\")\n",
    "    print(\"Confusion matrix \\n\", metrics.confusionMatrix().toArray().astype(int))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Models\n",
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Model created in 1.91s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8690 Recall = 0.8356 F1 Score = 0.8520\n",
      "Confusion matrix \n",
      " [[1001  167]\n",
      " [ 218 1108]]\n",
      "Total time 10.36s.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "classifier = LogisticRegression(maxIter=10, regParam=0.1, featuresCol = \"featuresIDF\", weightCol=\"weight\")\n",
    "start = time.time()\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "print(f\"Training started.\")\n",
    "model = pipeline.fit(transformed_data)\n",
    "print(f\"Model created in {time.time()-start:.2f}s.\")\n",
    "m_metrics_l(model,transformed_test)\n",
    "print(f\"Total time {time.time()-start:.2f}s.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Model created in 9.57s.\n",
      "Precision = 0.8698 Recall = 0.6940 F1 Score = 0.7720\n",
      "Confusion matrix \n",
      " [[ 730  166]\n",
      " [ 489 1109]]\n",
      "Total time 18.01s.\n"
     ]
    }
   ],
   "source": [
    "classifier = GBTClassifier(maxIter=10, featuresCol = \"featuresIDF\", weightCol=\"weight\", maxDepth=5)\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "start = time.time()\n",
    "print(f\"Training started.\")\n",
    "model = pipeline.fit(transformed_data)\n",
    "print(f\"Model created in {time.time()-start:.2f}s.\")\n",
    "m_metrics_l(model,transformed_test)\n",
    "print(f\"Total time {time.time()-start:.2f}s.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Model created in 1.54s.\n",
      "Precision = 0.8722 Recall = 0.8330 F1 Score = 0.8521\n",
      "Confusion matrix \n",
      " [[ 996  163]\n",
      " [ 223 1112]]\n",
      "Total time 10.10s.\n"
     ]
    }
   ],
   "source": [
    "classifier = LinearSVC(maxIter=10, regParam=0.1, featuresCol = \"featuresIDF\", weightCol=\"weight\")\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "start = time.time()\n",
    "print(f\"Training started.\")\n",
    "model = pipeline.fit(transformed_data)\n",
    "print(f\"Model created in {time.time()-start:.2f}s.\")\n",
    "m_metrics_l(model,transformed_test)\n",
    "print(f\"Total time {time.time()-start:.2f}s.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultilayerPerceptronClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Model created in 3.20s.\n",
      "Precision = 0.8643 Recall = 0.8406 F1 Score = 0.8523\n",
      "Confusion matrix \n",
      " [[1010  173]\n",
      " [ 209 1102]]\n",
      "Total time 12.05s.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# Multilayer Perceptron Classifier for a classification task with 1000 input features, a hidden layer with 30 nodes, and 2 output classes\n",
    "# The input layer must match the dimensionality of the input data currently = 1000\n",
    "layers = [1000, 30, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "classifier = MultilayerPerceptronClassifier(maxIter=10, layers=layers,featuresCol = \"featuresIDF\", blockSize=128, seed=1234)\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "start = time.time()\n",
    "print(f\"Training started.\")\n",
    "model = pipeline.fit(transformed_data)\n",
    "print(f\"Model created in {time.time()-start:.2f}s.\")\n",
    "m_metrics_l(model,transformed_test)\n",
    "print(f\"Total time {time.time()-start:.2f}s.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Selections\n",
    "In Spark the following Feature Selectors are available\n",
    "- VectorSlicer\n",
    "- RFormula\n",
    "- ChiSqSelector\n",
    "- UnivariateFeatureSelector\n",
    "- VarianceThresholdSelector\n",
    "\n",
    "We use the ChiSqSelector to reduce the number of features from 1000 to 200. Chi-square variable selection is a technique used in statistics and machine learning to identify the most relevant features or variables in a dataset for a given classification task. It is based on the chi-square test, which is a statistical test used to determine the independence of two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text_c\", outputCol=\"words\",)\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"filtered\")\n",
    "countVectorizer = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"rawFeatures\", vocabSize=1000)\n",
    "idf = IDF(inputCol=countVectorizer.getOutputCol(), outputCol=\"featuresIDF\")\n",
    "\n",
    "# Select the top 200 features based on their chi-squared test value\n",
    "selector = ChiSqSelector(numTopFeatures=200, featuresCol=idf.getOutputCol(), outputCol=\"features\", labelCol=\"label\")\n",
    "# Crate a preprocessing pipeline with 5 stages\n",
    "pipeline_p = Pipeline(stages=[tokenizer,remover, countVectorizer, idf,selector])\n",
    "# Learn the data preprocessing model\n",
    "data_model = pipeline_p.fit(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the selected features (words) by ChiSqSelector\n",
    "To print the selected words by the ChiSqSelector in the sentiment analysis model, we can use the selectedFeatures attribute of the transformer. This attribute returns an array of the indices of the selected features, which we can map back to the original vocabulary of the CountVectorizer to get the corresponding words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'make', 'like', 'good', 'get', 'watch', 'bad', 'even', 'story', 'great', 'look', 'say', 'also', 'play', 'love', 'thing', 'seem', 'life', 'want', 'plot', 'year', 'try', 'act', 'still', 'something', 'guy', 'performance', 'nothing', 'actually', 'young', 'role', 'become', 'point', 'minute', 'pretty', 'world', 'kill', 'horror', 're', 'mean', 'script', 'whole', 'least', 'may', 'always', 'enjoy', 'acting', 'live', 'family', 'series', 'anything', 'reason', 'effect', 'idea', 'fun', 'especially', 'bring', 'maybe', 'different', 'money', 'someone', 'shoot', 'job', 'true', 'recommend', 'waste', 'war', 'instead', 'hour', 'short', 'excellent', 'beautiful', 'else', 'view', 'half', 'attempt', 'poor', 'classic', 'suppose', 'human', 'stupid', 'rest', 'lack', 'either', 'meet', 'completely', 'wrong', 'save', 'dialogue', 'joke', 'awful', 'perfect', 'definitely', 'flick', 'terrible', 'fine', 'wonder', 'wonderful', 'sit', 'low', 'guess', 'experience', 'spend', 'fail', 'cut', 'throw', 'win', 'relationship', 'boring', 'favorite', 'horrible', 'rent', 'support', 'strong', 'heart', 'amazing', 'today', 'ill', 'brilliant', 'complete', 'chance', 'unfortunately', 'decent', 'obviously', 'simple', 'highly', 'silly', 'god', 'hilarious', 'crap', 'cheap', 'capture', 'dialog', 'none', 'seriously', 'apparently', 'touch', 'ridiculous', 'bore', 'annoying', 'avoid', 'modern', 'oscar', 'enjoyable', 'predictable', 'discover', 'deep', 'emotion', 'romantic', 'basically', 'suck', 'bunch', 'dull', 'entertaining', 'mess', 'york', 'fantastic', 'premise', 'realistic', 'sorry', 'lame', 'accent', 'spoiler', 'dumb', 'bother', 'masterpiece', 'appreciate', 'beauty', 'memorable', 'atmosphere', 'paul', 'perfectly', 'unless', 'poorly', 'superb', 'portrayal', 'powerful', 'personal', 'scream', 'okay', 'otherwise', 'badly', 'share', 'unique', 'fake', 'society', 'era', 'flat', 'awesome', 'pathetic', 'plain', 'neither', 'trash', 'pointless', 'rip', 'solid', 'excuse', 'complex', 'terrific', 'incredible']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = data_model.stages[2].vocabulary\n",
    "selected_indexes = data_model.stages[4].selectedFeatures\n",
    "selected_words = [vocabulary[i] for i in selected_indexes]\n",
    "print(selected_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the algorithm usin the slected features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "transformed_data = data_model.transform(df)\n",
    "transformed_test = data_model.transform(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "If we use ChiSqSelector to choose the top 200 features for the sentiment analysis model, we should anticipate that the model's performance may not be as high as when we use all 1000 features. The main factor causing this outcome is the reduction of information. By selecting only the top 200 features, we may discard vital information present in the remaining 800 features, which can lead to a decline in accuracy and overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Model created in 1.41s.\n",
      "Precision = 0.8667 Recall = 0.8131 F1 Score = 0.8390\n",
      "Confusion matrix \n",
      " [[ 965  170]\n",
      " [ 254 1105]]\n",
      "Total time 10.05s.\n"
     ]
    }
   ],
   "source": [
    "classifier = LogisticRegression(maxIter=5, featuresCol = \"features\")\n",
    "start = time.time()\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "print(f\"Training started.\")\n",
    "model = pipeline.fit(transformed_data)\n",
    "print(f\"Model created in {time.time()-start:.2f}s.\")\n",
    "m_metrics_l(model,transformed_test)\n",
    "print(f\"Total time {time.time()-start:.2f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started.\n",
      "Model created in 0.87s.\n",
      "Precision = 0.8604 Recall = 0.8072 F1 Score = 0.8330\n",
      "Confusion matrix \n",
      " [[ 957  178]\n",
      " [ 262 1097]]\n",
      "Total time 8.76s.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# Multilayer Perceptron Classifier for a classification task with 1000 input features, a hidden layer with 30 nodes, and 2 output classes\n",
    "# The input layer must match the dimensionality of the input data currently = 1000\n",
    "layers = [200, 8, 2]\n",
    "\n",
    "# create the trainer and set its parameters\n",
    "classifier = MultilayerPerceptronClassifier(maxIter=10, layers=layers,featuresCol = \"features\", blockSize=128, seed=1234)\n",
    "pipeline = Pipeline(stages=[classifier])\n",
    "start = time.time()\n",
    "print(f\"Training started.\")\n",
    "model = pipeline.fit(transformed_data)\n",
    "print(f\"Model created in {time.time()-start:.2f}s.\")\n",
    "m_metrics_l(model,transformed_test)\n",
    "print(f\"Total time {time.time()-start:.2f}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9db6cbf0fd79f8e79653fe7b0c50b956ca6e525ee712295da3c66f75e4fe96ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
