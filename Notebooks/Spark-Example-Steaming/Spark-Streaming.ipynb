{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Structured Streaming\n",
    "\n",
    "## Word frequency count example\n",
    "In this example, we will use PySpark Structured Streaming to count the frequency of words in a stream of text data. We will use Netcat to generate a stream of text data, which will be read by PySpark and processed to count the frequency of each word.\n",
    "\n",
    "**Step 1**\n",
    "To run Netcat on Windows, we will use the Windows Subsystem for Linux (WSL) with an Ubuntu distribution installed. If you are using MacOs or Linux, you can run Netcat directly on your system without using WSL.\n",
    "\n",
    "To lunch Netcat on Windows, you can open the Ubuntu application from the Start menu and run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "nc -lk 9999\n",
    "```\n",
    "\n",
    "This command will start Netcat on port 9999 and listen for incoming connections. You can now type text in the terminal and press Enter to send it to the PySpark application.\n",
    "\n",
    "**Step 2**\n",
    "We need to run the PySpark streaming applications. To do this, we will use the `spark-submit` command to run the `stream_wc.py` script. This script reads the stream of text data from Netcat, processes it to count the frequency of each word, and prints the results to the console. Open command prompt, navigate to the directory containing the `stream_wc.py` script, and run the following command:\n",
    "\n",
    "```bash\n",
    "spark-submit stream_wc.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDELT streaming example\n",
    "GDELT (Global Database of Events, Language, and Tone) is a global event database that extracts events from news articles around the world. The dataset is updated every 15 minutes and is available in CSV format. It contains detailed information such as event dates, actors involved, locations, and more. The latest dataset can be accessed via the URL:  \n",
    "http://data.gdeltproject.org/gdeltv2/lastupdate.txt\n",
    "\n",
    "There are two scripts in this example:\n",
    "\n",
    "- **gdelt-streaming.py:**  \n",
    "  This PySpark application continuously ingests streaming CSV files from the GDELT dataset. It enriches the data by joining it with a country mapping file, computes aggregations like the average Goldstein scale and event counts by country, and outputs real-time insights—specifically, the top 10 most positive and negative countries—directly to the console.\n",
    "\n",
    "- **gdelt_update.py:**  \n",
    "  This Python script periodically checks the GDELT last update URL to retrieve the latest CSV export (packaged as a ZIP file). It downloads and extracts the file into the designated `input_files` directory if it is not already present, ensuring that the most current data is available for processing. The script repeats this check every five minutes.\n",
    "\n",
    "**How to Run on Windows:**\n",
    "\n",
    "Open two Command Prompt windows to run the scripts concurrently. First run the `gdelt_update.py` script to download the latest GDELT data, then run the `gdelt-streaming.py` script to process the streaming data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
