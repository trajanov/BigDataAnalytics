{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "sc = SparkContext(\"local\")\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data frame Joining example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+-----------+\n",
      "|EmployeeID|   Name|Age| Department|\n",
      "+----------+-------+---+-----------+\n",
      "|         1|  Alice| 30|Engineering|\n",
      "|         2|    Bob| 25|Engineering|\n",
      "|         3|Charlie| 35|         HR|\n",
      "|         4|  David| 28|    Finance|\n",
      "|         5|    Eve| 22|         HR|\n",
      "+----------+-------+---+-----------+\n",
      "\n",
      "+-----------+-------------+\n",
      "| Department|     Location|\n",
      "+-----------+-------------+\n",
      "|Engineering|     New York|\n",
      "|         HR|San Francisco|\n",
      "|    Finance|  Los Angeles|\n",
      "+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, StructField, IntegerType, StructType\n",
    "# Create a list of tuples with sample employee data\n",
    "employee_data = [\n",
    "    (\"1\", \"Alice\", 30, \"Engineering\"),\n",
    "    (\"2\", \"Bob\", 25, \"Engineering\"),\n",
    "    (\"3\", \"Charlie\", 35, \"HR\"),\n",
    "    (\"4\", \"David\", 28, \"Finance\"),\n",
    "    (\"5\", \"Eve\", 22, \"HR\"),\n",
    "]\n",
    "\n",
    "# Define the schema for the employee DataFrame\n",
    "employee_schema = StructType([\n",
    "    StructField(\"EmployeeID\", StringType(), True),\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"Age\", IntegerType(), True),\n",
    "    StructField(\"Department\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create the employee DataFrame\n",
    "employee_df = spark.createDataFrame(employee_data, employee_schema)\n",
    "\n",
    "# Create a list of tuples with department data\n",
    "department_data = [\n",
    "    (\"Engineering\", \"New York\"),\n",
    "    (\"HR\", \"San Francisco\"),\n",
    "    (\"Finance\", \"Los Angeles\"),\n",
    "    # ... Add more department data if needed\n",
    "]\n",
    "\n",
    "# Define the schema for the department DataFrame\n",
    "department_schema = StructType([\n",
    "    StructField(\"Department\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True)\n",
    "])\n",
    "department_df = spark.createDataFrame(department_data, department_schema)\n",
    "\n",
    "employee_df.show()\n",
    "department_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join the Dataframes\n",
    "When we join dataframes that have the columns with the same nama then we can have a problem with the selection of the particular column. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+-----------+-----------+-------------+\n",
      "|EmployeeID|   Name|Age| Department| Department|     Location|\n",
      "+----------+-------+---+-----------+-----------+-------------+\n",
      "|         1|  Alice| 30|Engineering|Engineering|     New York|\n",
      "|         2|    Bob| 25|Engineering|Engineering|     New York|\n",
      "|         4|  David| 28|    Finance|    Finance|  Los Angeles|\n",
      "|         3|Charlie| 35|         HR|         HR|San Francisco|\n",
      "|         5|    Eve| 22|         HR|         HR|San Francisco|\n",
      "+----------+-------+---+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept = employee_df.join(department_df, employee_df.Department == department_df.Department, 'right')\n",
    "emp_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will rise the \"AnalysisException: Reference 'Department' is ambiguous, could be: Department, Department.\" exception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Reference 'Department' is ambiguous, could be: Department, Department.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43memp_dept\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDepartment\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py:1685\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m   1664\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[0;32m   1665\u001b[0m     \u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[0;32m   1666\u001b[0m \n\u001b[0;32m   1667\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1683\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[0;32m   1684\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1685\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msql_ctx)\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Spark\\spark-3.2.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    113\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Reference 'Department' is ambiguous, could be: Department, Department."
     ]
    }
   ],
   "source": [
    "emp_dept.select('Name', 'Department', 'Location').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this exception we can use the following code that adds aliases to each dataframe, so we can select the columns by the alias name. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---+-----------+-----------+-------------+\n",
      "|EmployeeID|   Name|Age| Department| Department|     Location|\n",
      "+----------+-------+---+-----------+-----------+-------------+\n",
      "|         1|  Alice| 30|Engineering|Engineering|     New York|\n",
      "|         2|    Bob| 25|Engineering|Engineering|     New York|\n",
      "|         4|  David| 28|    Finance|    Finance|  Los Angeles|\n",
      "|         3|Charlie| 35|         HR|         HR|San Francisco|\n",
      "|         5|    Eve| 22|         HR|         HR|San Francisco|\n",
      "+----------+-------+---+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept = employee_df.alias('emp').join(department_df.alias('dept'), employee_df.Department == department_df.Department, 'right')\n",
    "emp_dept.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the alias dept to access the departments dataframe and alias emp to access the employees dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----------+-------------+\n",
      "|   Name| Department| Department|     Location|\n",
      "+-------+-----------+-----------+-------------+\n",
      "|  Alice|Engineering|Engineering|     New York|\n",
      "|    Bob|Engineering|Engineering|     New York|\n",
      "|  David|    Finance|    Finance|  Los Angeles|\n",
      "|Charlie|         HR|         HR|San Francisco|\n",
      "|    Eve|         HR|         HR|San Francisco|\n",
      "+-------+-----------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept.select('Name', 'dept.Department', 'emp.Department','dept.Location').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "9db6cbf0fd79f8e79653fe7b0c50b956ca6e525ee712295da3c66f75e4fe96ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
